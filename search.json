[
  {
    "objectID": "posts/2021-08-13-fishing-of-ideas.html",
    "href": "posts/2021-08-13-fishing-of-ideas.html",
    "title": "CH2-捕获点子",
    "section": "",
    "text": "This is the surprise: Finding a trading idea is actually not the hardest part of building a quantitative trading business. There are hundreds, if not thousands, of trading ideas that are in the public sphere at any time, accessible to anyone at little or no cost. Many authors of these trading ideas will tell you their complete methodologies in addition to their backtest results. There are finance and investment books, newspapers and magazines, main\u0002stream media web sites, academic papers vailable online or in the nearest public library, trader forums, blogs, and on and on.\n\n你可能会奇怪：寻找交易点子不是建立量化交易事业最难的部分。在公共领域有成千上万的交易点子，任何个人任何时间都可以免费或者用一点点花费就能获取到。很多交易点子的作者，不仅会完全告诉你实现的方法，还会附上他们的回测结果。有很多金融投资的书籍，报纸，杂志，主流媒体的网站，在线的学术期刊，或者最近的公共图书馆，交易者论坛，博客等等。\n\n\n\n类型\n网址\n\n\n\n\nAcademic\n\n\n\nBusiness schools’ finance professors’ websites\nwww.hbs.edu/research/research.html\n\n\nSocial Science Research Network\nwww.ssrn.com\n\n\nNational Bureau of Economic Research\nwww.nber.org\n\n\nBusiness schools’ quantitative finance seminars\nwww.ieor.columbia.edu/seminars/financialengineering\n\n\nButtonwood column in the Economist magazine’s finance section\nwww.economist.com\n\n\nFinancial web sites and blogs\n\n\n\nYahoo! Finance\nfinance.yahoo.com\n\n\nTradingMarkets\nwww.TradingMarkets.com\n\n\nSeeking Alpha\nwww.SeekingAlpha.com\n\n\nTheStreet.com\nwww.TheStreet.com\n\n\nThe Kirk Report\nwww.TheKirkReport.com\n\n\nAlea Blog\nwww.aleablog.com\n\n\nAbnormal Returns\nwww.AbnormalReturns.com\n\n\nBrett Steenbarger Trading Psychology\nwww.brettsteenbarger.com\n\n\n本书作者\nepchan.blogspot.com\n\n\nTrader forums\n\n\n\nElite Trader\nwww.Elitetrader.com\n\n\nWealth-Lab\nwww.wealth-lab.com\n\n\nNewspaper and magazines\n\n\n\nStocks, Futures and Options magazine\nwww.sfomag.com\n\n\n\n\nNo, the difficulty is not the lack of ideas. The difficulty is to develop a taste for which strategy is suitable for your personal circumstances and goals, and which ones look viable even before you devote the time to diligently backtest them. This taste for prospective strategies is what I will try to convey in this chapter.\n\n困难不是缺少点子。困难是培养出一种敏锐的嗅觉，能分辨出哪些策略是适合你的环境和目标的，并能在对策略进行回测前发觉其是否可行。这种预见策略的嗅觉正是我要在本章试图阐明的。"
  },
  {
    "objectID": "posts/2021-08-13-fishing-of-ideas.html#从何处获取策略",
    "href": "posts/2021-08-13-fishing-of-ideas.html#从何处获取策略",
    "title": "CH2-捕获点子",
    "section": "",
    "text": "This is the surprise: Finding a trading idea is actually not the hardest part of building a quantitative trading business. There are hundreds, if not thousands, of trading ideas that are in the public sphere at any time, accessible to anyone at little or no cost. Many authors of these trading ideas will tell you their complete methodologies in addition to their backtest results. There are finance and investment books, newspapers and magazines, main\u0002stream media web sites, academic papers vailable online or in the nearest public library, trader forums, blogs, and on and on.\n\n你可能会奇怪：寻找交易点子不是建立量化交易事业最难的部分。在公共领域有成千上万的交易点子，任何个人任何时间都可以免费或者用一点点花费就能获取到。很多交易点子的作者，不仅会完全告诉你实现的方法，还会附上他们的回测结果。有很多金融投资的书籍，报纸，杂志，主流媒体的网站，在线的学术期刊，或者最近的公共图书馆，交易者论坛，博客等等。\n\n\n\n类型\n网址\n\n\n\n\nAcademic\n\n\n\nBusiness schools’ finance professors’ websites\nwww.hbs.edu/research/research.html\n\n\nSocial Science Research Network\nwww.ssrn.com\n\n\nNational Bureau of Economic Research\nwww.nber.org\n\n\nBusiness schools’ quantitative finance seminars\nwww.ieor.columbia.edu/seminars/financialengineering\n\n\nButtonwood column in the Economist magazine’s finance section\nwww.economist.com\n\n\nFinancial web sites and blogs\n\n\n\nYahoo! Finance\nfinance.yahoo.com\n\n\nTradingMarkets\nwww.TradingMarkets.com\n\n\nSeeking Alpha\nwww.SeekingAlpha.com\n\n\nTheStreet.com\nwww.TheStreet.com\n\n\nThe Kirk Report\nwww.TheKirkReport.com\n\n\nAlea Blog\nwww.aleablog.com\n\n\nAbnormal Returns\nwww.AbnormalReturns.com\n\n\nBrett Steenbarger Trading Psychology\nwww.brettsteenbarger.com\n\n\n本书作者\nepchan.blogspot.com\n\n\nTrader forums\n\n\n\nElite Trader\nwww.Elitetrader.com\n\n\nWealth-Lab\nwww.wealth-lab.com\n\n\nNewspaper and magazines\n\n\n\nStocks, Futures and Options magazine\nwww.sfomag.com\n\n\n\n\nNo, the difficulty is not the lack of ideas. The difficulty is to develop a taste for which strategy is suitable for your personal circumstances and goals, and which ones look viable even before you devote the time to diligently backtest them. This taste for prospective strategies is what I will try to convey in this chapter.\n\n困难不是缺少点子。困难是培养出一种敏锐的嗅觉，能分辨出哪些策略是适合你的环境和目标的，并能在对策略进行回测前发觉其是否可行。这种预见策略的嗅觉正是我要在本章试图阐明的。"
  },
  {
    "objectID": "posts/2021-08-13-fishing-of-ideas.html#如何辨别策略是否适合你",
    "href": "posts/2021-08-13-fishing-of-ideas.html#如何辨别策略是否适合你",
    "title": "CH2-捕获点子",
    "section": "如何辨别策略是否适合你",
    "text": "如何辨别策略是否适合你\n有如下几个评判依据： * 你的工作时间 * 你的编程技能 * 你的交易资本 * 你的目标"
  },
  {
    "objectID": "posts/2021-08-13-fishing-of-ideas.html#识别可用的策略及其陷阱",
    "href": "posts/2021-08-13-fishing-of-ideas.html#识别可用的策略及其陷阱",
    "title": "CH2-捕获点子",
    "section": "识别可用的策略及其陷阱",
    "text": "识别可用的策略及其陷阱\n用这些方法快测试策略，确保你不会浪费你的时间和金钱\n\n这个策略和基准比较表现如何?它的回报有多持久? &gt; Though a strategy may have the same average return as the benchmark, perhaps it delivered positive returns every month while the benchmark occasionally suffered some very bad months. In this case, we would still deem the strategy superior. This leads us to consider the information ratio or Sharpe ratio (Sharpe,1994), rather than returns, as the proper performance measurement of a quantitative trading strategy.\n\n虽让一个策略可能和基准有相同的回报，但在基准下跌的月份里它仍然有正收益。我们仍然认为这个策略是优越的。这促使我们考虑用信息比率或者夏普比率，而不是回报来，作为衡量量化交易策略业绩的指标。\n\n$ Information Ratio = $\n\n$ 信息比率 = $\n(其中：超额收益率 = 组合收益率 - 基准收益率)\n\nAs a rule of thumb, any strategy that has a Sharpe ratio of less than 1 is not suitable as a stand-alone strategy. For a strategy that achieves profitability almost every month, its (annualized) Sharpe ratio is typically greater than 2. For a strategy that is profitable al\u0002most every day, its Sharpe ratio is usually greater than 3.\n\n一般来说，任何夏普率小于1的策略不适合作为单独策略。对于每月都有收益的策略，它的年化夏普率通常大于2.对于每天有收益的策略，他的夏普率通常大于3.\n\n下挫有多深多久？\n\n你要问自己，你能承受多深多久的下挫而不清算你的投资组合关闭你的策略。\n\n交易成本如何影响策略？\n\n交易成本不仅包括券商收取的佣金，还有流动成本和机会成本。\n\n数据是否存在生存者偏差\n\n历史数据库中的股票报价不包含破产退市的股票。\n\n为什么策略的业绩过几年会发生改变？\n\n策略回测要关注近几年的表现，久远的数据会包含交易成本和幸存者偏差的影响。\n\n策略是否收到数据范围偏差影响？\n\n策略含有过多参数可能会对历史数据产生过度拟合。\n\n策略是否在基金经理的盲区\n\n\nyou should look for those strategies that fly under the radar of most institutional investors, for example, strategies that have very low capacities because they trade too often, strategies that trade very few stocks ev\u0002ery day, or strategies that have very infrequent positions.\n\n你应该去寻找被机构投资者忽视的策略，比如，交易频繁容量很低的策略，每天只交易很少股票的策略，持仓时段稀少的策略。这样的特色策觉才有利可图，因为他们还没有完全被巨型的对冲基金套利掉。"
  },
  {
    "objectID": "posts/2021-08-27-money-and-risk-management.html",
    "href": "posts/2021-08-27-money-and-risk-management.html",
    "title": "CH6-资金和风险管理",
    "section": "",
    "text": "想从量化交易中赚钱，风险管理至关重要，把挫跌控制在可接受的范围内，把头寸建在净值的最优杠杆水平上，才能实现财富的最大可能增长。 我们要用的主要工具是凯利公式。"
  },
  {
    "objectID": "posts/2021-08-27-money-and-risk-management.html#最佳资本配置和杠杆",
    "href": "posts/2021-08-27-money-and-risk-management.html#最佳资本配置和杠杆",
    "title": "CH6-资金和风险管理",
    "section": "最佳资本配置和杠杆",
    "text": "最佳资本配置和杠杆\n假设计划进行几个策略的交易，每个策略都有其预期收益和标准差。那么，如何在这些策略之间进行最优的资本配置呢？\n我们的优化目标是长期财富最大化。一定要避免赔光。\n假设策略i(这里用i代表第i个策略)的收益率服从正态分布，其均值\\(m_i\\)和标准差\\(s_i\\)已给定。\n用列向量\n\\(F^* = (f_1^* ,f_2^* ,...,f_n^* )^T\\)\n表示分配到n个策略的最优净值比例，其中T代表转置。\n给定优化目标并假设收益率服从正态分布，Thorp不是给出了以下最优配置公式：\n\\(F^* = C^{-1}M\\)\n\nC表示协方差矩阵，矩阵的元素\\(C_ij\\)表示第i 个策略和第j个策略收益率的协方差，-1表示矩阵的逆\n\\(M = (m_1,m_2,...,m_n)^T\\) 表示策略平均收益率的列向量\n\n如果假设所有策略在统计上独立，协方差矩阵就变为对角矩阵，对角线元素等于每个策略收益率的方差，公式：\n\\(f_i = m_i/s_i^2\\)\n这就是著名的凯利公式。"
  },
  {
    "objectID": "posts/2021-08-27-money-and-risk-management.html#做好心理准备",
    "href": "posts/2021-08-27-money-and-risk-management.html#做好心理准备",
    "title": "CH6-资金和风险管理",
    "section": "做好心理准备",
    "text": "做好心理准备\n\n禀赋效应、安于现状偏差、亏损厌恶\n代表性偏差（即人们倾向于对近期经验赋予过多权重，而低估了长期平均的作用）\n恐惧和贪婪"
  },
  {
    "objectID": "posts/2021-08-27-money-and-risk-management.html#收益率正态分布时凯利公式的简单推导",
    "href": "posts/2021-08-27-money-and-risk-management.html#收益率正态分布时凯利公式的简单推导",
    "title": "CH6-资金和风险管理",
    "section": "收益率正态分布时凯利公式的简单推导",
    "text": "收益率正态分布时凯利公式的简单推导\n适用于正态分布的复合杠杆增长率公式为：\n\\(g(f) = r + fm - s^2f^2/2\\)\n\nf为杠杆\nr为无风险利率\nm为平均非复合单期超额收益率\ns为非复合单期收益率的标准差\n\n为了得出使g最大化时的f，令g对f的一阶导数为零：\n\\(dg/df = m - s^2f = 0\\)\n由等式可得\\(f=m/s^2\\), 即为正态分布下策略或政权的凯利公式。"
  },
  {
    "objectID": "posts/2024-07-27-Algorithmic-Trading.html",
    "href": "posts/2024-07-27-Algorithmic-Trading.html",
    "title": "算法交易：制胜策略与原理(欧内斯特·陈)之一",
    "section": "",
    "text": "在均值回归交易策略所相关的系列当中，我们将讨论以多元的统计技术［如扩展版的迪基-富勒检验（Dickey-Fuller检验，即ADF检验）、赫斯特（Hurst）指数、方差比检验、半衰期检验模式等］来检测时间序列的均值回归之属性，以及相关的平稳性；同时，我们还要检测一个由金融工具所构建的投资组合之协整属性［相关检测模式包括协整型ADF检验（即CADF检验）、约翰森（Johansen）检验等］。除了前述这些统计测试模式被机械地应用于时间序列而外，我们还要努力传达一个直观的理解方法，即要认知相关测试的真正用意以及简易数学方程背后的深层含义。\n我们将解析一些具有均值回归属性之投资组合所相关的最简单的技术和策略模式［如线性交易模式、布林带线、卡尔曼过滤法则(Kalman filter)等］。\n这个可以用backtrader框架来实现"
  },
  {
    "objectID": "posts/2024-07-27-Algorithmic-Trading.html#泰勒连续展开公式",
    "href": "posts/2024-07-27-Algorithmic-Trading.html#泰勒连续展开公式",
    "title": "算法交易：制胜策略与原理(欧内斯特·陈)之一",
    "section": "泰勒连续展开公式",
    "text": "泰勒连续展开公式\n泰勒连续展开公式（Taylor Series Expansion）是数学中用于表示一个函数在某一点附近的一个无穷级数展开。这个公式是以英国数学家布鲁克·泰勒（Brook Taylor）的名字命名的。泰勒级数的基本思想是用一个函数在某一点附近的各阶导数值来代替该函数，从而得到一个关于变量的无穷级数表达式。\n泰勒连续展开公式的定义如下：设函数f(x)在点x=a的某邻域内具有n阶导数，那么对于这个区间上任意x，有：\n$ f(x) = f(a) + f’(a)(x - a) + (x - a)^2 + + (x - a)^n + R_n(x) $\n其中，f’(a)表示函数f的一阶导数在点a处的值，f’’(a)表示二阶导数在点a处的值，以此类推。R_n(x)是余项，它随着n的增大趋近于0。当n趋向于无穷大时，泰勒级数就变成了泰勒展开式。\n泰勒级数有很多实际应用，例如用于数值计算、求解微分方程、信号处理等领域。\n\n瞬时出现的数据信息会在瞬时之间即刻消失。这是因为对于相应的做市商与电子通信网络的运营商而言，他们没有义务向所有的交易参与方通报相关的价格信息。坦白地说，很多做市商将相关的交易信息视若珍宝，同时将对这些信息的掌控视为自身的专利与特权（他们这种做法很聪明，因为，如货币这般的高频交易策略往往要依赖于订单信息与价格信息，这一点我们在第7章会有所涉及。像银行之类金融机构中的外汇自营交易部门，就擅长于将前述的相关信息据为己有）。\n\n\n如果我们发现一个比较理想的概率，在其情境之下，应用相关交易策略所获取的收益不输于或好于随机的收益以及实际收益的观测值，那么，这并不意味着所谓的动量交易策略能够捕捉任何行情或收益，其之所以盈利是因为我们比较幸运，即攫取的收益率的观测数值所生成的概率分布恰巧具有一个特定的均值和分布形态。为了在规定的时刻内生成模拟的随机收益率的相应数据，我们可以从MATLAB系统中的统计软件内选取那些服从皮尔逊分布的随机数来创建相应的函数；在模拟收益率marketRet_sim生成之后，我们根据其变化创建模拟的价格序列cl_sim；最后，我们根据相应价格的变化运行相关的交易策略，同时计算该策略项下的平均收益率。我们需要模拟10000次，然后统计有多少次因此策略而产生的平均收益大于等于实际的观测数据。\n\n\n# Hypothesis Testing on a Futures Momentum Strategy\n\nimport numpy as np\nimport pandas as pd\n#from scipy.stats import describe\nfrom scipy.stats import pearson3\n\n# moments表示阶矩，pearsrnd表示皮尔逊分布，skewness表示收益率曲线的偏度，kurtosis指峰度值，long代表多头（买方），short代表空头（卖方），backshift指二次变分，pos指持仓头寸。\ndf=pd.read_csv('datas/TU.csv')\ndf['Time']=pd.to_datetime(df['Time']).dt.date # remove HH:MM:SS\ndf.set_index('Time', inplace=True)\n\nlookback=250\nholddays=25\n\n# 使用shift()方法将数据向下移动一行\nlongs= df['Close'] &gt; df['Close'].shift()\nshorts= df['Close'] &lt; df['Close'].shift()\n\npos=np.zeros(df.shape[0])\n\nfor h in range(0, holddays):\n    long_lag=longs.shift(h)\n    long_lag[long_lag.isna()]=False\n    long_lag=long_lag.astype(bool)\n\n    short_lag=shorts.shift(h)\n    short_lag[short_lag.isna()]=False\n    short_lag=short_lag.astype(bool)\n\n    pos[long_lag]=pos[long_lag]+1\n    pos[short_lag]=pos[short_lag]-1\n    \ncapital=np.nansum(np.array(pd.DataFrame(abs(pos)).shift()), axis=1)\npos[capital==0,]=0\ncapital[capital==0]=1\n# pct_change()方法是pandas库中DataFrame和Series对象的一个常用方法，用于计算数据的变化百分比。\n# 该方法计算相邻行之间的相对变化，并返回一个新的Series或DataFrame，其中包含每个元素相对于前一个元素的百分比变化。\n# 计算收盘价的每日百分比变化，得到一个包含每日收益率的Series对象。\nmarketRet=df['Close'].pct_change()\n# pd.DataFrame(pos).shift()：将持仓（pos）转换为DataFrame对象，并将其向上移动一行。这样做的目的是为了计算每个持仓在下一交易日的表现。\n# np.array(pd.DataFrame(pos).shift()) * np.array(marketRet)：将移动后的持仓数据与每日收益率相乘，得到每个持仓在下一交易日的收益。\n# np.nansum(..., axis=1)：沿着行方向对收益进行求和，得到每个交易日的总收益。\n# ... / capital / holddays：将总收益除以初始资本（capital）和持仓天数（holddays），得到每日收益率。\nret=np.nansum(np.array(pd.DataFrame(pos).shift())*np.array(marketRet), axis=1)/capital/holddays\n# 夏普比率（Sharpe Ratio） \n# np.sqrt(len(ret))：计算观测值的数量的平方根，即 (\\sqrt{T})，其中 (T) 是观测值的数量。\n# np.nanmean(ret)：计算收益率的平均值，忽略NaN值。\n# np.nanstd(ret)：计算收益率的标准差，忽略NaN值。\n# 将上述三个值相除，得到夏普比率\nsharpe=np.sqrt(len(ret))*np.nanmean(ret)/np.nanstd(ret)\n\nprint(\"Gaussian Test statistic=%f\" % sharpe)\n#Gaussian Test statistic=2.769741\n\n# Randomized market returns hypothesis test\n# =============================================================================\n#_,_,mean,var,skew,kurt=describe(marketRet, nan_policy='omit')\n# =============================================================================\n\n# fit方法用于估计Pearson III分布的参数。\n# fit方法返回三个参数：skew_、loc_和scale_，分别表示：\n# skew_：偏度参数，描述了分布的不对称程度。正值表示右偏（尾部向右延伸），负值表示左偏（尾部向左延伸）。\n# `loc_**：**位置参数，描述了分布的中心位置。通常与均值相关。\n# `scale_**：**尺度参数，描述了分布的宽度或分散程度。通常与标准差相关。\nskew_, loc_, scale_=pearson3.fit(marketRet[1:]) # First element is NaN\nnumSampleAvgretBetterOrEqualObserved=0\nfor sample in range(100):\n    # rvs方法用于从指定的Pearson III分布中随机抽取样本。\n    # 以下是代码中各个参数的含义：\n    # skew_：偏度参数，描述了分布的不对称程度。\n    # loc_：位置参数，描述了分布的中心位置。\n    # scale_：尺度参数，描述了分布的宽度或分散程度。\n    # size：生成的模拟数据的大小，这里设置为与原始市场收益率数据（marketRet）相同的大小。\n    # random_state：随机数生成器的种子，用于确保每次运行代码时生成的随机数据相同。\n    marketRet_sim=pearson3.rvs(skew=skew_, loc=loc_, scale=scale_, size=marketRet.shape[0], random_state=sample)\n    cl_sim=np.cumproduct(1+marketRet_sim)-1\n    \n    longs_sim =cl_sim &gt; pd.Series(cl_sim).shift(lookback)\n    shorts_sim=cl_sim &lt; pd.Series(cl_sim).shift(lookback)\n    \n    pos_sim=np.zeros(cl_sim.shape[0])\n    \n    for h in range(0, holddays):\n        long_sim_lag=longs_sim.shift(h)\n        long_sim_lag[long_sim_lag.isna()]=False\n        long_sim_lag=long_sim_lag.astype(bool)\n    \n        short_sim_lag=shorts_sim.shift(h)\n        short_sim_lag[short_sim_lag.isna()]=False\n        short_sim_lag=short_sim_lag.astype(bool)\n    \n        pos_sim[long_sim_lag]=pos_sim[long_sim_lag]+1\n        pos_sim[short_sim_lag]=pos_sim[short_sim_lag]-1\n        \n        capital=np.nansum(np.array(pd.DataFrame(abs(pos_sim)).shift()), axis=1)\n        pos_sim[capital==0,]=0\n        capital[capital==0]=1\n               \n        ret_sim=np.nansum(np.array(pd.DataFrame(pos_sim).shift())*np.array(marketRet_sim), axis=1)/capital/holddays\n        if (np.mean(ret_sim) &gt;= np.mean(ret)):\n            numSampleAvgretBetterOrEqualObserved=numSampleAvgretBetterOrEqualObserved+1\n            \nprint(\"Randomized prices: p-value=%f\" % (numSampleAvgretBetterOrEqualObserved/100))\n#Randomized prices: p-value=23.617800\n\n# Randomized entry trades hypothesis test\nnumSampleAvgretBetterOrEqualObserved=0\nfor sample in range(100):\n    # 生成一个随机排列的索引数组\n    P=np.random.permutation(len(longs))\n    longs_sim=longs[P]\n    shorts_sim=shorts[P]\n    \n    pos_sim=np.zeros(cl_sim.shape[0])\n\n    for h in range(0, holddays):\n        long_sim_lag=longs_sim.shift(h)\n        long_sim_lag[long_sim_lag.isna()]=False\n        long_sim_lag=long_sim_lag.astype(bool)\n    \n        short_sim_lag=shorts_sim.shift(h)\n        short_sim_lag[short_sim_lag.isna()]=False\n        short_sim_lag=short_sim_lag.astype(bool)\n    \n        pos_sim[long_sim_lag]=pos_sim[long_sim_lag]+1\n        pos_sim[short_sim_lag]=pos_sim[short_sim_lag]-1\n        \n        capital=np.nansum(np.array(pd.DataFrame(abs(pos_sim)).shift()), axis=1)\n        pos_sim[capital==0,]=0\n        capital[capital==0]=1\n               \n        ret_sim=np.nansum(np.array(pd.DataFrame(pos_sim).shift())*np.array(marketRet), axis=1)/capital/holddays\n        if (np.mean(ret_sim) &gt;= np.mean(ret)):\n            numSampleAvgretBetterOrEqualObserved=numSampleAvgretBetterOrEqualObserved+1\n            \nprint(\"Randomized trades: p-value=%f\" % (numSampleAvgretBetterOrEqualObserved/100))\n#Randomized trades: p-value=1.365600\n\nGaussian Test statistic=2.769741\nRandomized prices: p-value=23.500000\nRandomized trades: p-value=1.000000\n\n\n\n#是的，Python 中有约翰森检验函数，即 scipy.stats.johnsonsu。\n# 该函数用于对连续型数据进行正态性检验，并返回约翰森变换后的参数。\n#以下是 scumpy.stats.johnsonsu 函数的基本用法示例：\n\nfrom scipy import stats\n\n# 假设有一组连续型数据\ndata = [1.2, 3.4, 5.6, 7.8, 9.0]\n\n# 对数据进行约翰森正态性检验\nresult = stats.johnsonsu.fit(data)\n\n# 输出约翰森变换后的参数\nprint(result)\n\n\n# 需要注意的是，约翰森检验函数适用于连续型数据的正态性检验，对于离散型数据或其他类型的数据可能不适用。\n\n(np.float64(9.932945657673585), np.float64(1.8845431800180394), np.float64(11.36217894129853), np.float64(0.05378935338333127))\n\n\n\nimport altair as alt\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 创建一个简单的数据集\ndata = pd.DataFrame({\n    'x': [1, 2, 3, 4, 5],\n    'y': [2, 4, 6, 8, 10]\n})\n\n# 创建一个散点图\nchart = alt.Chart(data).mark_circle().encode(\n    x='x:Q',\n    y='y:Q'\n)\n# 显示图表\n# chart.show()\nplt.plot(data)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.rcParams[\"font.sans-serif\"] = [\"SimHei\"] \nmpl.rcParams['axes.unicode_minus'] = False # 正常显示负号\n# zhfont=mpl.font_manager.FontProperties(fname=\"/System/Library/Fonts/PingFang.ttc\")\n# mpl.rcParams['axes.unicode_minus'] = False\nx=np.linspace(-np.pi,np.pi,100)\ny=np.sin(x)\nplt.title(u'正弦函数')\nplt.plot(x,y)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# import altair with an abbreviated alias\nimport altair as alt\n# print(alt.Config)\n\n# alt.renderers.set_embed_options(base_url='js/',vega_url='js/vega@5',\n# vega_lite_url='js/vega-lite@4',\n# vega_embed_url='js/vega-embed@6')\n# 算了吧太费劲了，放弃altair\n# alt.renderers.enable('html', base_url='js')\n\n# load a sample dataset as a pandas DataFrame\nfrom vega_datasets import data\ncars = data.cars()\n# alt.renderers.enable('html', vega_cdn='https://cdnjs.cloudflare.com/libraries/vega', vega_lite_cdn='https://cdnjs.cloudflare.com/libraries/vega-lite')\n# https://cdnjs.cloudflare.com/ajax/libs/vega-embed/6.26.0/vega-embed.min.js\n# make the chart\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color='Origin',\n).interactive()"
  },
  {
    "objectID": "posts/2021-08-27-special-topics-in-quantitative-trading.html",
    "href": "posts/2021-08-27-special-topics-in-quantitative-trading.html",
    "title": "CH7-量化交易专题",
    "section": "",
    "text": "只有当证券价格是均值回归的或趋势的，交易策略才能盈利。否则，价格是随机漫步的，交易将无利可图。 如果你相信价格是均值回归的，并且目前相对较低，应当现在买入，并准备在以后价格升高时卖出。但是，如果你相信价格是趋势的，且目前处于低位，应当现在卖出，并准备在以后价格更低时买入。价格处于高位则刚好相反。\n学术研究表明，股票价格“一般而言”非常接近随机漫步。但这并不意味着在特殊条件下价格不会表现出一定程度的均值回归或趋势行为。"
  },
  {
    "objectID": "posts/2021-08-27-special-topics-in-quantitative-trading.html#均值回归策略和惯性策略",
    "href": "posts/2021-08-27-special-topics-in-quantitative-trading.html#均值回归策略和惯性策略",
    "title": "CH7-量化交易专题",
    "section": "",
    "text": "只有当证券价格是均值回归的或趋势的，交易策略才能盈利。否则，价格是随机漫步的，交易将无利可图。 如果你相信价格是均值回归的，并且目前相对较低，应当现在买入，并准备在以后价格升高时卖出。但是，如果你相信价格是趋势的，且目前处于低位，应当现在卖出，并准备在以后价格更低时买入。价格处于高位则刚好相反。\n学术研究表明，股票价格“一般而言”非常接近随机漫步。但这并不意味着在特殊条件下价格不会表现出一定程度的均值回归或趋势行为。"
  },
  {
    "objectID": "posts/2021-08-27-special-topics-in-quantitative-trading.html#状态转换",
    "href": "posts/2021-08-27-special-topics-in-quantitative-trading.html#状态转换",
    "title": "CH7-量化交易专题",
    "section": "状态转换",
    "text": "状态转换\n状态是金融市场中一个最基本的概念。如果没有状态，何来“牛市”和“熊市”？自金融市场诞生之日起，人们就试图预测状态转换，即寻找所谓的“拐点”。\n其他最常见的金融或经济状态研究，包括通货膨胀与经济衰退状态、高波动率与低波动率状态以及均值回归与趋势状态。其中，波动率状态转换似乎最实用经典计量经济学工具，如广义自回归条件异方差（GARCH）模型。\n学术界一般沿用以下思路对股票价格的状态转换进行建模：\n\n假设价格在两个（或多个）状态上的概率分布不同。最简单的情况，两个状态的价格都服从对数正态分布，但均值和（或）标准差不同。\n假设状态之间存在某种转移概率\n使用诸如最大似然估计这样的标准统计方法，通过拟合历史数据，来确定状态概率分布和转移概率的参数。\n根据上述拟合模型，找出下一个时间步长的期望状态，更重要的，找出股票的期望价格。\n\n这种方法通常被称为“马尔科夫状态转换模型”或“隐马尔科夫模型”，这一模型通常基于贝叶斯概率框架。\n虽然理论框架完美，马尔科夫状态转换模型在实际交易中的用途却不大。这是因为模型假设状态之间的转移概率都是固定的。实际应用中，这意味着在任何时候股票从正常的静止状态转移到不稳定状态的概率非常小。而这对于想知道转移概率在何时（以及何种情况下）突然达到峰值的交易员，是完全没有用的。这就需要用到拐点模型。\n拐点模型使用了数据挖掘方法：输入所有可能预测拐点或状态转换的变量。变量包括当前的波动率、最近一期的收益，以及消费者信心指数、石油价格变化、债券价格变化等宏观经济数据的变化。"
  },
  {
    "objectID": "posts/2021-08-27-special-topics-in-quantitative-trading.html#平稳性和协整性",
    "href": "posts/2021-08-27-special-topics-in-quantitative-trading.html#平稳性和协整性",
    "title": "CH7-量化交易专题",
    "section": "平稳性和协整性",
    "text": "平稳性和协整性\n如果一个时间序列不会越来越大地偏离初始值，这个时间序列就是“平稳的”。用专业术语来说，平稳的时间序列就是“零阶自积”的，即I(0)。不过，大多数股票的价格序列都是不平稳的，通常表现为几何随机游走，不断地离出事点（如首次公开发行价）价值越来越远。尽管如此，你能找到像买入一只股票、卖出一只股票这样的股票配对，配对的市场价值是平稳的。这种情况下，两个独立的时间序列被称为“协整”。协整配对中的两个股票来自同一行业。\n检测两个价格序列是否协整，如何找到最优的对冲比率。 协整检验的主要方法是ADF检验。\n如果一个价格序列（可以是一只股票、一对股票或是一个投资组合）是平稳的，只要未来继续保持平稳（这未必能保证），采用均值回归策略一定能盈利。"
  },
  {
    "objectID": "posts/2021-08-27-special-topics-in-quantitative-trading.html#因子模型",
    "href": "posts/2021-08-27-special-topics-in-quantitative-trading.html#因子模型",
    "title": "CH7-量化交易专题",
    "section": "因子模型",
    "text": "因子模型\n因子收益率是股票收益率的共同驱动因素，与单个股票无关。\n因子风险表示对哥哥共同驱动因素的敏感度。\n所有不能用共同因子解释的收益部分就是特有收益率。（比如，仅与某只股票相关，可以看做是APT模型中的随机噪音部分）"
  },
  {
    "objectID": "posts/2021-08-27-special-topics-in-quantitative-trading.html#清仓策略",
    "href": "posts/2021-08-27-special-topics-in-quantitative-trading.html#清仓策略",
    "title": "CH7-量化交易专题",
    "section": "清仓策略",
    "text": "清仓策略\n\n固定的持有期\n目标价格或盈利上限\n最新的建仓信号\n止损价格"
  },
  {
    "objectID": "posts/2021-08-27-special-topics-in-quantitative-trading.html#季节性交易策略",
    "href": "posts/2021-08-27-special-topics-in-quantitative-trading.html#季节性交易策略",
    "title": "CH7-量化交易专题",
    "section": "季节性交易策略",
    "text": "季节性交易策略"
  },
  {
    "objectID": "posts/2021-08-27-special-topics-in-quantitative-trading.html#高频交易策略",
    "href": "posts/2021-08-27-special-topics-in-quantitative-trading.html#高频交易策略",
    "title": "CH7-量化交易专题",
    "section": "高频交易策略",
    "text": "高频交易策略\n高频交易策略能获得高夏普比率的理由很简单：根据大数定律，交易的次数越多，收益率相对于均值的偏差就越小。而在高频交易策略下，一天可交易成百上千次。（这交易佣金也会很高啊）"
  },
  {
    "objectID": "posts/2021-08-19-executions-systems.html",
    "href": "posts/2021-08-19-executions-systems.html",
    "title": "CH5-交易执行系统",
    "section": "",
    "text": "An automated trading system will retrieve up-to-date market data from your brokerage or other data vendors, run a trading algorithm to generate orders, and submit those orders to your brokerage for execution.\n\n自动交易系统从经纪商或其他供应商获取市场数据，运行交易算法形成指令，提交指令给经纪商执行。\n全自动交易系统代价高昂，对于低频量化交易策略，半自动交易系统即可。\n\n半自动交易系统\n全自动交易系统"
  },
  {
    "objectID": "posts/2021-08-19-executions-systems.html#自动交易系统能为你做什么",
    "href": "posts/2021-08-19-executions-systems.html#自动交易系统能为你做什么",
    "title": "CH5-交易执行系统",
    "section": "",
    "text": "An automated trading system will retrieve up-to-date market data from your brokerage or other data vendors, run a trading algorithm to generate orders, and submit those orders to your brokerage for execution.\n\n自动交易系统从经纪商或其他供应商获取市场数据，运行交易算法形成指令，提交指令给经纪商执行。\n全自动交易系统代价高昂，对于低频量化交易策略，半自动交易系统即可。\n\n半自动交易系统\n全自动交易系统"
  },
  {
    "objectID": "posts/2021-08-19-executions-systems.html#雇佣编程顾问",
    "href": "posts/2021-08-19-executions-systems.html#雇佣编程顾问",
    "title": "CH5-交易执行系统",
    "section": "雇佣编程顾问",
    "text": "雇佣编程顾问\n这里有个策略保密的问题，其他的讨论不值一提，但是有个方法挺好的，就是把策略拆分雇佣不同的程序员，没人实现一部分，最后再合并。"
  },
  {
    "objectID": "posts/2021-08-19-executions-systems.html#最小化交易成本",
    "href": "posts/2021-08-19-executions-systems.html#最小化交易成本",
    "title": "CH5-交易执行系统",
    "section": "最小化交易成本",
    "text": "最小化交易成本\n\n可以通过避免交易低价股票来降低佣金\n根据股票的流动性来限制指令规模以减小市场冲击成本。警惕成交量小的股票，购买量不要超过其每日成交量的1%。\n根据股票市值来调整下单量也是减少市场冲击的方法\n但是不要按线性比率来调整下单量，因为公司市值差别巨大，按照固定比例会让下公司的下单量为0\n如果是超大单，分批次下单，以减少市场冲击。独立交易者通常不需要。"
  },
  {
    "objectID": "posts/2021-08-19-executions-systems.html#用仿真交易测试交易系统",
    "href": "posts/2021-08-19-executions-systems.html#用仿真交易测试交易系统",
    "title": "CH5-交易执行系统",
    "section": "用仿真交易测试交易系统",
    "text": "用仿真交易测试交易系统"
  },
  {
    "objectID": "posts/2021-08-19-executions-systems.html#为什么实际业绩偏离预期",
    "href": "posts/2021-08-19-executions-systems.html#为什么实际业绩偏离预期",
    "title": "CH5-交易执行系统",
    "section": "为什么实际业绩偏离预期",
    "text": "为什么实际业绩偏离预期\n\n自动交易软件是否有bug\n自动交易系统产生的交易和回测程序产生的交易是否一致\n交易成本是否比预期的高很多\n是否交易了流动性差的股票产生了市场冲击\n数据迁就偏差\n状态变更影响，市场结构和宏观经济环境发生巨变"
  },
  {
    "objectID": "posts/2021-12-06-backtrader.html",
    "href": "posts/2021-12-06-backtrader.html",
    "title": "backtrader回测系统",
    "section": "",
    "text": "import datetime\nimport backtrader as bt\nimport pandas as pd\n\nclass TestStrategy(bt.Strategy):\n    params = (\n        ('maperiod', 15),\n    )\n\n    def log(self, txt, dt=None):\n        dt = dt or self.datas[0].datetime.date(0)\n        print('%s, %s' % (dt.isoformat(), txt))\n    def __init__(self):\n        self.dataclose = self.datas[0].close\n        print('buflen %d' % self.dataclose.buflen())\n        self.order = None\n        self.buyprice = None\n        self.buycomm = None\n\n        # Add a MovingAverageSimple indicator\n        self.sma = bt.indicators.SimpleMovingAverage(\n            self.datas[0], period=self.params.maperiod)\n        \n        # Indicators for the plotting show\n        bt.indicators.ExponentialMovingAverage(self.datas[0], period=25)\n        bt.indicators.WeightedMovingAverage(self.datas[0], period=25,\n                                            subplot=True)\n        bt.indicators.StochasticSlow(self.datas[0])\n        bt.indicators.MACDHisto(self.datas[0])\n        rsi = bt.indicators.RSI(self.datas[0])\n        bt.indicators.SmoothedMovingAverage(rsi, period=10)\n        bt.indicators.ATR(self.datas[0], plot=False)\n\n    def notify_order(self, order):\n        if order.status in [order.Submitted, order.Accepted]:\n            return\n        if order.status in [order.Completed]:\n            if order.isbuy():\n                self.log('BUY EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f' %\n                        (order.executed.price,\n                        order.executed.value,\n                        order.executed.comm))\n                self.buyprice = order.executed.price\n                self.buycomm = order.executed.comm\n            elif order.issell():\n                self.log('SELL EXECUTED, Price: %.2f, Cost: %.2f, Comm %.2f' %\n                         (order.executed.price,\n                          order.executed.value,\n                          order.executed.comm))\n            self.bar_executed = len(self)\n        elif order.status in [order.Canceled, order.Margin, order.Rejected]:\n            self.log('Order Canceled/Margin/Rejected')\n        self.order = None\n\n    def notify_trade(self, trade):\n        if not trade.isclosed:\n            return\n\n        self.log('OPERATION PROFIT, GROSS %.2f, NET %.2f' %\n                 (trade.pnl, trade.pnlcomm))\n\n    def next(self):\n        self.log('Close, %.2f' % self.dataclose[0])\n        if self.order:\n            return\n        if not self.position:\n            if self.dataclose[0] &gt; self.sma[0]:\n                self.log('BUY CREATE, %.2f' % self.dataclose[0])\n                print(self.buy())\n        else:\n            if self.dataclose[0] &lt; self.sma[0]:\n                self.log('SELL CREATE, %.2f' % self.dataclose[0])\n                self.order = self.sell()\n\n\ncerebro = bt.Cerebro()\n\ncerebro.addstrategy(TestStrategy)\n\ndata = bt.feeds.YahooFinanceCSVData(\n        dataname=\"datas/orcl-1995-2014.txt\",\n        # Do not pass values before this date\n        fromdate=datetime.datetime(2000, 1, 1),\n        # Do not pass values after this date\n        todate=datetime.datetime(2000, 12, 31),\n        reverse=False)\n\n# print(data)\n\ncerebro.adddata(data)\n\ncerebro.broker.setcash(100000.0)\ncerebro.addsizer(bt.sizers.FixedSize, stake=10)\ncerebro.broker.setcommission(commission=0.001)\n\nprint('组合初始值: %.2f' % cerebro.broker.getvalue())\n\nresult=cerebro.run()\n\nprint('组合终结值: %.2f' % cerebro.broker.getvalue())\ncerebro.plot()\n\n组合初始值: 100000.00\nbuflen 252\n2000-02-18, Close, 26.05\n2000-02-22, Close, 26.38\n2000-02-22, BUY CREATE, 26.38\nRef: 81\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730172.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-02-23, BUY EXECUTED, Price: 26.77, Cost: 267.70, Comm 0.27\n2000-02-23, Close, 28.05\n2000-02-24, Close, 27.55\n2000-02-25, Close, 31.41\n2000-02-28, Close, 30.52\n2000-02-29, Close, 33.02\n2000-03-01, Close, 31.80\n2000-03-02, Close, 30.47\n2000-03-03, Close, 33.36\n2000-03-06, Close, 33.69\n2000-03-07, Close, 33.33\n2000-03-08, Close, 36.97\n2000-03-09, Close, 37.36\n2000-03-10, Close, 36.30\n2000-03-13, Close, 35.02\n2000-03-14, Close, 34.25\n2000-03-15, Close, 34.97\n2000-03-16, Close, 36.44\n2000-03-17, Close, 35.50\n2000-03-20, Close, 34.75\n2000-03-21, Close, 35.89\n2000-03-22, Close, 37.39\n2000-03-23, Close, 38.64\n2000-03-24, Close, 38.69\n2000-03-27, Close, 39.33\n2000-03-28, Close, 38.50\n2000-03-29, Close, 36.69\n2000-03-30, Close, 34.88\n2000-03-30, SELL CREATE, 34.88\n2000-03-31, SELL EXECUTED, Price: 35.66, Cost: 267.70, Comm 0.36\n2000-03-31, OPERATION PROFIT, GROSS 88.90, NET 88.28\n2000-03-31, Close, 34.72\n2000-04-03, Close, 34.19\n2000-04-04, Close, 33.77\n2000-04-05, Close, 34.80\n2000-04-06, Close, 36.55\n2000-04-06, BUY CREATE, 36.55\nRef: 83\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730216.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-04-07, BUY EXECUTED, Price: 37.22, Cost: 372.20, Comm 0.37\n2000-04-07, Close, 38.75\n2000-04-10, Close, 36.69\n2000-04-11, Close, 34.41\n2000-04-11, SELL CREATE, 34.41\n2000-04-12, SELL EXECUTED, Price: 34.66, Cost: 372.20, Comm 0.35\n2000-04-12, OPERATION PROFIT, GROSS -25.60, NET -26.32\n2000-04-12, Close, 32.52\n2000-04-13, Close, 31.99\n2000-04-14, Close, 27.80\n2000-04-17, Close, 33.27\n2000-04-18, Close, 35.11\n2000-04-18, BUY CREATE, 35.11\nRef: 85\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730228.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-04-19, BUY EXECUTED, Price: 34.97, Cost: 349.70, Comm 0.35\n2000-04-19, Close, 33.16\n2000-04-19, SELL CREATE, 33.16\n2000-04-20, SELL EXECUTED, Price: 32.83, Cost: 349.70, Comm 0.33\n2000-04-20, OPERATION PROFIT, GROSS -21.40, NET -22.08\n2000-04-20, Close, 31.49\n2000-04-24, Close, 32.22\n2000-04-25, Close, 33.61\n2000-04-26, Close, 32.11\n2000-04-27, Close, 34.38\n2000-04-27, BUY CREATE, 34.38\nRef: 87\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730237.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-04-28, BUY EXECUTED, Price: 34.91, Cost: 349.10, Comm 0.35\n2000-04-28, Close, 35.55\n2000-05-01, Close, 35.44\n2000-05-02, Close, 34.61\n2000-05-03, Close, 33.72\n2000-05-04, Close, 33.02\n2000-05-04, SELL CREATE, 33.02\n2000-05-05, SELL EXECUTED, Price: 32.91, Cost: 349.10, Comm 0.33\n2000-05-05, OPERATION PROFIT, GROSS -20.00, NET -20.68\n2000-05-05, Close, 34.16\n2000-05-05, BUY CREATE, 34.16\nRef: 89\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730245.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-05-08, BUY EXECUTED, Price: 33.49, Cost: 334.90, Comm 0.33\n2000-05-08, Close, 32.16\n2000-05-08, SELL CREATE, 32.16\n2000-05-09, SELL EXECUTED, Price: 32.77, Cost: 334.90, Comm 0.33\n2000-05-09, OPERATION PROFIT, GROSS -7.20, NET -7.86\n2000-05-09, Close, 32.02\n2000-05-10, Close, 30.08\n2000-05-11, Close, 32.19\n2000-05-12, Close, 32.99\n2000-05-15, Close, 34.25\n2000-05-15, BUY CREATE, 34.25\nRef: 91\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730255.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-05-16, BUY EXECUTED, Price: 34.52, Cost: 345.20, Comm 0.35\n2000-05-16, Close, 35.22\n2000-05-17, Close, 34.77\n2000-05-18, Close, 32.49\n2000-05-18, SELL CREATE, 32.49\n2000-05-19, SELL EXECUTED, Price: 32.02, Cost: 345.20, Comm 0.32\n2000-05-19, OPERATION PROFIT, GROSS -25.00, NET -25.67\n2000-05-19, Close, 31.16\n2000-05-22, Close, 30.16\n2000-05-23, Close, 27.85\n2000-05-24, Close, 28.57\n2000-05-25, Close, 29.55\n2000-05-26, Close, 29.80\n2000-05-30, Close, 32.99\n2000-05-30, BUY CREATE, 32.99\nRef: 93\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730270.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-05-31, BUY EXECUTED, Price: 32.58, Cost: 325.80, Comm 0.33\n2000-05-31, Close, 31.97\n2000-06-01, Close, 34.63\n2000-06-02, Close, 35.66\n2000-06-05, Close, 36.00\n2000-06-06, Close, 34.27\n2000-06-07, Close, 35.58\n2000-06-08, Close, 36.64\n2000-06-09, Close, 36.77\n2000-06-12, Close, 35.83\n2000-06-13, Close, 36.33\n2000-06-14, Close, 35.13\n2000-06-15, Close, 36.69\n2000-06-16, Close, 36.41\n2000-06-19, Close, 38.25\n2000-06-20, Close, 38.27\n2000-06-21, Close, 38.33\n2000-06-22, Close, 36.25\n2000-06-22, SELL CREATE, 36.25\n2000-06-23, SELL EXECUTED, Price: 35.94, Cost: 325.80, Comm 0.36\n2000-06-23, OPERATION PROFIT, GROSS 33.60, NET 32.91\n2000-06-23, Close, 35.36\n2000-06-26, Close, 36.77\n2000-06-26, BUY CREATE, 36.77\nRef: 95\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730297.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-06-27, BUY EXECUTED, Price: 36.64, Cost: 366.40, Comm 0.37\n2000-06-27, Close, 36.58\n2000-06-27, SELL CREATE, 36.58\n2000-06-28, SELL EXECUTED, Price: 36.50, Cost: 366.40, Comm 0.36\n2000-06-28, OPERATION PROFIT, GROSS -1.40, NET -2.13\n2000-06-28, Close, 36.89\n2000-06-28, BUY CREATE, 36.89\nRef: 97\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730299.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-06-29, BUY EXECUTED, Price: 36.50, Cost: 365.00, Comm 0.36\n2000-06-29, Close, 35.97\n2000-06-29, SELL CREATE, 35.97\n2000-06-30, SELL EXECUTED, Price: 35.75, Cost: 365.00, Comm 0.36\n2000-06-30, OPERATION PROFIT, GROSS -7.50, NET -8.22\n2000-06-30, Close, 37.39\n2000-06-30, BUY CREATE, 37.39\nRef: 99\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730301.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-07-03, BUY EXECUTED, Price: 36.08, Cost: 360.80, Comm 0.36\n2000-07-03, Close, 35.66\n2000-07-03, SELL CREATE, 35.66\n2000-07-05, SELL EXECUTED, Price: 34.16, Cost: 360.80, Comm 0.34\n2000-07-05, OPERATION PROFIT, GROSS -19.20, NET -19.90\n2000-07-05, Close, 32.16\n2000-07-06, Close, 33.63\n2000-07-07, Close, 33.75\n2000-07-10, Close, 32.97\n2000-07-11, Close, 32.16\n2000-07-12, Close, 33.22\n2000-07-13, Close, 33.69\n2000-07-14, Close, 33.86\n2000-07-17, Close, 33.86\n2000-07-18, Close, 32.99\n2000-07-19, Close, 32.80\n2000-07-20, Close, 34.75\n2000-07-20, BUY CREATE, 34.75\nRef: 101\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730321.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-07-21, BUY EXECUTED, Price: 34.44, Cost: 344.40, Comm 0.34\n2000-07-21, Close, 33.55\n2000-07-21, SELL CREATE, 33.55\n2000-07-24, SELL EXECUTED, Price: 34.30, Cost: 344.40, Comm 0.34\n2000-07-24, OPERATION PROFIT, GROSS -1.40, NET -2.09\n2000-07-24, Close, 33.36\n2000-07-25, Close, 33.80\n2000-07-25, BUY CREATE, 33.80\nRef: 103\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730326.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-07-26, BUY EXECUTED, Price: 33.27, Cost: 332.70, Comm 0.33\n2000-07-26, Close, 34.13\n2000-07-27, Close, 33.38\n2000-07-27, SELL CREATE, 33.38\n2000-07-28, SELL EXECUTED, Price: 33.41, Cost: 332.70, Comm 0.33\n2000-07-28, OPERATION PROFIT, GROSS 1.40, NET 0.73\n2000-07-28, Close, 32.19\n2000-07-31, Close, 33.44\n2000-07-31, BUY CREATE, 33.44\nRef: 105\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730332.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-08-01, BUY EXECUTED, Price: 33.44, Cost: 334.40, Comm 0.33\n2000-08-01, Close, 32.52\n2000-08-01, SELL CREATE, 32.52\n2000-08-02, SELL EXECUTED, Price: 32.47, Cost: 334.40, Comm 0.32\n2000-08-02, OPERATION PROFIT, GROSS -9.70, NET -10.36\n2000-08-02, Close, 32.52\n2000-08-03, Close, 34.44\n2000-08-03, BUY CREATE, 34.44\nRef: 107\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730335.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-08-04, BUY EXECUTED, Price: 34.83, Cost: 348.30, Comm 0.35\n2000-08-04, Close, 36.27\n2000-08-07, Close, 36.41\n2000-08-08, Close, 36.91\n2000-08-09, Close, 36.19\n2000-08-10, Close, 35.61\n2000-08-11, Close, 36.08\n2000-08-14, Close, 36.64\n2000-08-15, Close, 36.14\n2000-08-16, Close, 36.11\n2000-08-17, Close, 37.33\n2000-08-18, Close, 36.16\n2000-08-21, Close, 37.00\n2000-08-22, Close, 37.16\n2000-08-23, Close, 36.86\n2000-08-24, Close, 37.66\n2000-08-25, Close, 37.64\n2000-08-28, Close, 38.58\n2000-08-29, Close, 39.03\n2000-08-30, Close, 39.25\n2000-08-31, Close, 40.44\n2000-09-01, Close, 41.19\n2000-09-05, Close, 40.50\n2000-09-06, Close, 39.69\n2000-09-07, Close, 40.56\n2000-09-08, Close, 38.50\n2000-09-08, SELL CREATE, 38.50\n2000-09-11, SELL EXECUTED, Price: 38.28, Cost: 348.30, Comm 0.38\n2000-09-11, OPERATION PROFIT, GROSS 34.50, NET 33.77\n2000-09-11, Close, 37.11\n2000-09-12, Close, 35.30\n2000-09-13, Close, 36.39\n2000-09-14, Close, 37.78\n2000-09-15, Close, 34.83\n2000-09-18, Close, 34.01\n2000-09-19, Close, 35.27\n2000-09-20, Close, 35.55\n2000-09-21, Close, 35.11\n2000-09-22, Close, 35.91\n2000-09-25, Close, 35.02\n2000-09-26, Close, 35.33\n2000-09-27, Close, 35.52\n2000-09-28, Close, 36.24\n2000-09-28, BUY CREATE, 36.24\nRef: 109\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730391.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-09-29, BUY EXECUTED, Price: 36.18, Cost: 361.80, Comm 0.36\n2000-09-29, Close, 35.02\n2000-09-29, SELL CREATE, 35.02\n2000-10-02, SELL EXECUTED, Price: 35.47, Cost: 361.80, Comm 0.35\n2000-10-02, OPERATION PROFIT, GROSS -7.10, NET -7.82\n2000-10-02, Close, 35.02\n2000-10-03, Close, 30.91\n2000-10-04, Close, 30.30\n2000-10-05, Close, 30.38\n2000-10-06, Close, 30.08\n2000-10-09, Close, 29.69\n2000-10-10, Close, 28.74\n2000-10-11, Close, 27.69\n2000-10-12, Close, 28.02\n2000-10-13, Close, 31.69\n2000-10-16, Close, 30.74\n2000-10-17, Close, 29.96\n2000-10-18, Close, 29.85\n2000-10-19, Close, 32.36\n2000-10-19, BUY CREATE, 32.36\nRef: 111\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730412.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-10-20, BUY EXECUTED, Price: 32.13, Cost: 321.30, Comm 0.32\n2000-10-20, Close, 31.35\n2000-10-23, Close, 30.30\n2000-10-24, Close, 31.85\n2000-10-25, Close, 30.58\n2000-10-26, Close, 30.30\n2000-10-27, Close, 30.41\n2000-10-30, Close, 28.13\n2000-10-30, SELL CREATE, 28.13\n2000-10-31, SELL EXECUTED, Price: 29.02, Cost: 321.30, Comm 0.29\n2000-10-31, OPERATION PROFIT, GROSS -31.10, NET -31.71\n2000-10-31, Close, 29.35\n2000-11-01, Close, 27.91\n2000-11-02, Close, 26.30\n2000-11-03, Close, 26.96\n2000-11-06, Close, 24.85\n2000-11-07, Close, 23.63\n2000-11-08, Close, 22.07\n2000-11-09, Close, 24.18\n2000-11-10, Close, 22.63\n2000-11-13, Close, 22.01\n2000-11-14, Close, 25.24\n2000-11-15, Close, 25.68\n2000-11-16, Close, 24.35\n2000-11-17, Close, 25.63\n2000-11-17, BUY CREATE, 25.63\nRef: 113\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730441.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-11-20, BUY EXECUTED, Price: 21.63, Cost: 216.30, Comm 0.22\n2000-11-20, Close, 22.01\n2000-11-20, SELL CREATE, 22.01\n2000-11-21, SELL EXECUTED, Price: 22.07, Cost: 216.30, Comm 0.22\n2000-11-21, OPERATION PROFIT, GROSS 4.40, NET 3.96\n2000-11-21, Close, 21.24\n2000-11-22, Close, 19.85\n2000-11-24, Close, 21.46\n2000-11-27, Close, 20.57\n2000-11-28, Close, 20.15\n2000-11-29, Close, 20.35\n2000-11-30, Close, 23.57\n2000-11-30, BUY CREATE, 23.57\nRef: 115\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730454.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-12-01, BUY EXECUTED, Price: 23.46, Cost: 234.60, Comm 0.23\n2000-12-01, Close, 23.52\n2000-12-04, Close, 25.07\n2000-12-05, Close, 28.02\n2000-12-06, Close, 26.85\n2000-12-07, Close, 25.18\n2000-12-08, Close, 26.74\n2000-12-11, Close, 28.41\n2000-12-12, Close, 27.35\n2000-12-13, Close, 25.24\n2000-12-14, Close, 24.46\n2000-12-14, SELL CREATE, 24.46\n2000-12-15, SELL EXECUTED, Price: 26.18, Cost: 234.60, Comm 0.26\n2000-12-15, OPERATION PROFIT, GROSS 27.20, NET 26.70\n2000-12-15, Close, 25.41\n2000-12-15, BUY CREATE, 25.41\nRef: 117\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730469.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-12-18, BUY EXECUTED, Price: 26.68, Cost: 266.80, Comm 0.27\n2000-12-18, Close, 28.46\n2000-12-19, Close, 27.24\n2000-12-20, Close, 25.35\n2000-12-20, SELL CREATE, 25.35\n2000-12-21, SELL EXECUTED, Price: 24.74, Cost: 266.80, Comm 0.25\n2000-12-21, OPERATION PROFIT, GROSS -19.40, NET -19.91\n2000-12-21, Close, 26.24\n2000-12-21, BUY CREATE, 26.24\nRef: 119\nOrdType: 0\nOrdType: Buy\nStatus: 1\nStatus: Submitted\nSize: 10\nPrice: None\nPrice Limit: None\nTrailAmount: None\nTrailPercent: None\nExecType: 0\nExecType: Market\nCommInfo: None\nEnd of Session: 730475.9999999999\nInfo: AutoOrderedDict()\nBroker: None\nAlive: True\n2000-12-22, BUY EXECUTED, Price: 27.02, Cost: 270.20, Comm 0.27\n2000-12-22, Close, 28.35\n2000-12-26, Close, 27.52\n2000-12-27, Close, 27.30\n2000-12-28, Close, 27.63\n2000-12-29, Close, 25.85\n2000-12-29, SELL CREATE, 25.85\n组合终结值: 99969.64\n\n\n\n\n\n\n\n\n[[&lt;Figure size 640x480 with 8 Axes&gt;]]"
  },
  {
    "objectID": "posts/2021-08-18-setting-up-your-business.html",
    "href": "posts/2021-08-18-setting-up-your-business.html",
    "title": "CH4-建立你的事业",
    "section": "",
    "text": "在中国似乎没有这种选择，很难成为“自营交易公司”的员工。这种公司本身就几乎没有。"
  },
  {
    "objectID": "posts/2021-08-18-setting-up-your-business.html#业务结构零售还是自营",
    "href": "posts/2021-08-18-setting-up-your-business.html#业务结构零售还是自营",
    "title": "CH4-建立你的事业",
    "section": "",
    "text": "在中国似乎没有这种选择，很难成为“自营交易公司”的员工。这种公司本身就几乎没有。"
  },
  {
    "objectID": "posts/2021-08-18-setting-up-your-business.html#选择一家零售经纪公司",
    "href": "posts/2021-08-18-setting-up-your-business.html#选择一家零售经纪公司",
    "title": "CH4-建立你的事业",
    "section": "选择一家零售经纪公司",
    "text": "选择一家零售经纪公司\n主要看佣金，其他的诸如交易速度等等，在现在这个时代已经不是问题了。 * 相对较低的佣金 * 可交易金融工具品种广泛 * 有足够深度的流动资金池 * 获取实时数据和传送指令的API\n这个富途居然有接口https://openapi.futunn.com/futu-api-doc/intro/intro.html\n中泰XTP"
  },
  {
    "objectID": "posts/2021-08-18-setting-up-your-business.html#设备",
    "href": "posts/2021-08-18-setting-up-your-business.html#设备",
    "title": "CH4-建立你的事业",
    "section": "设备",
    "text": "设备\n\n性能良好的电脑\n高速的网络\n防中断电源UPS\n专业实时新闻工具\n多屏幕扩大可视空间"
  },
  {
    "objectID": "posts/2024-08-30-Algorithmic-Trading-4.html",
    "href": "posts/2024-08-30-Algorithmic-Trading-4.html",
    "title": "算法交易：制胜策略与原理(欧内斯特·陈)之四",
    "section": "",
    "text": "相对于应用均值回归策略交易一对股票而言，要想做到持续盈利其实是很困难的，除非你对每一个公司都有一个基本的认知，同时，你能够在某个公司相关的坏消息公布于众之前平仓离场。\n对可以配对交易的ETF基金的选择过程相当简单：我们只需要寻找那些暴露于共同的经济因素之下的ETF基金即可。除了国家级别的ETF基金以外，行业ETF基金也拥有一个能够发现具有协整性质资产的肥沃的土壤。例如，零售业基金RTH与消费品基金XLP就具有协整性质。随着对多少具有相同属性的行业ETF基金的增值功能的探索，相应的配对交易规模就稳步增长了。\n我所喜欢的另一个ETF基金的配对交易是大宗商品之ETF基金和由生产这些商品的公司股票所构成的ETF基金之间所构建的投资组合。黄金相关的GLD基金与金矿金矿股票指数型基金GDX就是一个很好的例子，其基本原理是：由于金矿公司的主要资产是黄金，因此其股票的价值应该与黄金现货价格具有协整关系，而且事实上，两者之间的协整关系一直持续到2008年7月14日左右。\n石油价格和黄金价格，以及与黄金企业股指之间的协整关系又是怎样的呢？显然，它们有很多相关性，而且事实证明：当石油价格变得昂贵，我就要花费更多的资金去开采黄金，如此，开采企业的利润就会减少，进而导致相关企业的股票价格相对于黄金现货价格来说表现不佳（“古怪黄金界”，2011）。"
  },
  {
    "objectID": "posts/2024-08-30-Algorithmic-Trading-4.html#日间均值回归交易策略缺口买入模式",
    "href": "posts/2024-08-30-Algorithmic-Trading-4.html#日间均值回归交易策略缺口买入模式",
    "title": "算法交易：制胜策略与原理(欧内斯特·陈)之四",
    "section": "日间均值回归交易策略：缺口买入模式",
    "text": "日间均值回归交易策略：缺口买入模式\n\n（1）于靠近开盘时刻，选择所有的自前一天的最低价至今日的开盘价之间的收益率低于一个标准差的股票。标准差的计算依据的是90天的日间收盘价的收益率，而被选中的股票就是“缺口”型的。\n（2）缩小所选股票的范围，留下那些开盘价格高于收盘价20日移动均线的股票。\n（3）在剩余股票中，买入10只，要求是：此类股票价格与前一日之最低价格相比较而得出的收益率最低。如果能够买入的数量少于10只股票，那就买入上述的所有股票。\n（4）在收盘之前清算所有股票。 上述这种策略的基本原理是：当股指期货的价格于开盘之前下跌，那么，由于市场上的恐慌情绪，某些股票于开盘之时会被不成比例地抛售。但是，一旦这种恐慌性抛售结束，股票于当天将会逐步地升值。\n\n上述第二个规则通常是非常有用的均值回归型的交易策略：它基本上是一个动量过滤法则再叠加一个均值回归策略，这是我们经常重复的一种技术。在通常情况之下，那些价格跌了“一点点”的股票相较于那些跌了“很多”的股票而言，会有一个更好的反转契机，因为后者经常会面临负面新闻的冲击，比如营业收入公告不如人意等。由负面消息所造成的价格下降是不太可能恢复的。\n\n# Example 4.1: Buy-on-Gap Model on SPX Stocks\n\nimport numpy as np\nimport pandas as pd\n#import matplotlib.pyplot as plt\n#import statsmodels.formula.api as sm\n#import statsmodels.tsa.stattools as ts\n#import statsmodels.tsa.vector_ar.vecm as vm\n\ntopN=10 # Max number of positions 最大持仓数量\nentryZscore=1 #用于计算买入价格的Z分数\nlookback=20 # for MA 用于计算移动平均的天数\n# 它需要三个T×N型的输入数组，即op数组、lo数组和cl数组，其中，T代表的是天数，N代表的是仓中的股票数量，同时，op包含的是每日的开盘价格，lo包含的是每日的最低价格，而cl是每日的收盘价。\nop=pd.read_csv('datas/inputDataOHLCDaily_20120424_op.csv')\nhi=pd.read_csv('datas/inputDataOHLCDaily_20120424_hi.csv')\nlo=pd.read_csv('datas/inputDataOHLCDaily_20120424_lo.csv')\ncl=pd.read_csv('datas/inputDataOHLCDaily_20120424_cl.csv')\n\nstocks=pd.read_csv('datas/inputDataOHLCDaily_20120424_stocks.csv')\n\nop['Var1']=pd.to_datetime(op['Var1'],  format='%Y%m%d').dt.date # remove HH:MM:SS\nop.columns=np.insert(stocks.values, 0, 'Date')\nop.set_index('Date', inplace=True)\n# print(op)\n\nhi['Var1']=pd.to_datetime(hi['Var1'],  format='%Y%m%d').dt.date # remove HH:MM:SS\nhi.columns=np.insert(stocks.values, 0, 'Date')\nhi.set_index('Date', inplace=True)\n\nlo['Var1']=pd.to_datetime(lo['Var1'],  format='%Y%m%d').dt.date # remove HH:MM:SS\nlo.columns=np.insert(stocks.values, 0, 'Date')\nlo.set_index('Date', inplace=True)\n\ncl['Var1']=pd.to_datetime(cl['Var1'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ncl.columns=np.insert(stocks.values, 0, 'Date')\ncl.set_index('Date', inplace=True)\n# 计算过去90天的收盘价标准差（stdretC2C90d）\nstdretC2C90d=cl.pct_change().rolling(90).std().shift(1)\n# 计算买入价格（buyPrice），即最低价减去一个Z分数乘以过去90天的收盘价标准差。\nbuyPrice=lo.shift()*(1-entryZscore*stdretC2C90d)\n# 计算开盘价与前一天的最低价之间的收益差（retGap）\nretGap=(op-lo.shift())/lo.shift()\n# 计算移动平均\nma=cl.rolling(lookback).mean()\n# 初始化持仓表（positionsTable）并遍历每一天\npositionsTable=np.zeros(retGap.shape)\n\n\nfor t in np.arange(1, cl.shape[0]):\n    # 对于每一天，找到满足以下条件的股票：收益差有限、开盘价低于买入价格且高于移动平均\n    hasData=np.where(np.isfinite(retGap.iloc[t, :]) & (op.iloc[t, :] &lt; buyPrice.iloc[t, :]).values & (op.iloc[t, :] &gt; ma.iloc[t, :]).values)\n    hasData=hasData[0]\n    if len(hasData)&gt;0:\n        # 对满足条件的股票按收益差排序，并将前topN个股票的持仓表位置设为1\n        idxSort=np.argsort(retGap.iloc[t, hasData])  \n        positionsTable[t, hasData[idxSort.values[np.arange(-np.min((topN, len(idxSort))),0)]]]=1\n\nretO2C=(cl-op)/op\n    \npnl=np.sum(positionsTable*retO2C, axis=1) # daily P&L of the strategy\nret=pnl/topN\n(np.cumprod(1+ret)-1).plot()\nprint('APR=%f Sharpe=%f' % (np.prod(1+ret)**(252/len(ret))-1, np.sqrt(252)*np.mean(ret)/np.std(ret)))\n\nAPR=0.054392 Sharpe=1.163273\n\n\n\n\n\n\n\n\n\n你可以同时做多，且做空某个公司的股票；或者，你可以做多股票，同时用股指期货的空头进行对冲；你也可以购买更大数量的股票，但同时限制同一行业的股票数量；你还可以于市场开盘时刻之外扩展购买的期限；你也可以实施盘中“抢帽子”交易。但是，这里有一个重要的信息：以日间棒线为样本、不具有均值回归属性的价格系列可以在特定的时期表现出强烈的均值回归属性，这是较短时间尺度所呈现的季节性的工作原理。"
  },
  {
    "objectID": "posts/2024-08-30-Algorithmic-Trading-4.html#抢帽子交易",
    "href": "posts/2024-08-30-Algorithmic-Trading-4.html#抢帽子交易",
    "title": "算法交易：制胜策略与原理(欧内斯特·陈)之四",
    "section": "“抢帽子”交易",
    "text": "“抢帽子”交易\n“抢帽子”交易是一种短线交易策略，通常是指在股票市场中，投资者试图通过在极短的时间内买入和卖出股票来获利。这种策略的名称来源于一种想象中的场景，即投资者像抢帽子一样迅速地买入和卖出股票。\n“抢帽子”交易的特点包括：\n\n高频交易：这种策略通常涉及大量的交易，而且交易频率非常高。投资者需要密切关注市场动态，并在短时间内做出决策。\n短线操作：由于交易频率高，投资者通常只持有股票很短的时间，可能是几分钟甚至几秒钟。因此，这种策略需要投资者具备快速反应和决策的能力。\n利用市场波动：抢帽子交易者通常会利用市场的短期波动来获利。他们会在股价上涨时迅速买入，然后在股价达到高点时迅速卖出；或者在股价下跌时迅速卖出，然后在股价触底时迅速买入。\n风险较高：由于抢帽子交易涉及高频交易和短线操作，因此风险也相对较高。如果市场波动剧烈或者投资者的决策出现失误，可能会导致巨大的损失。\n需要专业技能：抢帽子交易需要投资者具备丰富的市场经验和专业知识，以便能够准确判断市场趋势和时机。此外，投资者还需要具备良好的心理素质和风险控制能力。\n\n需要注意的是，“抢帽子”交易在中国是被禁止的。根据中国证监会发布的《关于加强对利用“荐股软件”从事证券投资咨询业务监管的暂行规定》，投资者在证券交易活动中不得有“抢帽子”交易等操纵市场的行为。因此，投资者应该遵守相关法律法规，远离任何非法的交易行为。"
  },
  {
    "objectID": "posts/2024-08-30-Algorithmic-Trading-4.html#etf基金与成分股之间的套利模式",
    "href": "posts/2024-08-30-Algorithmic-Trading-4.html#etf基金与成分股之间的套利模式",
    "title": "算法交易：制胜策略与原理(欧内斯特·陈)之四",
    "section": "ETF基金与成分股之间的套利模式",
    "text": "ETF基金与成分股之间的套利模式\n许多读者对“指数套利”的相关策略应该是很熟悉的，即在股票的投资组合与指数期货之间构建一种对冲的关系，并按照它们之间的价差进行相关的交易。\n高频交易员已经能够利用日间交易中的两个缺陷来计算股指期货与成分股之间的套利比率及相关指标，其中第一个缺陷是：许多主要的指数，包括道琼斯指数（DowJones）、标准普尔指数（S&P）、纳斯达克指数（Nasdaq）和罗素指数（Russell），其计算程序只使用初级市场的交易数据（详见第1章对初级市场与综合股票价格的论述），而这只代表了不到30%的被交易的股票（阿诺、萨鲁奇，2012）；第二个缺陷是：相关该指数每隔几秒钟就要更新一次。前述两个缺陷会导致由一组股票所构成的“篮子”证券的价格以及股指本身的价格在不断地被更新，从而生成它们之间的市值差异。因此，对股指期货价值的预期较之瞬时变化的股票市值而言，会有稍许的延迟。如果股指期货的价值高于成分股的瞬时市值，那我们就可以做空股指期货，反之亦然。那么，我们在哪里可以发现这个事实呢？是瞬时变化的股票市值吗？当然，我们需要从所有美国证交所和电子交易平台（ECN）（而不是证券行业自动化公司，SIAC）订阅直接的数据，同时，我们要在相关的交易场所对指数相关的所有股票的交易价格实施监控，而且，延迟的时间要以毫秒来计算。所以，没有人能说“高频交易”是件很容易的事情！\n我们将选择一年的数据（在我们的例子中，我们选择的是2007年1月1日至2007年12月31日的数据）作为训练集，然后以约翰森检验模式寻找所有那些与SPY基金的协整属性达到90%概率之上的股票，接下来，我们用这些股票构建一个投资组合，且对每只股票平均分配资金，再用约翰森检验模式进行测试，确认此多头形式的投资组合是否仍然与SPY基金具有协整关系。前述这一步是必要的，因为一个任意金额以相等权重将资本平均分配给每只股票，而由这些股票所构建的组合并不一定与SPY基金具有协整关系，即使每个成分股都与SPY基金具有协整关系，投资组合的情况也是如此。我们在第二个测试当中使用了价格的对数形式，因为我们希望每天都调整这个组合，进而使每个股票的资本都是恒定的（详见第3章的讨论内容）。在确认相关的协整属性之后，我们就可以回测在第2章所描述的线性均值回归的交易策略，\n\n# Example 4.2: Arbitrage between SPY and Its Component Stocks\n\nimport numpy as np\nimport pandas as pd\n#import matplotlib.pyplot as plt\n#import statsmodels.formula.api as sm\n#import statsmodels.tsa.stattools as ts\nimport statsmodels.tsa.vector_ar.vecm as vm\n\n# Stocks\ncl=pd.read_csv('datas/inputDataOHLCDaily_20120424_cl.csv')\nstocks=pd.read_csv('datas/inputDataOHLCDaily_20120424_stocks.csv')\n\ncl['Var1']=pd.to_datetime(cl['Var1'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ncl.columns=np.insert(stocks.values, 0, 'Date')\ncl.set_index('Date', inplace=True)\n\n# ETFs\ncl_etf=pd.read_csv('datas/inputData_ETF_cl.csv')\netfs=pd.read_csv('datas/inputData_ETF_stocks.csv')\n\ncl_etf['Var1']=pd.to_datetime(cl_etf['Var1'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ncl_etf.columns=np.insert(etfs.values, 0, 'Date')\ncl_etf.set_index('Date', inplace=True)\n\n# Merge on common dates\ndf=pd.merge(cl, cl_etf, how='inner', on='Date')\n\ncl_stocks=df[cl.columns]\ncl_etf=df[cl_etf.columns]\n\n# Use SPY only\ncl_etf=cl_etf['SPY'] # This turns cl_etf into Series\n\ntrainDataIdx=df.index[(df.index &gt; pd.to_datetime(\"2007-01-01\").date()) & (df.index &lt;= pd.to_datetime(\"2007-12-31\").date())]\ntestDataIdx =df.index[df.index &gt; pd.to_datetime(\"2007-12-31\").date()]\n\n# 我们基于约翰森检验的原理对SPY基金的每只成分股与SPY基金本身之间的协整关系进行测试，之后我们发现：有98只股票分别与SPY基金具有协整关系。\n# 现在，我们可以将所有与SPY基金具有协整关系的股票之多头形式构成一个投资组合，且配置同样的资本，但是，我们还必须测试这个投资组合与SPY基金的协整关系，\n\nisCoint=np.full(stocks.shape[1], False)\nfor s in range(stocks.shape[1]):\n    # Combine the two time series into a matrix y2 for input into Johansen test\n    y2=pd.concat([cl_stocks.loc[trainDataIdx].iloc[:, s], cl_etf.loc[trainDataIdx]], axis=1)\n    y2=y2.loc[y2.notnull().all(axis=1),]\n    \n    if (y2.shape[0] &gt; 250):\n        # Johansen test 对每个成分股与SPY进行协整检验\n        result=vm.coint_johansen(y2.values, det_order=0, k_ar_diff=1)\n        if (result.lr1[0] &gt; result.cvt[0,0]): # 如果存在协整关系，则将对应的成分股标记为True\n            isCoint[s]=True\n        \nprint(isCoint.sum())\n\nyN=cl_stocks.loc[trainDataIdx, isCoint]\n# 计算仅做多组合的净市场价值\nlogMktVal_long=np.sum(np.log(yN), axis=1) # The net market value of the long-only portfolio is same as the \"spread\"\n\n# Confirm that the portfolio cointegrates with SPY 确认组合与SPY的协整关系\nytest=pd.concat([logMktVal_long, np.log(cl_etf.loc[trainDataIdx])], axis=1)\nresult=vm.coint_johansen(ytest, det_order=0, k_ar_diff=1)\nprint(result.lr1)\nprint(result.cvt)\nprint(result.lr2)\nprint(result.cvm)\n# 上述约翰森测试表明股票的多头组合与SPY基金的协整属性达到95%以上的概率。所以，我们可以构建一个包括股票和SPY基金在内的多单-空单相间的平稳投资组合，\n# 同时，我们可以使用约翰森检验中的特征向量来确定SPY基金与股票投资组合的权重（事实上，即使二者之间存在协整关系，我们也可以选择特征向量矩阵eigenmatrix中的、位于第一列的最大值来构建平稳的投资组合）。\n\n#Apply linear mean-reversion model on test set 应用线性均值回归模型\nyNplus=pd.concat([cl_stocks.loc[testDataIdx, isCoint], pd.DataFrame(cl_etf.loc[testDataIdx])], axis=1)  # Array of stock and ETF prices\n# 计算测试集中的投资组合权重\nweights=np.column_stack((np.full((testDataIdx.shape[0], isCoint.sum()), result.evec[0,0]), np.full((testDataIdx.shape[0], 1), result.evec[1, 0]))) # Array of log market value of stocks and ETF's\n# 计算投资组合的净市场价值\nlogMktVal=np.sum(weights*np.log(yNplus), axis=1) # Log market value of long-short portfolio\n\nlookback=5\n# 计算投资组合的单位资本投入\nnumUnits =-(logMktVal-logMktVal.rolling(lookback).mean())/logMktVal.rolling(lookback).std() # capital invested in portfolio in dollars.  movingAvg and movingStd are functions from epchan.com/book2\n# 计算每日持仓\npositions=pd.DataFrame(np.expand_dims(numUnits, axis=1)*weights)# results.evec(:, 1)' can be viewed as the capital allocation, while positions is the dollar capital in each ETF.\npnl=np.sum((positions.shift().values)*(np.log(yNplus)-np.log(yNplus.shift()).values), axis=1) # daily P&L of the strategy\n# print(positions.shift())\ndenominator = np.sum(np.abs(positions.shift()), axis=1)  \n# print(denominator.values)\n# print(pnl.values)\ndenominator = np.where(denominator == 0, 1e-9, denominator)  # 使用一个非常小的数替换零\n# print(denominator)\nret=pd.DataFrame(pnl.values/denominator)\n(np.cumprod(1+ret)-1).plot()\nprint('APR=%f Sharpe=%f' % ((np.prod(1+ret,axis=0)**(252/len(ret))-1).iloc[0], (np.sqrt(252)*np.mean(ret)/np.std(ret,axis=0)).iloc[0]))\n\n98\n[15.86864835  6.19735725]\n[[13.4294 15.4943 19.9349]\n [ 2.7055  3.8415  6.6349]]\n[9.6712911  6.19735725]\n[[12.2971 14.2639 18.52  ]\n [ 2.7055  3.8415  6.6349]]\nAPR=0.044930 Sharpe=1.320011\n\n\n\n\n\n\n\n\n\n“PnL” 是 “Profit and Loss” 的缩写，中文意思是“盈亏”或“损益”。在金融领域，PnL 通常用于描述一个投资策略、交易或其他金融产品的盈利或亏损情况。它反映了在一定时间范围内，投资从初始投资金额到终值的变化。\nPnL 可以用于评估投资策略的表现，帮助投资者了解他们的投资是否成功，以及成功的程度如何。PnL 的计算通常包括以下几个步骤：\n\n确定投资组合的初始价值。\n确定投资组合在特定时间点的价值。\n计算两者之间的差异，即 PnL = 投资组合终值 - 投资组合初始价值。\n\n正的 PnL 表示盈利，负的 PnL 表示亏损。通过分析 PnL，投资者可以更好地了解他们的投资策略的风险和回报特性，从而做出更明智的投资决策。\n\n当然，与上述问题系统的理念可以适用于任何ETF基金、指数或指数子集的交易。此外，如果存在一个跟踪指数或指数子集的期货合约，那我们就可以使用这个期货来取代ETF基金，但是在这种情况下，我们必须对相应的期货价格倍加小心，即使用于回测的期货价格与股票的收盘价格同步运行，也不例外.\n你可能想知道：为什么我们不直接运行约翰森检验模式，检测标准普尔指数所包含的500只股票与SPY基金之间的协整属性，然后让相关算法为包含SPY基金在内的所有具有协整关系的金融工具自动寻找一个特征向量呢？（其实，并不是所有具有协整关系的股票+SPY基金的仓内头寸都必然包括SPY基金本身，但是我们只需要选择一个。）与这种相关的方法论有着双重含义：\n* （1）在约翰森测试的实施过程中，我知道的其所能接受的函数符号最多是12个（LeSage，1998）。 * （2）特征向量通常会涉及股票的多、空两个头寸，这意味着我们不能做多股票的投资组合，同时用SPY基金的空头进行对冲，反之亦然。这就存在一个问题：如果我们在股票投资组合中拥有空头头寸，而同时又做空SPY基金，那么，即使我们做多股票的投资组合，但存在的两个空方头寸会增加我们的特定的风险。\n\n\n关于上述的问题，我们有一个替代的方法，即构造一个多-空头寸并存的股票投资组合。我们仍然可以使用约翰森检验模式——单独测试标准普尔（SPX）指数中每只股票与SPY基金的协整属性，找到相应的股票子集之后，将其纳入股票的投资组合之内，然后，使用一个约束优化方法（如遗传算法和模拟退火算法）来减少股票投资组合的价格系列和SPY基金的价格系列之间的平均差的绝对值——在这种情况下，我们想要优化的变量就是股票的对冲比率，同时，我们的约束条件是：所有对冲比率的值必须是正数。全球MATLAB优化工具箱会提供遗传算法和模拟退火函数来完成这种具有“约束性”的优化任务。\n\n\n由于上述这一策略涉及股票的空方头寸，因此同样受制于卖空交易的约束机制，其实，任何涉及做空的策略都不例外。但是，在这里，此类问题不是太严重，因为相应的投资组合包含的是非常多样化的股票，如果由于卖空约束机制的限制而必须移除几只股票，那影响应该也是有限的。\n\n遗传算法（Genetic Algorithm, GA）和模拟退火算法（Simulated Annealing, SA）都是启发式优化算法，用于解决组合优化问题和全局优化问题。它们都模拟了自然现象，遗传算法模仿了生物进化过程中的自然选择和基因交叉，而模拟退火算法则模仿了物理中的退火过程。\n\n遗传算法 (GA)\n遗传算法的基本思想是通过模拟生物进化过程中的自然选择、遗传和变异来搜索最优解。遗传算法的主要步骤包括：\n\n初始化种群：随机生成一组解作为初始种群。\n适应度评估：计算每个个体（解）的适应度（目标函数值）。\n选择：根据适应度选择优秀的个体进行繁殖。\n交叉：通过基因交叉操作产生新的个体。\n变异：对新产生的个体进行变异操作，增加种群的多样性。\n终止条件：达到预设的迭代次数或其他停止条件时终止算法。\n\n遗传算法的优点是能够在大规模搜索空间中找到近似最优解，并且具有较好的全局搜索能力。缺点是收敛速度较慢，且有可能陷入局部最优解。\n\n\n模拟退火算法 (SA)\n模拟退火算法的基本思想是通过模拟物理中的退火过程来搜索最优解。退火是一种金属热处理工艺，通过将金属加热到高温，然后缓慢降温，使金属内部的原子达到稳定状态。模拟退火算法的主要步骤包括：\n\n初始化：随机生成一个初始解，并设置初始温度。\n邻域搜索：在当前解的邻域内随机生成一个新解。\n接受准则：根据Metropolis准则判断是否接受新解。如果新解的目标函数值更好，则接受新解；如果更差，则以一定概率接受新解，这个概率与温度和目标函数值差有关。\n降温：降低温度。\n终止条件：达到预设的迭代次数或温度降至设定阈值时终止算法。\n\n模拟退火算法的优点是能够跳出局部最优解，具有较好的全局搜索能力。缺点是收敛速度较慢，且参数设置对算法性能有很大影响。\n\n\n总结\n遗传算法和模拟退火算法都是有效的启发式优化算法，适用于解决复杂的优化问题。遗传算法通过模拟生物进化过程，具有较好的全局搜索能力；而模拟退火算法通过模拟物理退火过程，能够跳出局部最优解。在实际应用中，可以根据问题的特点和需求选择合适的算法，或者将两种算法结合使用以提高优化效果。\n在基于协整检验的均值回归策略相关的交易当中，我们以一组固定的金融工具和固定数量的股票，或每个金融工具所配置的一定数额的美元资本来构建一个投资组合。这种固定的数量可以由菲亚特模式（如例4-2所示）、线性回归模式、约翰森测试模式，或约束优化模式来予以确定。但是，没有理由来解析一个问题，即为什么投资组合每天需要包含相同的、数量固定的金融工具，而且，数量固定的金融工具具有相同的权重。其实，许多基于股票交易策略而构建的投资组合的优势恰恰应该是：每日精明地选择股票，同时，对相应的权重，要重新进行加权。\n在所谓的“跨行业”（通常被称之为“横截面”）均值回归策略当中，相关个股价格不一定向自己的历史性的均值回归（这种类型的策略通常只包括股票交易，不包含期货交易，或货币交易）；相反，其重点是相关股票的短期相对收益率；同时，我们依靠这些相对收益率的序列反相关模式而生成相应利润。在大多数情况下，相对收益率的数值是通过特定股票的收益率减去仓内头寸相关的所有股票的平均收益率而得出的。之所以如此，是因为我们预期那些表现不佳的股票在之后的绩效可能会超出市场的平均水平，反之亦然。因为我们只测量相对的收益率，因此，我们很有可能做空一些股票，即使其先前的（绝对）收益率是负值，但只要它的数值高于仓内所有股票的平均收益率即可。\n我在之前的一本书中曾经描述过一个交易策略，此策略由汉丹和罗开发（出现于陈先生的2009年著作之中的例3-7；原始论文由汉丹和罗于2007年所著）。这种策略是：我们投资买入一些股票，这些股票大都是标准普尔500指数（S&P500）、标准普尔1500指数（S&P1500），或罗素2000指数（Russell2000）中的成分股，我们对每只股票都予以配置资本，但配置的比重各有不同。在每一天靠近收盘的时间段内，我们将决定多头资本，或空头资本wi配置给第i只股票（ith）\n换句话说，如果一只股票相对于同类股票来说有一个较高的回报率，那么，我们会大量做空此股票，而如果它相对于同类股票来说表现消极，那我们就会大量做多此股票。\n\n# Example 4.3: Linear Long-Short Model on Stocks\n# 股票相关的线性多-空模式\nimport numpy as np\nimport pandas as pd\n#import matplotlib.pyplot as plt\n#import statsmodels.formula.api as sm\n#import statsmodels.tsa.stattools as ts\n#import statsmodels.tsa.vector_ar.vecm as vm\n\n# Stocks\ncl_=pd.read_csv('datas/inputDataOHLCDaily_20120424_cl.csv')\nstocks=pd.read_csv('datas/inputDataOHLCDaily_20120424_stocks.csv')\n\ncl_['Var1']=pd.to_datetime(cl_['Var1'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ncl_.columns=np.insert(stocks.values, 0, 'Date')\ncl_.set_index('Date', inplace=True)\n\ncl_=cl_.loc[(cl_.index &gt;= pd.to_datetime(\"2007-01-03\").date()) & (cl_.index &lt;= pd.to_datetime(\"2011-12-30\").date()),:]\n\nop=pd.read_csv('datas/inputDataOHLCDaily_20120424_op.csv')\nop['Var1']=pd.to_datetime(op['Var1'],  format='%Y%m%d').dt.date # remove HH:MM:SS\nop.columns=np.insert(stocks.values, 0, 'Date')\nop.set_index('Date', inplace=True)\n\nop=op.loc[(op.index &gt;= pd.to_datetime(\"2007-01-03\").date()) & (op.index &lt;= pd.to_datetime(\"2011-12-30\").date()),:]\n# 基于收盘价的策略\nret=cl_.pct_change() # daily returns\n\nmarketRet=np.mean(ret, axis=1) # equal weighted market index return\n\nweights=-(np.array(ret)-np.reshape(marketRet.values, (ret.shape[0], 1)))\nweights=weights/pd.DataFrame(np.abs(weights)).sum(axis=1).values.reshape((weights.shape[0], 1))\nweights=pd.DataFrame(weights, columns=stocks.values[0], index=np.array(ret.index))\n\ndailyret=(weights.shift()*ret).sum(axis=1) # Capital is always one\n\n\n((1+dailyret).cumprod()-1).plot()\nprint('APR=%f Sharpe=%f' % (np.prod(1+dailyret)**(252/len(dailyret))-1, np.sqrt(252)*np.mean(dailyret)/np.std(dailyret)))\n# APR=13.7%, Sharpe=1.3\n# 基于开盘价和收盘价的策略\nret=(op-cl_.shift())/cl_.shift() # daily returns\n\nmarketRet=np.mean(ret, axis=1) # equal weighted market index return\n\nweights=-(np.array(ret)-np.reshape(marketRet.values, (ret.shape[0], 1)))\nweights=weights/pd.DataFrame(np.abs(weights)).sum(axis=1).values.reshape((weights.shape[0], 1))\nweights=pd.DataFrame(weights, columns=stocks.values[0], index=np.array(ret.index))\n\ndailyret=(weights*(cl_-op)/op).sum(axis=1) # Capital is always one\n\n((1+dailyret).cumprod()-1).plot()\nprint('APR=%f Sharpe=%f' % (np.prod(1+dailyret)**(252/len(dailyret))-1, np.sqrt(252)*np.mean(dailyret)/np.std(dailyret)))\n\nAPR=0.136776 Sharpe=1.259979\nAPR=0.731553 Sharpe=4.715156\n\n\n\n\n\n\n\n\n\n上述策略的显著特点是：它完全是线性的，且没有参数，相应资本又是完美中性的。\n在我以前的著作当中，我也曾建议：我们应该使用靠近今日开盘时刻的之前的收益率指标来确定今日开盘时各类资产的权重，进而提高相关交易策略的整体收益。同时，在收盘之前，我们应该将仓内的头寸全部结清，从而使相应策略变成盘中（日间）的交易策略，\n以上程序的运行结果是：同期年化收益率和夏普比率分别是73%和4.7。尽管有这样出色的绩效，但是开盘价-收盘价的模式仍然有一定的期限，而此缺陷在收盘价-收盘价模式中是没有的，那么，相应模式都有哪些缺陷呢？第一，交易成本（不包括在我们的回测系统当中）将翻倍，因为，我们是一天交易两次，而不是一次交易一天；第二，由于这种策略也必须使用“开盘”价格来确定开盘时刻入场的交易信号，而这同样会产生我在例4-1中所提到的噪声交易信号。\n实际上，即使对于收盘价-收盘价的交易策略而言，我们也不能用确切的收盘价来确定相关的权重，然后按照这些价格入场。但是，在这种情况下，闭市几秒钟之前的价格通常更接近实际的官方（初级交易所）公布的收盘价格，因为这些收盘之前的价格在初级市场第二天开盘之时需要被打印出来，而且具有较高的流动性。\n另外，上述相关策略还有可能存在其他变量（也被称为“要素”），这些变量更善于预测股票价格相关的跨行业（横截面）式的均值回归模式的运行效应，可能比我们在例4-3和例4-4中所使用的相对收益率的模式要好：一个流行的，且被交易员常常使用的变量是衡量股票等级的市盈率（P/E）比率，此比率可能是最后一个季度的收益率，也可能是由分析师或股票发行公司本身所估计的预期收益。收益率的推理模式是：如果收入公告或相关预期发生变化，那么，股票价格将转向一个新的均衡点位，所以，如果对股票的收益预期是正偏的，那么，相关股票的收益率就可能呈现正值，同时，我们不应预期：如果相应收益率与预期的估计值呈等比例变化，那么，相关股票价格就会呈现均值回归的属性。因此，如果我们使用市盈率对股票进行排名的话，我们就可以避免卖空这些股票。"
  },
  {
    "objectID": "posts/2021-08-12-the-whats-whos-and-whys-of-quantitative-trading.html",
    "href": "posts/2021-08-12-the-whats-whos-and-whys-of-quantitative-trading.html",
    "title": "CH1-量化交易简介",
    "section": "",
    "text": "Quantitative trading, also known as algorithmic trading, is the trading of securities based strictly on the buy/sell decisions of computer algorithms. The computer algorithms are designed and perhaps programmed by the traders themselves, based on the historical performance of the encoded strategy tested against historical financial data.\n\n定量交易，也被称为算法交易，是一种严格基于计算机算法买卖决策的证券交易。计算机算法是由交易员自己设计甚至编写的，并根据历史财务数据测试编码策略，以确定期历史业绩。\n\nAs long as you can convert information into bits and bytes that the computer can understand, it can be re\u0002garded as part of quantitative trading.\n\n只要您能将信息转换为计算机能够理解的比特和字节，它就可以被视为定量交易的一部分。"
  },
  {
    "objectID": "posts/2021-08-12-the-whats-whos-and-whys-of-quantitative-trading.html#什么是量化交易",
    "href": "posts/2021-08-12-the-whats-whos-and-whys-of-quantitative-trading.html#什么是量化交易",
    "title": "CH1-量化交易简介",
    "section": "",
    "text": "Quantitative trading, also known as algorithmic trading, is the trading of securities based strictly on the buy/sell decisions of computer algorithms. The computer algorithms are designed and perhaps programmed by the traders themselves, based on the historical performance of the encoded strategy tested against historical financial data.\n\n定量交易，也被称为算法交易，是一种严格基于计算机算法买卖决策的证券交易。计算机算法是由交易员自己设计甚至编写的，并根据历史财务数据测试编码策略，以确定期历史业绩。\n\nAs long as you can convert information into bits and bytes that the computer can understand, it can be re\u0002garded as part of quantitative trading.\n\n只要您能将信息转换为计算机能够理解的比特和字节，它就可以被视为定量交易的一部分。"
  },
  {
    "objectID": "posts/2021-08-12-the-whats-whos-and-whys-of-quantitative-trading.html#谁可以成为量化交易员",
    "href": "posts/2021-08-12-the-whats-whos-and-whys-of-quantitative-trading.html#谁可以成为量化交易员",
    "title": "CH1-量化交易简介",
    "section": "谁可以成为量化交易员",
    "text": "谁可以成为量化交易员\n\nThe ideal independent quantitative trader is therefore someone who has some prior experience with finance or computer program\u0002ming, who has enough savings to withstand the inevitable losses and periods without income, and whose emotion has found the right bal\u0002ance between fear and greed.\n\n因此，理想的独立定量交易员是那些有金融或计算机编程经验，有足够的储蓄来承受不可避免的损失和一定时期的无收入，他的情感已经在恐惧和贪婪之间找到了正确的平衡。"
  },
  {
    "objectID": "posts/2021-08-12-the-whats-whos-and-whys-of-quantitative-trading.html#量化交易的特点",
    "href": "posts/2021-08-12-the-whats-whos-and-whys-of-quantitative-trading.html#量化交易的特点",
    "title": "CH1-量化交易简介",
    "section": "量化交易的特点",
    "text": "量化交易的特点\n\n易扩大\n\nIt is easy to find yourselves trading millions of dollars in the comfort of your own home, as long as your strategy is consistently profitable. This is because scaling up often just means changing a number in your program.\n\n如果你的策略可以持续产生收益的话，扩大规模仅仅意味着在你的程序里修改一个数字。\n\n\n时间自由\n\nif you treasure your leisure time or if you need time and financial resources to explore other businesses, quantitative trading is the business for you.\n\n如果你珍惜你的闲暇时间，或者如果你需要时间和财力来探索其他业务，那么量化交易正适合你。\n\n\n无需市场营销\n\nIn trading, your counterparties in the financial marketplace base their purchase decisions on nothing but the price. Unless you are managing money for other people (which is beyond the scope of this book), there is absolutely no marketing to do in a quantitative trading business.\n\n在交易中，你在金融市场上的交易对手，他们的购买决定只基于价格。除非你是在为其他人管理资金(这超出了本书的范围),在量化交易业务中，绝对没有营销要做。\n\nthe business of quantitative trading allows you to focus exclusively on your product (the strategy and the software), and not on anything that has to do with influencing other people’s perception of yourself.\n\n量化交易业务允许你只专注于你的产品（策略和软件），而不需要做任何影响他人对你自己看法的事。"
  },
  {
    "objectID": "posts/2021-08-12-the-whats-whos-and-whys-of-quantitative-trading.html#前方的路",
    "href": "posts/2021-08-12-the-whats-whos-and-whys-of-quantitative-trading.html#前方的路",
    "title": "CH1-量化交易简介",
    "section": "前方的路",
    "text": "前方的路\n\nIf you are convinced that you want to become a quantitative trader,a number of questions immediately follow: How do you find the right strategy to trade? How do you recognize a good versus a bad strategy even before devoting any time to backtesting them? How do you rigorously backtest them? If the backtest performance is good, what steps do you need to take to implement the strategy, in terms of both the business structure and the technological infrastructure? If the strategy is profitable in initial real-life trading, how does one scale up the capital to make it into a growing income stream while managing the inevitable (but, hopefully, only occasional) losses that come with trading?\n\n如果你确信要成为一名量化交易员，你需要面对以下问题 * 怎样找到正确的交易策略？ * 在投入时间回测之前怎样区分好坏策略？ * 你怎样严谨地回测你的策略？ * 如果回测效果很好，你要通过哪些步骤实现这些策略，有哪些业务架构和技术基础设施的模式？ * 如果策略在真实交易中获利，怎样在扩大资金规模提升收入流的同时管控交易中不可必免的损失？"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "王君敕的炼丹房",
    "section": "",
    "text": "算法交易：制胜策略与原理(欧内斯特·陈)之六\n\n\n\n\n\n\n算法交易\n\n\n\n日间动量型交易策略\n\n\n\n\n\nSep 5, 2024\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\n算法交易：制胜策略与原理(欧内斯特·陈)之五\n\n\n\n\n\n\n算法交易\n\n\n\n货币交易与期货交易的均值回归交易策略\n\n\n\n\n\nSep 4, 2024\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\n算法交易：制胜策略与原理(欧内斯特·陈)之四\n\n\n\n\n\n\n算法交易\n\n\n\n股票与ETF的均值回归模式\n\n\n\n\n\nAug 30, 2024\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\n算法交易：制胜策略与原理(欧内斯特·陈)之三\n\n\n\n\n\n\n算法交易\n\n\n\n均值回归策略的运行机制\n\n\n\n\n\nAug 29, 2024\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\n算法交易：制胜策略与原理(欧内斯特·陈)之二\n\n\n\n\n\n\n算法交易\n\n\n\n均值回归策略的运行机制\n\n\n\n\n\nAug 22, 2024\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\n算法交易中的一些概念\n\n\n\n\n\n\n量化交易概念\n\n\n\n\n\n\n\n\n\nAug 21, 2024\n\n\n王君敕\n\n\n\n\n\n\n\n\n\n\n\n\n关于迪基-富勒检验\n\n\n\n\n\n\n量化交易概念\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\n算法交易：制胜策略与原理(欧内斯特·陈)之一\n\n\n\n\n\n\n算法交易\n\n\n\n均值回归模式的基本要义\n\n\n\n\n\nJul 27, 2024\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\n随机漫步\n\n\n\n\n\n\n量化交易\n\n\n\n\n\n\n\n\n\nJul 25, 2024\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\nbacktrader回测系统\n\n\n\n\n\n\n量化交易\n\n\n\nbacktrader\n\n\n\n\n\nDec 6, 2021\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\nCH7-量化交易专题\n\n\n\n\n\n\n量化交易\n\n\n\nSpecial Topics in Quantitative Trading\n\n\n\n\n\nAug 27, 2021\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\nCH6-资金和风险管理\n\n\n\n\n\n\n量化交易\n\n\n\nMoney and Risk Management\n\n\n\n\n\nAug 27, 2021\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\nCH5-交易执行系统\n\n\n\n\n\n\n量化交易\n\n\n\nExecutions Systems\n\n\n\n\n\nAug 19, 2021\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\nCH4-建立你的事业\n\n\n\n\n\n\n量化交易\n\n\n\nSetting Up Your Business\n\n\n\n\n\nAug 18, 2021\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\nCH3-回测\n\n\n\n\n\n\n量化交易\n\n\n\nBacktesting\n\n\n\n\n\nAug 16, 2021\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\nCH2-捕获点子\n\n\n\n\n\n\n量化交易\n\n\n\nfishing of ideas\n\n\n\n\n\nAug 13, 2021\n\n\n王一刀\n\n\n\n\n\n\n\n\n\n\n\n\nCH1-量化交易简介\n\n\n\n\n\n\n量化交易\n\n\n\n量化交易是什么，谁参与量化交易，为什么要量化交易\n\n\n\n\n\nAug 12, 2021\n\n\n王一刀\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "关于",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2024-07-25-random-walk.html",
    "href": "posts/2024-07-25-random-walk.html",
    "title": "随机漫步",
    "section": "",
    "text": "布朗运动是将看起来连成一片的液体，在高倍显微镜下看其实是由许许多多分子组成的。液体分子不停地做无规则的运动，不断地随机撞击悬浮微粒。当悬浮的微粒足够小的时候，由于受到的来自各个方向的液体分子的撞击作用是不平衡的。在某一瞬间，微粒在另一个方向受到的撞击作用超强的时候，致使微粒又向其它方向运动，这样，就引起了微粒的无规则的运动就是布朗运动。（布朗运动指的是分子迸出的微粒的随机运动，而不是分子的随机运动。）\n即布朗运动代表了一种随机涨落现象。普遍的观点仍认为，股票市场是随机波动的，随机波动是股票市场最根本的特性，是股票市场的常态。（随机现象的数学定义是：在个别试验中其结果呈现出不确定性；在大量重复试验中其结果又具有统计规律性的现象。）而布朗运动假设是现代资本市场理论的核心假设。"
  },
  {
    "objectID": "posts/2024-07-25-random-walk.html#几何布朗运动brownian-motion",
    "href": "posts/2024-07-25-random-walk.html#几何布朗运动brownian-motion",
    "title": "随机漫步",
    "section": "",
    "text": "布朗运动是将看起来连成一片的液体，在高倍显微镜下看其实是由许许多多分子组成的。液体分子不停地做无规则的运动，不断地随机撞击悬浮微粒。当悬浮的微粒足够小的时候，由于受到的来自各个方向的液体分子的撞击作用是不平衡的。在某一瞬间，微粒在另一个方向受到的撞击作用超强的时候，致使微粒又向其它方向运动，这样，就引起了微粒的无规则的运动就是布朗运动。（布朗运动指的是分子迸出的微粒的随机运动，而不是分子的随机运动。）\n即布朗运动代表了一种随机涨落现象。普遍的观点仍认为，股票市场是随机波动的，随机波动是股票市场最根本的特性，是股票市场的常态。（随机现象的数学定义是：在个别试验中其结果呈现出不确定性；在大量重复试验中其结果又具有统计规律性的现象。）而布朗运动假设是现代资本市场理论的核心假设。"
  },
  {
    "objectID": "posts/2024-07-25-random-walk.html#随机游走",
    "href": "posts/2024-07-25-random-walk.html#随机游走",
    "title": "随机漫步",
    "section": "随机游走",
    "text": "随机游走\n其概念接近于布朗运动，是布朗运动的理想数学状态。任何分子所带的守恒量都各自对应着一个扩散运输定律。\n随机游走过程\\(S_t\\)​遵循几何布朗运动，满足微分方程：\n\\(dS_t​=uS_t​d_t+σS_t​dW_t\\)\n\\(​dS_t​/S_t​=ud_t+σdW_t\\)\n​设定初试状态\\(S_0\\)，根据伊藤积分，可以解出：\n\\(S_t​=S_0​exp((u−σ^2/2)t+σW_t​)\\)\n在数学领域，函数exp(x)代表自然指数函数，即以实数e（e≈2.71828）为底的指数函数。其表达式为exp(x)=\\(e^x\\)\n所以上面的式子也可以写成：\n\\(S_t​=S_0​e^{(u−σ^2/2)t+σW_t​}\\)\n其中μ (‘百分比drift’) 和σ (‘百分比volatility’)是常量。\n漂移率（Drift Rate）： 在金融领域，漂移率指的是资产价格的平均变动率，用来衡量资产价格的趋势性。例如，股票价格的漂移率可以帮助投资者判断价格的长期趋势\n波动率（volatility） 波动性，在金融数学领域，指金融资产在一定时间段的变化性。通常以一年内涨落的标准差来测量。金融市场中，投资的波动性与其风险有着密切的联系。\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#rect=[0.1,5.0,0.1,0.1]\nfig=plt.figure()\n\n #time span\nT=2\n#drift factor飘移率\nmu=0.1 \n#volatility波动率\nsigma=0.04 \n#t=0初试价\nS0=20 \n#length of steps\ndt=0.01 \nN=round(T/dt)\nt=np.linspace(0,T,N)\n\n#布朗运动\nW=np.random.standard_normal(size=N)\nW=np.cumsum(W)*np.sqrt(dt)\n\nX=(mu-0.5*sigma**2)*t+sigma*W\n\nS=S0*np.exp(X)\n\nplt.plot(t,S,lw=2)\nplt.show()"
  },
  {
    "objectID": "posts/2024-08-20-Things-About-Dickey-Fuller.html",
    "href": "posts/2024-08-20-Things-About-Dickey-Fuller.html",
    "title": "关于迪基-富勒检验",
    "section": "",
    "text": "时间序列模型是一种用于分析和预测时间序列数据的统计模型。这些模型通过识别数据中的趋势、季节性和其他模式，来预测未来的值。以下是关于时间序列模型的相关信息：\n\n\n时间序列模型定义为一个随机过程，即按时间排序的随机变量的集合。每个时刻的位置点作为一个随机变量，其取值是从一个分布中采样得到的。\n\n\n\n\n自回归模型 (AR)：根据历史观测值来预测未来时序。\n移动平均模型 (MA)：根据白噪声的系数加权和来预测未来时序。\n自回归移动平均模型 (ARMA)：结合了AR和MA的特点，同时考虑历史观测值和白噪声的影响。\n差分移动自回归模型 (ARIMA)：适用于有明显上升或下降趋势的数据集，通过差分使数据平稳后使用ARMA拟合。\n\n\n\n\n平稳性是指时间序列的统计特性（如均值、方差和自协方差）不随时间变化。如果时间序列有季节性和趋势性，则视为不平稳。平稳性是许多时间序列预测模型（如自回归模型）的基础。\n\n\n\n特征根是判断时间序列模型平稳性的关键。一个平稳时间序列模型的特征根都在单位圆内，这意味着模型是稳定的，能够提供可靠的预测。"
  },
  {
    "objectID": "posts/2024-08-20-Things-About-Dickey-Fuller.html#时间序列模型",
    "href": "posts/2024-08-20-Things-About-Dickey-Fuller.html#时间序列模型",
    "title": "关于迪基-富勒检验",
    "section": "",
    "text": "时间序列模型是一种用于分析和预测时间序列数据的统计模型。这些模型通过识别数据中的趋势、季节性和其他模式，来预测未来的值。以下是关于时间序列模型的相关信息：\n\n\n时间序列模型定义为一个随机过程，即按时间排序的随机变量的集合。每个时刻的位置点作为一个随机变量，其取值是从一个分布中采样得到的。\n\n\n\n\n自回归模型 (AR)：根据历史观测值来预测未来时序。\n移动平均模型 (MA)：根据白噪声的系数加权和来预测未来时序。\n自回归移动平均模型 (ARMA)：结合了AR和MA的特点，同时考虑历史观测值和白噪声的影响。\n差分移动自回归模型 (ARIMA)：适用于有明显上升或下降趋势的数据集，通过差分使数据平稳后使用ARMA拟合。\n\n\n\n\n平稳性是指时间序列的统计特性（如均值、方差和自协方差）不随时间变化。如果时间序列有季节性和趋势性，则视为不平稳。平稳性是许多时间序列预测模型（如自回归模型）的基础。\n\n\n\n特征根是判断时间序列模型平稳性的关键。一个平稳时间序列模型的特征根都在单位圆内，这意味着模型是稳定的，能够提供可靠的预测。"
  },
  {
    "objectID": "posts/2024-08-20-Things-About-Dickey-Fuller.html#特征根",
    "href": "posts/2024-08-20-Things-About-Dickey-Fuller.html#特征根",
    "title": "关于迪基-富勒检验",
    "section": "特征根",
    "text": "特征根\n特征根是线性代数中的一个重要概念，特别是在时间序列分析中。特征根与特征向量一起，构成了理解线性变换和动态系统行为的基础。以下是关于特征根的相关信息：\n\n特征根的定义\n特征根是线性代数中一个矩阵的特征方程的根，它表示矩阵对应的线性变换在特定方向上的伸缩因子。在时间序列分析中，特征根与时间序列的稳定性密切相关。\n\n\n特征根在时间序列分析中的应用\n\n平稳性检验：通过检查特征根的位置（是否在单位圆内），可以判断时间序列是否平稳。如果所有特征根都在单位圆内，则时间序列是平稳的，否则是非平稳的。\n模型选择：不同的时间序列模型（如AR、MA、ARMA）的特征根有不同的性质，这些性质可以帮助我们选择合适的模型进行分析和预测。\n\n\n\n特征根与时间序列稳定性的关系\n单位根与序列稳定性：如果一个时间序列的特征根包含单位根（即特征根的绝对值等于1），则该序列是非平稳的。单位根的存在意味着序列的统计特性会随时间变化，这可能会影响到时间序列的预测能力。\n特征方程：对于时间序列模型，特征方程是通过将模型的差分方程转化为代数方程得到的。例如，AR(1)模型的特征方程为 1−aZ=0，其中 a 是自回归系数，Z 是特征根\n对模型预测能力的影响：特征根的位置决定了时间序列模型的预测能力。平稳的时间序列模型（所有特征根都在单位圆内）可以提供可靠的短期预测，而非平稳序列可能需要通过差分等方法转换为平稳序列后才能进行有效预测。\n\n\n特征根的理解\n特征根是线性代数中的一个重要概念，它与矩阵的特征方程密切相关。特征方程是一个关于未知数λ的方程，这个方程是通过求解矩阵A减去λ倍的单位矩阵后的行列式等于0得到的。\n具体来说，对于一个n×n的矩阵A，其特征方程可以表示为： det(A - λI) = 0\n其中，det表示行列式，I是n×n的单位矩阵，λ是我们要求的特征根。\n这个特征方程的解，即特征根λ，具有特殊的意义。它表示矩阵A对应的线性变换在某个特定方向上的伸缩因子。换句话说，如果我们有一个向量v，它是对应于特征根λ的特征向量，那么矩阵A作用在这个向量上，就相当于把这个向量伸缩了λ倍。\n举个例子来说明这个概念：\n假设我们有一个2×2的矩阵A：\nA = [[3, 1],\n     [1, 3]]\n我们需要找到这个矩阵的特征根和特征向量。\n首先，我们写出特征方程： det(A - λI) = det([[3-λ, 1], [1, 3-λ]]) = (3-λ)^2 - 1 = λ^2 - 6λ + 8 = 0\n解这个方程，我们得到两个特征根： λ1 = 2, λ2 = 4\n接下来，我们分别求对应于这两个特征根的特征向量。\n对于λ1 = 2，我们解方程组(A - 2I)v = 0，得到一个特征向量v1 = [1, -1]^T（T表示转置）。\n对于λ2 = 4，我们解方程组(A - 4I)v = 0，得到另一个特征向量v2 = [1, 1]^T。\n现在，我们可以验证特征根的含义。对于特征向量v1 = [1, -1]^T，矩阵A作用在这个向量上得到的结果是： Av1 = A[1, -1]^T = [3 * 1 + 1(-1), 1  1 + 3*(-1)]^T = [2, -2]^T = 2*v1\n可以看到，矩阵A将特征向量v1伸缩了2倍，这与我们找到的特征根λ1 = 2是一致的。\n同样地，对于特征向量v2 = [1, 1]^T，矩阵A作用在这个向量上得到的结果是： Av2 = A[1, 1]^T = [3 * 1 + 1 * 1, 1 * 1 + 3 * 1]^T = [4, 4]^T = 4*v2\n矩阵A将特征向量v2伸缩了4倍，这与我们找到的特征根λ2 = 4也是一致的。\n通过这个例子，我们可以看到特征根确实表示了矩阵对应的线性变换在特定方向上的伸缩因子。\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# 定义一个2x2矩阵\nA = np.array([[3, 1],\n              [1, 3]])\n\n# 使用numpy.linalg.eig函数计算特征根和特征向量\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nprint(\"特征根:\", eigenvalues)\nprint(\"特征向量:\\n\", eigenvectors)\n\n# 生成一个非平稳的时间序列数据\nnp.random.seed(0)\ndata = np.cumsum(np.random.randn(100))\n\n# 将数据转换为pandas的Series对象\ndata_series = pd.Series(data)\n\n# 绘制时间序列图\n# data_series.plot()\n\n# 进行DF检验\nresult_df = adfuller(data_series)\n\n# 输出DF检验结果\nprint('ADF Statistic:', result_df[0])\nprint('p-value:', result_df[1])\nprint('Critical Values:', result_df[4])\n\n# 根据p-value判断是否拒绝原假设\nif result_df[1] &lt; 0.05:\n    print('拒绝原假设，序列是平稳的')\nelse:\n    print('接受原假设，序列是非平稳的')\n\n\n# 进行ADF检验\nresult_adf = adfuller(data_series, regression='c', autolag='AIC')\n\n# 输出ADF检验结果\nprint('ADF Statistic:', result_adf[0])\nprint('p-value:', result_adf[1])\nprint('Critical Values:', result_adf[4])\n\n# 根根据p-value判断是否拒绝原假设\nif result_adf[1] &lt; 0.05:\n    print('拒绝原假设，序列是平稳的')\nelse:\n    print('接受原假设，序列是非平稳的')\n\n# 进行KPSS检验\nresult_kpss = kpss(data_series, regression='c')\n\n# 输出KPSS检验结果\nprint('KPSS Statistic:', result_kpss[0])\nprint('p-value:', result_kpss[1])\nprint('Critical Values:', result_kpss[3])\n\n# 根据p-value判断是否拒绝原假设\nif result_kpss[1] &gt; 0.05:\n    print('拒绝原假设，序列是平稳的')\nelse:\n    print('接受原假设，序列是非平稳的')\n\n特征根: [4. 2.]\n特征向量:\n [[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\nADF Statistic: -1.1320384625097901\np-value: 0.7021277385898382\nCritical Values: {'1%': -3.498198082189098, '5%': -2.891208211860468, '10%': -2.5825959973472097}\n接受原假设，序列是非平稳的\nADF Statistic: -1.1320384625097901\np-value: 0.7021277385898382\nCritical Values: {'1%': -3.498198082189098, '5%': -2.891208211860468, '10%': -2.5825959973472097}\n接受原假设，序列是非平稳的\nKPSS Statistic: 1.1001161286471666\np-value: 0.01\nCritical Values: {'10%': 0.347, '5%': 0.463, '2.5%': 0.574, '1%': 0.739}\n接受原假设，序列是非平稳的\n\n\nC:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_15296\\3430932899.py:56: InterpolationWarning: The test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n  result_kpss = kpss(data_series, regression='c')\n\n\nDF检验，也称为Dickey-Fuller检验，是一种用于检验时间序列数据是否含有单位根的统计方法。单位根是指一个时间序列的期望值不为零，但其自协方差函数仅依赖于时间间隔的长度，而不依赖于时间的起点。这种特性使得时间序列数据表现出非平稳性，即它们的统计特性会随时间变化。DF检验的基本思想是通过检验时间序列的回归模型中的自回归系数是否为零来确定是否存在单位根。以下是DF检验的相关信息：\n\n\nDF检验的基本原理\nDF检验通过检验时间序列的回归模型中的自回归系数是否为零来确定是否存在单位根。如果自回归系数显著不为零，则表明序列含有单位根，即序列是非平稳的。\n\n\nDF检验的应用场景\nDF检验广泛应用于经济学、金融学和统计学等领域，用于检验时间序列数据的平稳性。例如，在经济学中，它可以帮助分析经济增长率、通货膨胀率等时间序列数据的稳定性。\n\n\nDF检验的优缺点\n优点：DF检验是一种简单且易于实施的统计方法，适用于均值比较的问题，并且可以处理样本大小不一致的情况。 缺点：DF检验对样本数据的正态性有一定要求，如果数据不符合正态分布，可能会影响检验结果的准确性。它只能比较两个样本均值之间的差异，不能提供对多个样本进行比较的信息。\n\n\nDF检验与ADF检验的区别\nDF检验是最早提出的单位根检验方法之一，它是建立在一个简单的自回归模型(AR(1))上的。相比之下，ADF检验使用了更复杂的自回归模型，并且引入了趋势项来对序列的趋势进行建模，从而提高了检验的适用性和准确性。 DF检验只能应用于一阶情况，当序列存在高阶的滞后相关时，可以使用ADF检验，所以说ADF是对DF检验的扩展。\n\nimport statsmodels.api as sm\nimport numpy as np\n\n# 假设我们有以下数据\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 3, 4, 5, 6])\nprint(x)\n# 添加截距项\nx = sm.add_constant(x)\n\nprint(x)\n\n# 拟合线性回归模型\n# model = sm.OLS(y, x).fit()\n\n# # 输出模型摘要\n# print(model.summary())\n\n# # 预测\n# predictions = model.predict(x)\n# print(\"预测值:\", predictions)\n\n[1 2 3 4 5]\n[[1. 1.]\n [1. 2.]\n [1. 3.]\n [1. 4.]\n [1. 5.]]"
  },
  {
    "objectID": "posts/2024-08-20-Things-About-Dickey-Fuller.html#赫斯特hurst指数",
    "href": "posts/2024-08-20-Things-About-Dickey-Fuller.html#赫斯特hurst指数",
    "title": "关于迪基-富勒检验",
    "section": "赫斯特（Hurst）指数",
    "text": "赫斯特（Hurst）指数\n赫斯特（Hurst）指数是一种用于分析时间序列数据的统计工具，主要用于衡量时间序列的长期记忆性或自相关性。赫斯特指数是由英国水文学家哈罗德·赫斯特（Harold Edwin Hurst）在20世纪50年代提出的，最初用于分析尼罗河的水位变化。\n赫斯特指数的取值范围在0到1之间，可以分为以下几种情况：\nH = 0.5：时间序列是随机的，没有自相关性。这种情况下，时间序列的未来值与过去值之间没有任何关系。\n0.5 &lt; H &lt; 1：时间序列具有长期记忆性，即未来的趋势与过去的趋势有关。这种情况下，时间序列呈现出持久性（persistence），即如果过去是上升的，未来也很可能继续上升；如果过去是下降的，未来也很可能继续下降。\n0 ≤ H &lt; 0.5：时间序列具有反记忆性（antipersistence），即未来的趋势与过去的趋势相反。这种情况下，时间序列呈现出均值回归（mean reversion）的特性，即如果过去是上升的，未来很可能下降；如果过去是下降的，未来很可能上升。\n赫斯特指数的计算方法有很多种，其中最常用的是重标极差法（Rescaled Range Analysis, R/S）。以下是计算赫斯特指数的基本步骤：\n\n将时间序列分成若干个等长的子序列。\n对每个子序列计算累积和（cumulative sum）。\n对每个子序列计算标准差（standard deviation）。\n计算每个子序列的重标极差（rescaled range），即累积和的最大值与最小值之差除以子序列的标准差。\n计算整个时间序列的重标极差均值。\n根据重标极差均值和时间序列的长度计算赫斯特指数。\n\n赫斯特指数在许多领域都有广泛的应用，如金融市场分析、气象学、水文学、地球物理学等。通过计算赫斯特指数，可以帮助我们了解时间序列数据的特性，预测未来趋势，以及评估不同时间序列之间的相关性。"
  },
  {
    "objectID": "posts/2024-08-20-Things-About-Dickey-Fuller.html#方差比检验",
    "href": "posts/2024-08-20-Things-About-Dickey-Fuller.html#方差比检验",
    "title": "关于迪基-富勒检验",
    "section": "方差比检验",
    "text": "方差比检验\n（Variance Ratio Test）是一种用于检验两个或多个独立样本方差是否相等的统计方法。它基于F分布，通过比较两组数据的方差来判断它们是否来自具有相同方差的总体。\n方差比检验的步骤如下：\n提出假设：\n原假设（H0）：两组数据的方差相等。 备择假设（H1）：两组数据的方差不等于相等。 计算检验统计量：\n计算两组数据的样本方差 ( s_1^2 ) 和 ( s_2^2 )。 计算方差比 ( VR = ) 或 ( VR = )，具体取决于哪组数据的方差较大。 根据样本大小 ( n_1 ) 和 ( n_2 )，查找F分布表，确定临界值 ( F_{/2} ) 和 ( F_{1-/2} )。 做出决策：\n如果 ( VR ) 落在拒绝域内（即 ( VR &lt; F_{/2} ) 或 ( VR &gt; F_{1-/2} )），则拒绝原假设，认为两组数据的方差不等于相等。 如果 ( VR ) 落在接受域内，则不拒绝原假设，认为两组数据的方差相等。 方差比检验的适用条件包括：\n样本是独立的。 样本来自正态分布的总体。 方差比检验通常用于比较两个独立样本的方差，但也可以扩展到多个样本的情况。 需要注意的是，方差比检验对样本大小和分布的正态性有一定的要求。如果样本大小较小或数据分布偏离正态性，检验结果可能不准确。在实际应用中，可以考虑使用其他统计方法，如Levene检验或Bartlett检验，来检验方差的齐性。"
  },
  {
    "objectID": "posts/2024-08-20-Things-About-Dickey-Fuller.html#半衰期检验",
    "href": "posts/2024-08-20-Things-About-Dickey-Fuller.html#半衰期检验",
    "title": "关于迪基-富勒检验",
    "section": "半衰期检验",
    "text": "半衰期检验\n半衰期检验（Half-Life Test）是一种用于评估放射性物质衰变速率的实验方法。在核物理学、化学和环境科学等领域，半衰期检验被广泛应用于研究放射性同位素的衰变特性。通过测量放射性物质在一定时间内的衰变量，可以计算出其半衰期，从而了解其衰变速率和放射性强度。\n半衰期检验的基本原理是：在一个封闭系统中，放射性物质的原子核会随着时间的推移逐渐衰变成其他元素或同位素。在衰变过程中，放射性物质的剩余量会按照一定的规律减少。半衰期是指放射性物质剩余量减少到原来的一半所需的时间。\n进行半衰期检验的步骤如下：\n准备样品：选择一个合适的放射性物质样品，并将其置于一个封闭的容器中。\n测量初始放射性强度：使用辐射探测器测量样品的初始放射性强度。\n等待一段时间：等待足够长的时间，使得样品的放射性强度发生明显变化。\n再次测量放射性强度：在等待时间结束后，再次使用辐射探测器测量样品的放射性强度。\n计算半衰期：根据两次测量的放射性强度和时间间隔，使用以下公式计算半衰期：\n[ t_{1/2} = ]\n其中，( t_{1/2} ) 是半衰期，( (2) ) 是自然对数的底数（约等于 0.693），( k ) 是衰变速率常数。\n分析结果：根据计算出的半衰期，分析放射性物质的衰变速率和放射性强度。\n需要注意的是，半衰期检验需要在专业的实验室环境中进行，并采取适当的安全措施，以防止辐射对人体的伤害。此外，半衰期检验的结果可能受到测量误差和其他因素的影响，因此需要进行多次实验并取平均值以提高准确性。\n半衰期检验（Half-Life Test）本身并不是专门用于测试时间序列的回归性和平稳性的方法。半衰期检验主要用于评估放射性物质衰变速率，而不是时间序列分析"
  },
  {
    "objectID": "posts/2024-08-20-Things-About-Dickey-Fuller.html#协整关系",
    "href": "posts/2024-08-20-Things-About-Dickey-Fuller.html#协整关系",
    "title": "关于迪基-富勒检验",
    "section": "协整关系",
    "text": "协整关系\n协整关系（Cointegration）是时间序列分析中的一个重要概念，它描述了两个或多个非平稳时间序列之间存在的长期稳定关系。\n在经济学和金融学中，协整关系常用于分析不同经济变量之间的长期均衡关系。例如，协整关系可以用来检验两个经济指标（如价格和工资）是否长期同步变动，从而判断它们之间是否存在长期的均衡关系。\n协整关系的存在需要满足一定的条件，包括：\n\n变量的非平稳性：协整关系通常存在于非平稳的时间序列之间。如果所有变量都是平稳的，那么它们之间就不存在协整关系。\n长期稳定性：协整关系描述的是变量之间的长期稳定关系。即使短期内变量之间可能出现波动或偏离，但在长期内它们会回归到均衡状态。\n最小二乘法估计的有效性：在存在协整关系的情况下，可以使用最小二乘法（OLS）对回归模型进行估计，并且得到的估计量是一致的。\n\n协整关系的检验通常使用Engle-Granger两步法或其他更复杂的统计方法。如果检验结果表明变量之间存在协整关系，那么就可以进一步分析它们之间的长期均衡关系，并进行相关的预测和政策分析。"
  },
  {
    "objectID": "posts/2024-08-20-Things-About-Dickey-Fuller.html#协整型adf检验",
    "href": "posts/2024-08-20-Things-About-Dickey-Fuller.html#协整型adf检验",
    "title": "关于迪基-富勒检验",
    "section": "协整型ADF检验",
    "text": "协整型ADF检验\n协整型ADF检验（Cointegration Augmented Dickey-Fuller Test）是一种用于检验两个或多个非平稳时间序列是否存在协整关系的统计方法。协整关系指的是两个或多个时间序列之间存在长期的稳定关系，即使它们各自是非平稳的。\n在进行协整型ADF检验之前，通常需要先对时间序列进行单位根检验，以确定它们是否为非平稳的。如果两个时间序列都是非平稳的，但它们的线性组合是平稳的，那么这两个时间序列之间存在协整关系。\n协整型ADF检验的基本步骤如下：\n单位根检验：首先对两个或多个时间序列分别进行单位根检验，确定它们是否为非平稳的。常用的单位根检验方法有 Augmented Dickey-Fuller (ADF) 检验和 Phillips-Perron (PP) 检验。\n估计协整关系：如果两个时间序列都是非平稳的，接下来需要估计它们之间的协整关系。这可以通过最小二乘法或其他回归方法来实现。\n协整型ADF检验：在估计出协整关系后，进行协整型ADF检验。这个检验实际上是检验残差序列是否平稳。如果残差序列是平稳的，那么两个时间序列之间存在协整关系；否则，它们之间不存在协整关系。\n解释结果：根据协整型ADF检验的结果，可以得出两个时间序列之间是否存在协整关系的结论。如果存在协整关系，那么可以利用这个关系进行预测和分析。\n需要注意的是，协整型ADF检验对样本大小和数据质量有一定的要求。如果样本太小或数据存在严重的自相关或异方差问题，检验结果可能不准确。在实际应用中，可以考虑使用其他协整检验方法，如 Engle-Granger 两步法或 Johansen 协整检验。\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.regression.linear_model import OLS\n\n# 生成两个非平稳的时间序列\nnp.random.seed(42)\nn = 100\nx = np.cumsum(np.random.randn(n))\ny = 2 * x + np.cumsum(np.random.randn(n))\n\n# 单位根检验\nresult_x = adfuller(x)\nresult_y = adfuller(y)\n\nprint(\"x 的单位根检验结果：\", result_x)\nprint(\"y 的单位根检验结果：\", result_y)\n\n# 估计协整关系\nmodel = OLS(y, x).fit()\nresiduals = model.resid\n\n# 协整型ADF检验\nresult_residuals = adfuller(residuals)\n\nprint(\"残差序列的单位根检验结果：\", result_residuals)\n\n# 解释结果\nalpha = 0.05\nif result_x[1] &gt; alpha and result_y[1] &gt; alpha and result_residuals[1] &lt; alpha:\n    print(\"x 和 y 存在协整关系\")\nelse:\n    print(\"x 和 y 不存在协整关系\")\n\nx 的单位根检验结果： (-1.3583317659818985, 0.6020814791099101, 0, 99, {'1%': -3.498198082189098, '5%': -2.891208211860468, '10%': -2.5825959973472097}, 222.8689980232281)\ny 的单位根检验结果： (-1.258001500815765, 0.6481611011736785, 1, 98, {'1%': -3.4989097606014496, '5%': -2.891516256916761, '10%': -2.5827604414827157}, 356.0626708213774)\n残差序列的单位根检验结果： (-1.9559190170990346, 0.3062290299375441, 0, 99, {'1%': -3.498198082189098, '5%': -2.891208211860468, '10%': -2.5825959973472097}, 237.03854879262025)\nx 和 y 不存在协整关系\n\n\n约翰森测试（Johansen Test），也称为约翰森协整检验（Johansen Cointegration Test），是一种用于检验多个非平稳时间序列之间是否存在长期稳定关系的统计方法。这种方法是基于向量自回归（VAR）模型，通过估计模型参数并构造相应的统计量来检验协整关系的存在性。以下是关于约翰森测试的详细介绍：\n\n约翰森测试的基本原理\n约翰森测试的核心思想是，如果两个或多个非平稳时间序列之间存在协整关系，那么它们的线性组合应该是平稳的。通过构建一个向量自回归（VAR）模型，并估计模型参数，可以检验这些时间序列的协整关系。\n\n\n约翰森测试的主要步骤\n\n建立VAR模型：首先确定VAR模型的最优滞后阶数，并估计模型参数。\n计算残差：对VAR模型进行估计后，计算模型的残差。\n单位根检验：对残差进行单位根检验，以判断是否存在协整关系。\n\n\n\n约翰森测试的结果解读\n\n特征值：特征值表示每个潜在协整关系的强度。特征值越接近1，表示协整关系越强。\n特征向量：特征向量表示协整关系的方向。\n似然比统计量：用于检验是否存在协整关系以及协整关系的数量。\n临界值：用于确定似然比统计量是否显著。\n\n\n\n约翰森测试的应用场景\n约翰森测试广泛应用于经济学和金融学领域，特别是用于研究股票价格、汇率、利率等变量之间的长期均衡关系。它可以帮助分析师判断两个或多个时间序列之间是否存在长期的稳定关系，从而为投资决策提供依据。\n\n\n约翰森测试与相关性检验的区别\n\n协整性：描述的是两个或多个非平稳时间序列之间存在的一种长期稳定关系。\n相关性：衡量的是两个时间序列之间的线性关系强度。\n\n约翰森测试的结果可以帮助我们避免使用非平稳数据进行回归分析所导致的“伪回归”问题，从而更准确地分析变量之间的长期均衡关系。在实际操作中，交易者通常会结合其他技术指标和市场分析来综合判断股票之间的协整性，以制定更为有效的交易策略。\n\nimport numpy as np  \nimport pandas as pd  \nfrom statsmodels.tsa.vector_ar.vecm import coint_johansen  \n  \n# 生成模拟数据  \nnp.random.seed(42)  \nt = np.linspace(0, 10, 100)  \nx1 = np.sin(t) + np.random.normal(0, 0.1, 100)  \nx2 = np.cos(t) + np.random.normal(0, 0.1, 100)  \nx3 = 0.5 * x1 + 0.5 * x2 + np.random.normal(0, 0.1, 100)  \n  \n# 将数据转换为DataFrame  \ndata = pd.DataFrame({'X1': x1, 'X2': x2, 'X3': x3})  \nprint(data)\n# 使用Johansen协整检验  \nmodel = coint_johansen(data, det_order=0, k_ar_diff=1)  \n  \n# 打印结果  \nprint(\"Johansen协整检验结果:\")  \nprint(\"\\n--- 迹统计量（Trace Statistics, lr1）---\")  \nprint(model.lr1)  \nprint(\"\\n--- 临界值（Critical Values，通常为1%、5%、10%显著性水平）---\")  \nprint(model.cvt)  \nprint(\"\\n--- 最大特征值统计量（Maximum Eigenvalue Statistics, lr2）---\")  \nprint(model.lr2)  \nprint(\"\\n--- 临界值（Critical Values，通常为1%、5%、10%显著性水平）---\")  \nprint(model.cvm)  \n  \n# 解释结果并打印的函数  \ndef print_rejection(stat, cv, name, significance_level=0.05):  \n    # 将显著性水平转换为列索引（cvt的第一行通常是显著性水平说明，从第二行开始是临界值）  \n    sig_levels = ['1%', '5%', '10%']  # 假设cvt的列标题是这样，实际情况可能不同  \n    col_index = sig_levels.index(f\"{significance_level*100:.0f}%\")  \n    if col_index == -1:  # 如果指定的显著性水平不在列表中，则默认使用5%  \n        col_index = 1  \n    print('cv[0]:',cv[0])\n    print('col_index:',col_index)\n    num_rejections = 0  \n    for value, c_value in zip(stat, cv[:, col_index]):  \n        if value &gt; c_value:  \n            num_rejections += 1  \n            print(f\"在{name}检验中，拒绝了不存在至少{num_rejections}个协整向量的原假设（统计量={value:.2f}，{significance_level*100:.0f}%临界值={c_value:.2f}）\")  \n        else:  \n            break  \n  \nprint_rejection(model.lr1, model.cvt, \"迹\",significance_level=0.05)\nprint_rejection(model.lr2, model.cvm, \"最大特征值\")  \n  \n# 得出结论  \n# 注意：由于迹统计量和最大特征值统计量可能会给出不同的结论，  \n# 我们通常根据其中一个或结合两者来得出结论。  \n# 在这里，我们简单地基于迹统计量的结果来得出结论。  \ncol_idx = 1\nprint('model.lr1 &gt; model.cvt[:, col_idx]:',model.lr1 &gt; model.cvt[:, col_idx])\nprint('np.any(model.lr1 &gt; model.cvt[:, col_idx]):',np.any(model.lr1 &gt; model.cvt[:, col_idx]))\nif np.any(model.lr1 &gt; model.cvt[:, col_idx]):  # 检查是否有拒绝发生  \n    num_cointegrating_vectors = np.sum(model.lr1 &gt; model.cvt[:, col_idx])  \n    print(f\"\\n结论：时间序列之间存在至少{num_cointegrating_vectors}个协整向量（基于迹统计量）。\")  \nelse:  \n    print(\"\\n结论：时间序列之间不存在协整向量（基于迹统计量）。\")  \n  \n# 注意：实际中，你可能需要更仔细地分析两个统计量的结果，  \n# 并考虑数据的性质、样本大小、滞后阶数的选择等因素来综合得出结论。\n\n          X1        X2        X3\n0   0.049671  0.858463  0.489846\n1   0.087012  0.952838  0.576004\n2   0.265418  0.945392  0.713710\n3   0.450717  0.874209  0.767843\n4   0.369721  0.903352  0.498769\n..       ...       ...       ...\n95 -0.316698 -0.946852 -0.701066\n96 -0.239231 -1.051570 -0.555440\n97 -0.338493 -0.915792 -0.596413\n98 -0.456126 -0.883832 -0.588693\n99 -0.567480 -0.953369 -0.697461\n\n[100 rows x 3 columns]\nJohansen协整检验结果:\n\n--- 迹统计量（Trace Statistics, lr1）---\n[144.34074724  83.97031191  37.69871001]\n\n--- 临界值（Critical Values，通常为1%、5%、10%显著性水平）---\n[[27.0669 29.7961 35.4628]\n [13.4294 15.4943 19.9349]\n [ 2.7055  3.8415  6.6349]]\n\n--- 最大特征值统计量（Maximum Eigenvalue Statistics, lr2）---\n[60.37043533 46.2716019  37.69871001]\n\n--- 临界值（Critical Values，通常为1%、5%、10%显著性水平）---\n[[18.8928 21.1314 25.865 ]\n [12.2971 14.2639 18.52  ]\n [ 2.7055  3.8415  6.6349]]\ncv[0]: [27.0669 29.7961 35.4628]\ncol_index: 1\n在迹检验中，拒绝了不存在至少1个协整向量的原假设（统计量=144.34，5%临界值=29.80）\n在迹检验中，拒绝了不存在至少2个协整向量的原假设（统计量=83.97，5%临界值=15.49）\n在迹检验中，拒绝了不存在至少3个协整向量的原假设（统计量=37.70，5%临界值=3.84）\ncv[0]: [18.8928 21.1314 25.865 ]\ncol_index: 1\n在最大特征值检验中，拒绝了不存在至少1个协整向量的原假设（统计量=60.37，5%临界值=21.13）\n在最大特征值检验中，拒绝了不存在至少2个协整向量的原假设（统计量=46.27，5%临界值=14.26）\n在最大特征值检验中，拒绝了不存在至少3个协整向量的原假设（统计量=37.70，5%临界值=3.84）\nmodel.lr1 &gt; model.cvt[:, col_idx]: [ True  True  True]\nnp.any(model.lr1 &gt; model.cvt[:, col_idx]): True\n\n结论：时间序列之间存在至少3个协整向量（基于迹统计量）。"
  },
  {
    "objectID": "posts/2021-08-16-backtesting.html",
    "href": "posts/2021-08-16-backtesting.html",
    "title": "CH3-回测",
    "section": "",
    "text": "A key difference between a traditional investment management process and a quantitative investment process is the possibility of back\u0002testing a quantitative investment strategy to see how it would have performed in the past\n量化投资流程和传统投资管理流程最重要的区别是：量化投资策略可以进行回测以观察它在过去的表现。"
  },
  {
    "objectID": "posts/2021-08-16-backtesting.html#常用的回测工具",
    "href": "posts/2021-08-16-backtesting.html#常用的回测工具",
    "title": "CH3-回测",
    "section": "常用的回测工具",
    "text": "常用的回测工具\n\nExcel &gt; The major disadvantage of Excel is that it can be used to backtest only fairly simple models.But, as I explained in the previous chapter, simple models are often the best!\n\nExcel的主要缺点是只能用于简单模型的回测。但是，简单模型往往是最好的。\n\nMATLAB\n\nMATLAB优点很多，功能强大，但是价格昂贵。有一些替代品:\nO-Matrix\nOctave\nScilab\n\nTradeStation\nHigh-End Backtesting Platforms"
  },
  {
    "objectID": "posts/2021-08-16-backtesting.html#寻找并使用历史数据库",
    "href": "posts/2021-08-16-backtesting.html#寻找并使用历史数据库",
    "title": "CH3-回测",
    "section": "寻找并使用历史数据库",
    "text": "寻找并使用历史数据库\n\nWhile finding sources of data on the Internet is even easier than finding prospective strategies, there are a number of issues and pitfalls with many of these databases that I will discuss later in this section. These issues apply mostly to stock and exchange-traded fund (ETF) data only. Here are the most important ones.\n\n虽然在互联网上寻找数据源比寻找有效的策略容易，但这些数据中存在问题和陷阱。下面是最主要的问题，这些问题仅适用股票和交易基金数据。\n\n数据是否已经过分拆和股息调整？\n是否剔除了幸存偏差数据？\n你的策略用最高最低价吗？\n\n\n使用最高最低价的回测没有使用开盘收盘价的回测可靠。\n\n\n业绩度量\n\n夏普比率\n\n年化夏普比率的计算：一般而言，假设每个交易时段的长度为T，T可以是1个月、1天、1小时等，若要计算平均收益率、标准差以及相应的年化指标，就必须先算出一年的交易时段数\\(N_T\\)\n\\(年化夏普比率= \\sqrt {N_T} * 基于T的夏普比率\\)\n年化标准差和日标准差的关系推理：\n\n其中cov是指协方差。\n最大挫跌和最长挫跌期\n\n这个指标算起来挺麻烦，从图中看的话，倒是挺直观。"
  },
  {
    "objectID": "posts/2021-08-16-backtesting.html#避免常见的回测陷阱",
    "href": "posts/2021-08-16-backtesting.html#避免常见的回测陷阱",
    "title": "CH3-回测",
    "section": "避免常见的回测陷阱",
    "text": "避免常见的回测陷阱\n\n前视偏差\n\n例如，“在日最低价的1%之内买入股票”的交易规则，就有前视偏差，因为在当日市场收盘前，是不可能知道日最低价的。\n\n数据迁就偏差\n\n因迁就历史数据的噪声而过度优化模型参数，造成策略的回测业绩高于未来业绩，即为数据迁就偏差。"
  },
  {
    "objectID": "posts/2021-08-16-backtesting.html#交易成本",
    "href": "posts/2021-08-16-backtesting.html#交易成本",
    "title": "CH3-回测",
    "section": "交易成本",
    "text": "交易成本\n没有考虑交易成本的回测业绩是不真实的。"
  },
  {
    "objectID": "posts/2021-08-16-backtesting.html#策略改进",
    "href": "posts/2021-08-16-backtesting.html#策略改进",
    "title": "CH3-回测",
    "section": "策略改进",
    "text": "策略改进\n对基本策略进行微小调整，来提升收益。策略的改进，最好基于经济学基本原理，或者透彻研究过的市场现象，而不是依据一些主观的试错法则。否则，就有可能产生数据迁就偏差。\n\nimport pandas as pd \nimport numpy as np\n\nxlsx = pd.ExcelFile(\"datas/example3_4.xls\")\ndf = pd.read_excel(xlsx,\"table\")\n# 计算每日收益率：\ndf['daily_ret'] = df['Adj Close'].pct_change()\n#假设年化收益率为4%，每年252个交易日 计算每日超额收益\ndf['excess_daily_ret'] = df['daily_ret'] - 0.04/252\n\nsharp_ratio = np.sqrt(252) * df['excess_daily_ret'].mean() / df['excess_daily_ret'].std()\n\nprint(sharp_ratio)\n\n0.7889300350874483"
  },
  {
    "objectID": "posts/2024-09-04-Algorithmic-Trading-5.html",
    "href": "posts/2024-09-04-Algorithmic-Trading-5.html",
    "title": "算法交易：制胜策略与原理(欧内斯特·陈)之五",
    "section": "",
    "text": "传统的智慧告诉我们：货币和期货是动量型交易者施展才华的领域，而这种传统的智慧所言是正确的。\n大多数货币对或期货对是不具有协整属性的，并且由货币或者期货所构建的大部分投资组合都没有表现出横截面式均值回归的特质。因此，在货币和期货交易中，应用均值回归之策略的机会是有限的，但并不是不存在。\n\n约翰森检验之特征向量相关的美元/澳元与美元/加元的配对交易\n在这里，两种货币之间的对冲比例不是一个值，所以我们不能将其用一个交叉汇率澳元/加元来替代进行相关的交易，同时，我们也不能分别对美元/澳元与加元/美元进行约翰森检验；我们应该对澳元/美元、加元/美元进行约翰森检验，从而使每种金融工具之美元的点数变化标准整齐划一，从直观的角度来说，此种模式也是说得通的，因为就均值回归的交易策略而言，如果加元/美元的价格比澳元/美元的价格低很多的话，那我们就该买入加元。\n我们假设输入两个T×1型的数组usdcad与usdaud，其各自代表自身每日的价格系列；T×1型的数组yport代表的是由澳元/美元与加元/美元所构建的投资组合的单位市值，且此市值以美元计价；numUnits代表的相应交易策略要求我们持有的投资组合的数量单位；这里还有一个T×2型的表示相应头寸的数组，其所表示的是我们应该拥有的澳元/美元与加元/美元之以美元计价的相关市值。自然，相应损益表（仍然是以美元计价）是每个金融工具之的市值总和乘以相应的收益率，投资组合的日间收益率等于损益表总值（P&L）除以前一天收盘时投资组合的总市值。\n\n# Example 5.1: Pair Trading USD.AUD vs USD.CAD Using the Johansen Eigenvector\n\nimport numpy as np\nimport pandas as pd\n#import matplotlib.pyplot as plt\n#import statsmodels.formula.api as sm\n#import statsmodels.tsa.stattools as ts\nimport statsmodels.tsa.vector_ar.vecm as vm\n\ndf1=pd.read_csv('datas/inputData_USDCAD_20120426.csv')\ndf1['Date']=pd.to_datetime(df1['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ndf1.rename(columns={'Close': 'CAD'}, inplace=True)\ndf1['CAD']=1/df1['CAD']\n\ndf2=pd.read_csv('datas/inputData_AUDUSD_20120426.csv')\ndf2['Date']=pd.to_datetime(df2['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ndf2.rename(columns={'Close': 'AUD'}, inplace=True)\n\ndf=pd.merge(df1, df2, how='inner', on='Date')\n\ndf.set_index('Date', inplace=True)\n\ntrainlen=250\nlookback=20\n\nhedgeRatio=np.full(df.shape, np.NaN)\nnumUnits=np.full(df.shape[0], np.NaN)\n\nfor t in range(trainlen+1, df.shape[0]):\n    # Johansen test\n    result=vm.coint_johansen(df.values[(t-trainlen-1):t-1], det_order=0, k_ar_diff=1)\n    hedgeRatio[t,:]=result.evec[:, 0]\n    yport=pd.DataFrame(np.dot(df.values[(t-lookback):t], result.evec[:, 0])) #  (net) market value of portfolio\n    ma=yport.mean()\n    mstd=yport.std()\n    numUnits[t]=-(yport.iloc[-1,:]-ma)/mstd  \n\npositions=pd.DataFrame(np.expand_dims(numUnits, axis=1)*hedgeRatio)*df.values # results.evec(:, 1)' can be viewed as the capital allocation, while positions is the dollar capital in each ETF.\npnl=np.sum((positions.shift().values)*(df.pct_change().values), axis=1)# daily P&L of the strategy\nret=pnl/np.sum(np.abs(positions.shift()), axis=1)\n(np.cumprod(1+ret)-1).plot()\nprint('APR=%f Sharpe=%f' % (np.prod(1+ret)**(252/len(ret))-1, np.sqrt(252)*np.mean(ret)/np.std(ret)))\n# APR=0.064512 Sharpe=1.362926\n\nC:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_20380\\4249882280.py:37: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n  numUnits[t]=-(float(yport.iloc[-1,:])-ma)/mstd\n\n\nAPR=0.064512 Sharpe=1.362926\n\n\n\n\n\n\n\n\n\n交叉货币对交易的一个特色是：如果你持有此货币对的隔夜单，那么，你所收入或支付利差是不同的。我们这里要注意的是：所谓“隔夜”的外汇交易意味着你所持有的头寸要一直延续到或超出美国东部时间的下午5：00点。如果我们持有一个货币对B/Q，且持之隔夜，那相应的利差就是iB-iQ，iB与iQ分别是货币B与回报Q的日利率——如果iQ&gt;iB，那么，这个利差也被称为展期的利息，而此利差实际是一种借息（例如，此项目要计入你账户的借方），实际上，由于相关清算系统运行机制的原因，如果你的买单/空单（对任意交叉货币对儿而言）所持有的时间超过了第T日美国东部时间下午5：00点的话，相应利差就应按T+2日的原则清算，而如果第T日是周末或假日，那相应利差就应按T+3日的原则清算。\n\n\n展期利息相关的澳元/加元的配对交易\n这里，我们继续使用线性均值回归的交易策略，但与例5-1相比，为了简单起见，我们选择交易一个现成的货币对澳元/加元，而不是美元/加元和澳元/美元。我们将考虑隔夜的展期利息，因为我们的交易策略之中的头寸持有时间超过美国东部时间5：00点。\n\n# Example 5.2: Pair Trading AUD.CAD with Rollover Interests\n\nimport numpy as np\nimport pandas as pd\n#import matplotlib.pyplot as plt\n#import statsmodels.formula.api as sm\n#import statsmodels.tsa.stattools as ts\n#import statsmodels.tsa.vector_ar.vecm as vm\n\ndf=pd.read_csv('datas/inputData_AUDCAD_20120426.csv')\n#df['Date']=pd.to_datetime(df['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ndf['Date']=pd.to_datetime(df['Date'],  format='%Y%m%d')\ndf.set_index('Date', inplace=True)\n\naud=pd.read_csv('datas/AUD_interestRate.csv')\naudindex=pd.PeriodIndex.from_fields(year=aud.Year, month=aud.Month, freq='M')\n#aud.index=audindex.to_timestamp().date\naud.index=audindex.to_timestamp()\n\ncad=pd.read_csv('datas/CAD_interestRate.csv')\ncadindex = pd.PeriodIndex.from_fields(year=cad.Year, month=cad.Month, freq='M')\n#cad.index=cadindex.to_timestamp().date\ncad.index=cadindex.to_timestamp()\n\ndf=pd.merge(df, aud, how='outer', left_index=True, right_index=True)\ndf.drop({'Year', 'Month'}, axis=1, inplace=True)\ndf.rename({'Rates': 'AUD_Rates'}, axis=1, inplace=True)\n\ndf=pd.merge(df, cad, how='outer', left_index=True, right_index=True)\ndf.drop({'Year', 'Month'}, axis=1, inplace=True)\ndf.rename({'Rates': 'CAD_Rates'}, axis=1, inplace=True)\n\n# df.fillna(method='ffill', axis=0, inplace=True)\ndf.ffill(inplace=True)\n# df.loc[:,{'AUD_Rates', 'CAD_Rates'}]=df.loc[:,{'AUD_Rates', 'CAD_Rates'}]/365/100 # convert from annual to daily rates\ncols = ['AUD_Rates', 'CAD_Rates']  \ndf[cols] = df[cols] / 365 / 100\n\nisWednesday=df.index.weekday==2\ndf.loc[isWednesday, 'AUD_Rates'] = df.loc[isWednesday, 'AUD_Rates'] * 3\n\nisThursday=df.index.weekday==3\ndf.loc[isThursday,'CAD_Rates']=df.loc[isThursday,'CAD_Rates']*3\n\nlookback=20\n\nma=df['Close'].rolling(lookback).mean()\nmstd=df['Close'].rolling(lookback).std()\nz=(df['Close']-ma)/mstd  \n\nret=-np.sign(z).shift()*(np.log(df['Close'])+(-np.log(df['Close'])+np.log(1+df['AUD_Rates'])-np.log(1+df['CAD_Rates'])).shift())\n(np.cumprod(1+ret)-1).plot()\nprint('APR=%f Sharpe=%f' % (np.prod(1+ret)**(252/len(ret))-1, np.sqrt(252)*np.mean(ret)/np.std(ret)))\n# APR=0.064719 Sharpe=0.610818\n\nAPR=0.064719 Sharpe=0.610818\n\n\n\n\n\n\n\n\n\n期货合约价格因各种到期日（或“期限”）的不一致而各不相同，它们各自的收益率也有少许的区别。将期限不同的期货合约进行配对，进而交易的模式被称为“跨期套利”。由于跨期套利所对应的标的资产具有两个价格，所以人们会认为此种交易可能为均值回归型的交易策略提供一个良好的机遇，但在现实中，它们不属于一般性质的均值回归交易。\n\n\n现货溢价相较于期货溢价的助记符\n这个助记符起源于约翰·梅纳德·凯恩斯（霍尔，1997），他和约翰·希克斯认为：对于真正拥有正常的、实务性的商品的那些人（即“套期保值者”，如农民或石油生产者）来说，他们倾向于通过卖空期货来对冲相应的头寸，冲销其所预期的亏损；与此同时，拥有净多头仓位的投机者也需要补偿相应的敞口风险，所以，他们会只购入连续收益为正值的期货合约，或与期货价格等值的，且低于期货现货价格的预期值的合约，也就是说，此种交易的情境是“现货溢价”式的。所以，我们应该记住“现货溢价”总是与“常态”相伴的，而“常态”就是指期货价格总是低于现货价格。\n当然，上述这种说法是不完全正确的，因为我们会发现：原油是一个完美的“常态”商品，而在在各个时期呈现的却是“期货溢价”的形式。但是，上述的解析给了我们一个良好的助记符。\n\n\n以固定收益模型预期现货收益及连续收益\n如果我们设定：现货收益率与连续收益率在相应时间序列之内是固定的常数，那么，我们就可以使用线性回归模式来估计它们的值。以前述这种方式，我们会很容易地找到现货收益率的值——我们只需要将现货价格的对数值相对于时间做回归处理即可，但是，如果要发现连续收益率的数值的话，我们就需要选一个固定的时间点，将期限不同的各个期货合约的价格相对于不同的到期日做相应的回归处理。在实践当中，相关的回归系数将取决于固定的时间点，同时取决于当期一组期货合约的价格。所以，尽管我们设定连续收益是常数，但最终我们还是会以预期缓慢变化之γ值的方式来结束相关过程。\n我们会将上述的程序应用于价格不同类别的期货，即巴西雷亚尔期货BR、玉米期货C、西得克萨斯（WTI）原油期货CL、铜期货HG、两年期美国国债期货TU。\n在下列过程中，我们设定：现货价格包含在一个τ×1型的数组spot之内，期货收盘价数据存储在τ×M数组cl之内，τ是交易日的步长，M是期货合同的数量。当然，不是所有合同在时间序列之内都能够存在，因此，我们将那些不存在的合约所对应的日期标示于数组NaN（NotaNumber）之内。\n\n# Example 5.3: Estimating Spot and Roll Returns Using the Constant Returns Model\nimport numpy as np\nimport pandas as pd\n#import matplotlib.pyplot as plt\nimport statsmodels.api as sm\n#import statsmodels.tsa.stattools as ts\n#import statsmodels.tsa.vector_ar.vecm as vm\n\ndf=pd.read_csv('datas/inputDataDaily_C2_20120813.csv')\ndf['Date']=pd.to_datetime(df['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ndf.set_index('Date', inplace=True)\n\n# Find spot prices\nspot=df['C_Spot']\ndf.drop('C_Spot', axis=1, inplace=True)\n\nT=sm.add_constant(range(spot.shape[0]))\n\nmodel=sm.OLS(np.log(spot), T)\nres=model.fit() # Note this can deal with NaN in top row\n\nprint('Average annualized spot return=', 252*res.params.iloc[1])\n#Average annualized spot return= 0.02805562210100287\n\n# Fitting gamma to forward curve\ngamma=np.full(df.shape[0], np.nan)\nfor t in range(df.shape[0]):\n    idx=np.where(np.isfinite(df.iloc[t, :]))[0]\n    idxDiff = np.roll(idx, -1) - idx \n    all_ones = all(idxDiff[0:4]==1) \n\n    if (len(idx)&gt;=5) and all_ones:\n        FT=df.iloc[t, idx[:5]]\n        T=sm.add_constant(range(FT.shape[0]))\n        model=sm.OLS(np.log(FT.values), T)\n        res=model.fit()\n        gamma[t]=-12*res.params[1]\n          \n       \npd.DataFrame(gamma).plot()\n\nprint('Average annualized roll return=', np.nanmean(gamma))\n# Average annualized roll return= -0.12775650227459556\n\nAverage annualized spot return= 0.02805562210100191\nAverage annualized roll return= -0.1277565022746007\n\n\n\n\n\n\n\n\n\n\n\n跨期套利具有均值回归属性吗\n跨期套利是构建一个投资组合，由一个期货合约的多头和另一个期货合约的空头所组成，且标的资产相同，但到期月份不一样。基于我们之前所掌握的点差，跨期套利似乎是非常适合均值回归交易的可选项，但两个仓位头寸所跟踪的是同一个标的资产吗？在这里，答案为是的。\n收益率系列（而不是价格系列）几乎总是具有均值回归的性质。我们正在考虑的不是期货的总收益，而是构成总收益的连续收益率，所以相应情况可能有所不同［尽管式（5-7）所表达模型的前提是现货收益率和连续收益率都是常数，不过，我们可以尝试将其应用于连续收益率是变量的情境之下］。我们以原油期货CL价格之日历价差的对数形式，运行期限为12个月的ADF检验模式，并且发现：相关组合确实是平稳的，且概率为99%，半衰期为36日。此外，如果我们应用正常的线性均值回归策略检测2008年1月2日至2012年8月13日的原油期货CL之日历价差的对数值，我们可以得到一个8.3%的年化收益率，且夏普比率为1.3，\n\n# Example 5.4: Mean Reversion Trading of Calendar Spreads\nimport numpy as np\nimport pandas as pd\n#import matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\n\n\ndf=pd.read_csv('datas/inputDataDaily_CL_20120502.csv')\ndf['Date']=pd.to_datetime(df['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ndf.set_index('Date', inplace=True)\n\n# Fitting gamma to forward curve\ngamma=np.full(df.shape[0], np.nan)\nfor t in range(df.shape[0]):\n    idx=np.where(np.isfinite(df.iloc[t, :]))[0]\n    idxDiff=np.array(list(set(idx[1:])-set(idx)))\n    if ((len(idx)&gt;=5) & (all(idxDiff[0:4]==1))):\n        FT=df.iloc[t, idx[:5]]\n        T=sm.add_constant(range(FT.shape[0]))\n        model=sm.OLS(np.log(FT.values), T)\n        res=model.fit()\n        gamma[t]=-12*res.params[1]\n          \nresults=adfuller(gamma[np.where(np.isfinite(gamma))], maxlag=1, regression='c', autolag=None)\nprint(results)\n#(-4.586684184658408, 0.00013666960538551907, 1, 1995, {'1%': -3.4336320721769433, '5%': -2.862989840784964, '10%': -2.56754183359401})\n\ngamma=pd.DataFrame(gamma)\n# gamma.fillna(method='ffill')\ngamma.ffill(inplace=True)\n\n#gammaGood=gamma.iloc[np.where(np.isfinite(gamma)), :]\ngammaGood=gamma[gamma.notna().values]\ngammalag=gammaGood.shift()\ndeltaGamma=gammaGood-gammalag\ndeltaGamma=deltaGamma[1:]\ngammalag=gammalag[1:]\n\nX=sm.add_constant(gammalag)\nmodel=sm.OLS(deltaGamma, X)\nres=model.fit()\nhalflife=-np.log(2)/res.params.iloc[1]\n# 41.095311903707795\n\nlookback=int(halflife)\nMA=gamma.rolling(lookback).mean()\nMSTD=gamma.rolling(lookback).std()\nzScore=(gamma-MA)/MSTD\n\npositions=np.zeros(df.shape)\nisExpireDate=np.isfinite(df) & ~np.isfinite(df.shift(-1));\nholddays=3*21\nnumDaysStart=holddays+10\nnumDaysEnd=10\nspreadMonth=12\n\nfor c in range(0, df.shape[1]-spreadMonth):\n    expireIdx=np.where(isExpireDate.iloc[:,c])[-1]\n    if c==0:\n        startIdx=max(0, expireIdx-numDaysStart)\n        endIdx=expireIdx-numDaysEnd    \n    else:\n        myStartIdx=endIdx+1\n        myEndIdx=expireIdx-numDaysEnd\n        if (myEndIdx-myStartIdx &gt;= holddays):\n            startIdx=myStartIdx\n            endIdx=myEndIdx\n        else:\n            startIdx=np.inf\n            \n    if ((len(expireIdx) &gt; 0) & (endIdx &gt; startIdx)):\n        positions[startIdx[0]:endIdx[0], c]=-1\n        positions[startIdx[0]:endIdx[0], c+spreadMonth]=1\n        \npositions[zScore.isna().values.flatten(), :]=0\nzScore.fillna(-np.inf, inplace=True)\n\npositions[zScore.values.flatten() &gt; 0, :]=-positions[zScore.values.flatten() &gt; 0, :]\npositions=pd.DataFrame(positions)\ndf_filled = df.ffill()\npnl=np.sum((positions.shift().values)*(df_filled.pct_change().values), axis=1) # daily P&L of the strategy\nret=pnl/np.sum(np.abs(positions.shift()), axis=1)\n(np.cumprod(1+ret)-1).plot()\n\nprint('APR=%f Sharpe=%f' % (np.prod(1+ret)**(252/len(ret))-1, np.sqrt(252)*np.mean(ret)/np.std(ret)))\n#APR=0.024347 Sharpe=1.275860\n\n(np.float64(-4.586684184658414), np.float64(0.0001366696053855159), 1, 1995, {'1%': np.float64(-3.4336320721769433), '5%': np.float64(-2.862989840784964), '10%': np.float64(-2.56754183359401)})\nAPR=0.024347 Sharpe=1.275860"
  },
  {
    "objectID": "posts/2024-08-22-Algorithmic-Trading-2.html",
    "href": "posts/2024-08-22-Algorithmic-Trading-2.html",
    "title": "算法交易：制胜策略与原理(欧内斯特·陈)之二",
    "section": "",
    "text": "我们所说的均值回归并不是指价格的波动，而是指相应收益率通常以均值为零的形式所进行的随机分布。\n在本章中，我所描述的测试和交易策略都是根据时间序列型均值回归模式定制的；有另一种均值回归模式，叫作“横断面”型均值回归，横截面型均值回归意味着一篮子的金融工具之累积收益率将恢复至整个篮子的累积收益水平，这也意味着：短期之内，相应金融工具之相对收益率是连续反相关的（一个金融工具的相对收益率是该项目的收益率减去整个组合篮子的收益率）。\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import adfuller\nimport statsmodels.api as sm\n# from genhurst import genhurst \n\ndef genhurst(z):\n# =============================================================================\n# calculation of Hurst exponent given log price series z\n# =============================================================================\n    z=pd.DataFrame(z)\n    # 计算一系列时间延迟 taus，这些时间延迟是时间序列长度的十分之一左右。时间延迟不能与时间序列长度相同，因为这会导致统计不稳定。\n    taus=np.arange(np.round(len(z)/10)).astype(int) # We cannot use tau that is of same magnitude of time series length \n    # 初始化一个空数组 logVar，用于存储每个时间延迟对应的对数方差。\n    logVar=np.empty(len(taus)) # log variance\n    # 对于每个时间延迟 tau，计算 z 的差分序列（即相邻元素之间的差异）的方差，并取对数。将结果存储在 logVar 数组中。\n    for tau in taus:\n        logVar[tau]=np.log(z.diff(tau).var(ddof=0))\n\n    # 创建两个数组 X 和 Y，分别表示时间延迟的对数和对应的对数方差。 \n    X=np.log(taus)    \n    Y=logVar[:len(taus)]\n    # 删除 logVar 中包含非有限值（如NaN或无穷大）的元素对应的 X 和 Y 中的元素。\n    X=X[np.isfinite(logVar)]\n    Y=Y[np.isfinite(logVar)]\n#    pd.DataFrame(np.asmatrix([X, Y]).T).to_csv('XY.csv')\n\n    X = sm.add_constant(X)\n    # plt.scatter(X[:,1], Y) # for debug only\n    # 使用 statsmodels 库中的 OLS 函数拟合一个线性回归模型，其中 Y 是因变量，X 是自变量（包括一个常数项）。\n    model=sm.OLS(Y, X)\n    results=model.fit()\n    # 从拟合结果中提取 Hurst 指数 H（即斜率的一半）和相应的 p 值 pVal\n    H=results.params[1]/2\n    pVal=results.pvalues[1]\n    return H, pVal\n\ndf=pd.read_csv('datas/inputData_USDCAD.csv')\n# print(df)\ndf.set_index('Date',inplace=True)\ny=df.loc[df['Time']==1659, 'Close']\n# 将索引转换为matplotlib可识别的日期格式\ny.index = pd.to_datetime(y.index,format='%Y%m%d')\n# print(y)\nplt.plot(y)\n\n# y：要检验的时间序列。\n# maxlag：最大滞后阶数，用于构建ADF检验的模型。这里设置为1。\n# regression：回归类型。'c'表示在回归模型中包含常数项。\n# autolag：自动选择滞后阶数。这里设置为None，表示不自动选择滞后阶数。\nresults=adfuller(y, maxlag=1, regression='c', autolag=None)\n# adfuller 函数是 statsmodels 库中的一个函数，用于执行 Augmented Dickey-Fuller (ADF) 单位根检验\n# Test Statistic（检验统计量）：这是用于检验单位根假设的统计量。如果此值小于临界值，则拒绝原假设，认为时间序列是平稳的。\n# p-value（p值）：这是检验统计量的概率值。如果 p 值小于预设的显著性水平（通常为 0.05），则拒绝原假设，认为时间序列是平稳的。\n# Critical Values（临界值）：这是一组用于与检验统计量进行比较的值。通常有三个临界值，分别对应于 1%、5% 和 10% 的显著性水平。如果检验统计量小于这些临界值，则拒绝原假设。\n# Number of Lags Used（使用的滞后阶数）：这是用于 ADF 检验的滞后阶数。滞后阶数的选择会影响检验的结果。\n# Number of Observations Used in Regression Analysis（回归分析中使用的观测值数量）：这是用于回归分析的观测值数量。\n# Critical Values (1%, 5%, and 10%)（临界值，1%、5% 和 10%）：这是一组用于与检验统计量进行比较的值，分别对应于 1%、5% 和 10% 的显著性水平。\n# ADF Statistic（ADF 统计量）：这是 ADF 检验的主要统计量，与 Test Statistic 相同。\n# p-value (Adjusted)（调整后的 p 值）：这是调整后的 p 值，考虑了滞后阶数的选择。\nprint(results)\n\n# Find Hurst exponent\nH, pVal=genhurst(np.log(y))\nprint(\"H=%f pValue=%f\" % (H, pVal))\n\n(-1.8430182830405568, 0.35932298598891743, 1, 1214, {'1%': -3.4357480073434905, '5%': -2.863923702481129, '10%': -2.568039121778048})\nH=0.475844 pValue=0.000000\n\n\nC:\\Users\\win10\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\nC:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_22572\\527508146.py:22: RuntimeWarning: divide by zero encountered in log\n  X=np.log(taus)\n\n\n\n\n\n\n\n\n\n(-1.8430182830405568, 0.35932298598891743, 1, 1214, {‘1%’: -3.4357480073434905, ‘5%’: -2.863923702481129, ‘10%’: -2.568039121778048}) 这个结果来自ADF单位根检验，以下是对结果的解读：\nADF统计量：-1.8430182830405568\np值：0.35932298598891743\n使用的滞后阶数：1\n观测值数量：1214\n临界值：{‘1%’: -3.4357480073434905, ‘5%’: -2.863923702481129, ‘10%’: -2.568039121778048}\n根据ADF单位根检验的结果，我们可以得出以下结论：\nADF统计量为-1.8430182830405568，小于5%的临界值-2.863923702481129，但大于10%的临界值-2.568039121778048。 p值为0.35932298598891743，大于预设的显著性水平0.05。\n由于ADF统计量大于10%的临界值，且p值大于0.05，我们不能拒绝原假设，即时间序列存在单位根，因此时间序列是非平稳的。\n将回归系数λ和均值回归的半衰期连接在一起对交易者来说非常有用，其原因是： * 第一，如果我们发现λ是正值，那么就意味着价格系列并不是均值回归的形态，甚至我们不应该试图以均值回归的策略去进行相关交易； * 第二，如果λ值非常接近于零，这意味着半衰期很长，运用均值回归的交易策略将不会很赚钱，因为我们无法在给在给定的时间范围内完成许多个回合的交易； * 第三，这个λ也可以为我们交易策略中的许多参数决定一个自然的时间尺度。\n\ndf=pd.read_csv('datas/inputData_USDCAD.csv')\n# print(df)\ndf.set_index('Date',inplace=True)\ny=df.loc[df['Time']==1659, 'Close']\n# 将索引转换为matplotlib可识别的日期格式\ny.index = pd.to_datetime(y.index,format='%Y%m%d')\ny = y[y.notna().values] #删除包含缺失值的行。\n# print(y)\n# plt.plot(y)\nylag = y.shift() #创建一个滞后变量\n# print(ylag)\ndeltay = y - ylag #计算收盘价的差分\n# print(deltay)\ndeltay = deltay[1:] #删除第一个NaN值\n# print(deltay)\n\n# print(ylag[1:])\nX=sm.add_constant(ylag[1:]) # 为滞后变量添加常数项\n# print(X)\nmodel=sm.OLS(deltay, X) #创建一个线性回归模型\nres=model.fit() #拟合模型\nhalflife=-np.log(2)/res.params[1] #计算半衰期\nprint('halflife:',halflife)\n\nhalflife: 115.20979448515476\n\n\n如果我们可以找到一个由几个非平稳的价格系列所构建的平稳的线性组合，那么，这些价格系列则被称之为协整形式。\n如果我们能结合两个或多个非平稳的价格系列组成平稳的投资组合，那么这些价格系列被称为协整。\n协整可以用CADF测试或约翰森测试进行测试。\n加强版的ADF检验（CADF）和约翰森检验的模式。前者是只适合一对价格系列，而后者则适用于任何数量的价格系列。\n一个特定资产的对冲比率是在投资组合中我们应该做多，或做空多大数量的单位资产——如果资产是股票，那么单位数量所对应的就是股票的股数，而一个负的对冲比率表明我们应该做空此类资产.\n首先通过运行两个价格系列之间的线性回归的相关性来确定最优的对冲比率，然后使用这种对冲比率形成相应的投资组合，最后在该组合之内对相应的价格系列进行平稳性的测试\nEWA：iShares MSCI Australia ETF，是一种交易所交易基金，旨在追踪澳大利亚股票市场的表现。\nEWC：iShares MSCI Canada ETF，同样是一种交易所交易基金，旨在追踪加拿大股票市场的表现。\nIGE：一个由自然资源类股票所构成的基金。\n\n# Using the CADF test for cointegration\n\nimport numpy as np\nimport pandas as pd\n#import matplotlib.pyplot as plt\nimport statsmodels.formula.api as sm\nimport statsmodels.tsa.stattools as ts\nimport statsmodels.tsa.vector_ar.vecm as vm\n\ndf=pd.read_csv('datas/inputData_EWA_EWC_IGE.csv')\ndf['Date']=pd.to_datetime(df['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ndf.set_index('Date', inplace=True)\n# 绘制EWA和EWC的时间序列图\ndf.plot()\n# 绘制EWA和EWC的散点图\ndf.plot.scatter(x='EWA', y='EWC')\n# 使用普通最小二乘法（OLS）拟合EWA和EWC之间的关系\nresults=sm.ols(formula=\"EWC ~ EWA\", data=df[['EWA', 'EWC']]).fit()\n# 这行代码打印出模型的参数，包括截距和斜率\nprint(results.params)\n# 计算对冲比率\nhedgeRatio=results.params[1]\nprint('hedgeRatio=%f' % hedgeRatio)\n# 绘制残差图\npd.DataFrame((df['EWC']-hedgeRatio*df['EWA'])).plot()\n\n# cadf test\ncoint_t, pvalue, crit_value=ts.coint(df['EWA'], df['EWC'])\n# coint_t：t统计量，用于检验两个序列是否协整。\n# pvalue：p值，用于判断t统计量的显著性。如果p值小于某个显著性水平（如0.05），则拒绝原假设（即两个序列不协整）。\n# crit_value：临界值，用于与t统计量进行比较。\nprint('t-statistic=%f' % coint_t)\nprint('pvalue=%f' % pvalue)\nprint('crit_value:',crit_value)\n\n# Johansen test\nresult=vm.coint_johansen(df[['EWA', 'EWC']].values, det_order=0, k_ar_diff=1)\n# result.lr1：第一个特征值的迹统计量。\n# result.cvt：第一个特征值的临界值。\n# result.lr2：第二个特征值的迹统计量。\n# result.cvm：第二个特征值的临界值。\n# 约翰森检验的输出结果包括迹统计量和相应的临界值，用于判断多个时间序列之间是否存在协整关系。如果迹统计量大于临界值，则拒绝原假设（即不存在协整关系）。\nprint(result)\nprint('result.lr1:',result.lr1)\nprint('result.cvt:',result.cvt)\nprint('result.lr2:',result.lr2)\nprint('result.cvm:',result.cvm)\n\n# Add IGE for Johansen test\nresult=vm.coint_johansen(df.values, det_order=0, k_ar_diff=1)\nprint(result)\nprint('result.lr1:',result.lr1)\nprint('result.cvt:',result.cvt)\nprint('result.lr2:',result.lr2)\nprint('result.cvm:',result.cvm)\n# 特征值\nprint('result.eig:',result.eig) # eigenvalues\n# 特征向量\nprint('result.evec:',result.evec) # eigenvectors\n\n# 计算投资组合的市值   result.evec[:, 0] 是一个NumPy数组操作，它表示从result.evec这个二维数组中取出所有行（用 : 表示）的第0列（用 0 表示）的元素\n# np.dot 是 NumPy 库中的一个函数，用于计算两个数组的点积。它可以用于计算两个向量的点积，或者计算一个矩阵与另一个矩阵的乘积。\n# 这里的\"市值\"并不是传统意义上的市值，而是通过主成分分析得到的一个度量值。这个度量值可以帮助我们理解数据的主要变化趋势和结构。\nyport=pd.DataFrame(np.dot(df.values, result.evec[:, 0])) #  (net) market value of portfolio\nylag=yport.shift()\ndeltaY=yport-ylag\ndf2=pd.concat([ylag, deltaY], axis=1)\ndf2.columns=['ylag', 'deltaY']\nregress_results=sm.ols(formula=\"deltaY ~ ylag\", data=df2).fit() # Note this can deal with NaN in top row\nprint(regress_results.params)\n\nhalflife=-np.log(2)/regress_results.params['ylag']\nprint('halflife=%f days' % halflife)\n\n#  Apply a simple linear mean reversion strategy to EWA-EWC-IGE\n # 设置回望期为上面找到的半衰期\nlookback=np.round(halflife).astype(int) #  setting lookback to the halflife found above\n# yport.rolling(lookback).mean() 计算了一个移动平均值，窗口大小为 lookback。这个移动平均值代表了在过去 lookback 个时间点的平均市值。\n# yport.rolling(lookback).std() 计算了一个移动标准差，窗口大小为 lookback。这个移动标准差代表了在过去 lookback 个时间点的市值波动程度。\n# (yport - yport.rolling(lookback).mean()) 计算了每个时间点的市值与其移动平均值的差值。正值表示市值高于平均值，负值表示市值低于平均值。\n# -(yport - yport.rolling(lookback).mean()) / yport.rolling(lookback).std() 计算了每个时间点的单位数量。正值表示买入，负值表示卖出。单位数量的计算公式为：(市值 - 移动平均值) / 移动标准差。这个公式实际上计算了一个标准化后的差值，使得单位数量在不同的时间点和不同的投资组合之间具有可比性。\nnumUnits =-(yport-yport.rolling(lookback).mean())/yport.rolling(lookback).std() # capital invested in portfolio in dollars.  movingAvg and movingStd are functions from epchan.com/book2\nprint('numUnits:',numUnits)\n# 计算了基于线性均值回归策略的投资组合的市值分布\n# result.evec[:, 0] 是一个一维数组，表示第一个特征向量。这个特征向量是从主成分分析（PCA）或其他类似方法中得到的。\n# np.expand_dims(result.evec[:, 0], axis=1).T 将一维数组转换为二维数组，并进行转置。这样做的目的是为了使其能够与 numUnits.values 相乘。\n# np.dot(numUnits.values, np.expand_dims(result.evec[:, 0], axis=1).T) 计算单位数量与特征向量的点积。这个点积表示每个投资组合单位在第一个特征向量方向上的投影。\n# np.dot(numUnits.values, np.expand_dims(result.evec[:, 0], axis=1).T) * df.values 将点积结果与原始数据相乘，得到每个投资组合单位在每个资产上的市值。\npositions=pd.DataFrame(np.dot(numUnits.values, np.expand_dims(result.evec[:, 0], axis=1).T)*df.values) # results.evec(:, 1)' can be viewed as the capital allocation, while positions is the dollar capital in each ETF.\nprint('positions:',positions)\nprint('np.expand_dims(result.evec[:, 0], axis=1).T : ',np.expand_dims(result.evec[:, 0], axis=1).T)\nprint('np.dot(numUnits.values, np.expand_dims(result.evec[:, 0], axis=1).T):',np.dot(numUnits.values, np.expand_dims(result.evec[:, 0], axis=1).T))\n# 计算了基于线性均值回归策略的投资组合的每日收益（P&L）\n# positions.shift().values：将 positions DataFrame 向前移动一个时间点，得到前一天的投资组合市值。\n# df.pct_change().values：计算原始数据（df）的每日百分比变化。\n# (positions.shift().values)*(df.pct_change().values)：将前一天的投资组合市值与每日百分比变化相乘，得到每日收益。\n# np.sum((positions.shift().values)*(df.pct_change().values), axis=1)：沿着列方向（axis=1）求和，得到每个时间点的总收益。\npnl=np.sum((positions.shift().values)*(df.pct_change().values), axis=1) # daily P&L of the strategy\n# pnl：上一行代码计算出的每日收益。\n# np.sum(np.abs(positions.shift()), axis=1)：计算前一天的投资组合市值的绝对值之和。\n# ret = pnl / np.sum(np.abs(positions.shift()), axis=1)：将每日收益除以前一天的投资组合市值的绝对值之和，得到每日收益率。\nret=pnl/np.sum(np.abs(positions.shift()), axis=1)\n# np.cumprod(1+ret)：计算每日收益率的累积乘积，得到累计收益率。\n# np.cumprod(1+ret)-1：将累计收益率减去1，得到累计收益。\npd.DataFrame((np.cumprod(1+ret)-1)).plot()\nprint('============')\nprint('np.prod(1+ret):',np.prod(1+ret))\nprint('252/len(ret):',252/len(ret))\n# np.prod() 是 NumPy 库中的一个函数，用于计算数组中所有元素的乘积。这个函数可以用于计算一组数值的乘积，例如计算一组收益率的乘积以得到累积收益率。\nprint('APR=%f Sharpe=%f' % (np.prod(1+ret)**(252/len(ret))-1, np.sqrt(252)*np.mean(ret)/np.std(ret)))\n# APR=0.125739 Sharpe=1.391310\n\nIntercept    6.411331\nEWA          0.962429\ndtype: float64\nhedgeRatio=0.962429\nt-statistic=-3.063528\npvalue=0.095866\ncrit_value: [-3.90376106 -3.34020915 -3.04728056]\n&lt;statsmodels.tsa.vector_ar.vecm.JohansenTestResult object at 0x000001E479B796D0&gt;\nresult.lr1: [19.98321869  3.98276124]\nresult.cvt: [[13.4294 15.4943 19.9349]\n [ 2.7055  3.8415  6.6349]]\nresult.lr2: [16.00045745  3.98276124]\nresult.cvm: [[12.2971 14.2639 18.52  ]\n [ 2.7055  3.8415  6.6349]]\n&lt;statsmodels.tsa.vector_ar.vecm.JohansenTestResult object at 0x000001E40D6A4490&gt;\nresult.lr1: [34.42862022 17.53171895  4.47102054]\nresult.cvt: [[27.0669 29.7961 35.4628]\n [13.4294 15.4943 19.9349]\n [ 2.7055  3.8415  6.6349]]\nresult.lr2: [16.89690127 13.06069841  4.47102054]\nresult.cvm: [[18.8928 21.1314 25.865 ]\n [12.2971 14.2639 18.52  ]\n [ 2.7055  3.8415  6.6349]]\nresult.eig: [0.01121626 0.00868086 0.00298021]\nresult.evec: [[ 0.7599635  -0.11204898  0.0789828 ]\n [-1.04602749 -0.5796762   0.26467204]\n [ 0.22330592  0.53159644 -0.09515547]]\nIntercept   -0.115768\nylag        -0.030586\ndtype: float64\nhalflife=22.662578 days\nnumUnits:              0\n0          NaN\n1          NaN\n2          NaN\n3          NaN\n4          NaN\n...        ...\n1495  1.266708\n1496  0.694832\n1497 -0.277333\n1498 -1.594578\n1499 -1.588591\n\n[1500 rows x 1 columns]\npositions:               0          1          2\n0           NaN        NaN        NaN\n1           NaN        NaN        NaN\n2           NaN        NaN        NaN\n3           NaN        NaN        NaN\n4           NaN        NaN        NaN\n...         ...        ...        ...\n1495  22.747467 -38.160335  11.280594\n1496  12.282373 -20.619697   6.125721\n1497  -4.843326   8.085019  -2.399788\n1498 -28.017310  46.219527 -13.694791\n1499 -27.682721  45.630541 -13.480184\n\n[1500 rows x 3 columns]\nnp.expand_dims(result.evec[:, 0], axis=1).T :  [[ 0.7599635  -1.04602749  0.22330592]]\nnp.dot(numUnits.values, np.expand_dims(result.evec[:, 0], axis=1).T): [[        nan         nan         nan]\n [        nan         nan         nan]\n [        nan         nan         nan]\n ...\n [-0.21076268  0.29009756 -0.06193002]\n [-1.21182137  1.66797282 -0.3560788 ]\n [-1.20727087  1.66170944 -0.35474169]]\n============\nnp.prod(1+ret): 2.0238397644099786\n252/len(ret): 0.168\nAPR=0.125739 Sharpe=1.402653\n\n\nC:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_15472\\4197053767.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  hedgeRatio=results.params[1]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n在约翰森检测结果中，我们应该期待第一协整关系是“最强”的，也就是说，相应均值回归的半衰期最短，进而使我们很自然地选择这个特征向量形成平稳的投资组合（特征向量可以确定ETF基金内各类资产的权重），同时，我们可以用与之前相同的方法发现其半衰期，并应对一个平稳的价格系列。\n均值回归之配对交易的背后通常都有一个基本的原理：为什么EWA基金要和EWC基金之间进行协整呢？这是因为加拿大和澳大利亚的经济中，占主导地位的是大宗商品交易。那么，股票指数型基金GDX为什么要和黄金GLD协整呢？这是因为金矿公司的市值是基于黄金价值的。即使一对协整关系破裂（停止协整），我们通常都要对其分崩离析的原因进行解析。例如，在第4章的分析中，我们发现GDX基金和黄金GLD的协整关系在2008年年初便处于解体的状态，而能源价格的高腾导致黄金的开采异常昂贵。\n交易策略相关的另一个极端情境是：依赖于基本面分析的投资者偏好于投资那些被低估并持有很多年的股票，耐心地等待它们的价格回归至“正常”的价值水平。\n短时间内完成交易对我们这一类交易者最有利，因为较短的时间尺度意味着每年有更高的交易数量，进而使我们的回测系统与实时交易具有更高的统计信心和更高的夏普比率，最终使我们的交易策略具有更高的复合收益率。\n看似具有很高一致性的均值回归的交易策略最终可能会失效。迈克尔·德弗指出，这种高度一致性经常会使相关的交易员过度自信，且过度举债（德弗，2011）（考虑一下长期的资本管理问题）。当均值回归交易策略突然崩坏，其原因也许是我们都是“事后诸葛亮”，而且，此类事件经常发生在我们以此策略获得一系列成功之后、加大交易杠杆之时，由此而产生的罕见的损失往往是非常痛苦的，有时是灾难性的。因此，所谓的风险管理的概念对均值回归型的交易者尤为重要，但也特别困难，因为通常意义上的止损是不能按逻辑进行部署的。\n\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport backtrader as bt\nimport statsmodels.tsa.vector_ar.vecm as vm\nimport statsmodels.formula.api as sm\nimport statsmodels.tsa.stattools as ts\n# 读取CSV文件\ndata_df = pd.read_csv('datas/inputData_EWA_EWC_IGE.csv')\n\n# 将日期列转换为datetime类型\ndata_df['Date'] = pd.to_datetime(data_df['Date'], format='%Y%m%d')\ndata_df.set_index('Date', inplace=True)\n\nresult=vm.coint_johansen(data_df.values, det_order=0, k_ar_diff=1)\nprint(\"特征值：\", result.eig)\nprint(\"特征向量：\", result.evec)\nprint(\"迹统计量：\", result.lr1)\nprint(\"迹统计量的临界值：\", result.cvt)\nprint(\"最大特征值统计量：\", result.lr2)\nprint(\"最大特征值统计量的临界值：\", result.cvm)\n\nprint(result.cvt[:, 0])\nprint('result.lr1 &gt; result.cvt[:, 0]:',result.lr1 &gt; result.cvt[:, 0])\nprint('np.any(result.lr1 &gt; result.cvt[:, 0]):',np.any(result.lr1 &gt; result.cvt[:, 0]))\n\n# 二维三列矩阵和特征向量相乘  即每种产品价格和占比相乘后相加 得到总市值\nprint('result.evec[:, 0]:',result.evec[:, 0])\nyport=pd.DataFrame(np.dot(data_df.values, result.evec[:, 0]),columns=['close']) \n# print(yport) 算出的值都是负值，整体上是空头，基本没法玩\nylag=yport.shift()\ndeltaY=yport-ylag\ndf2=pd.concat([ylag, deltaY], axis=1)\ndf2.columns=['ylag', 'deltaY']\nregress_results=sm.ols(formula=\"deltaY ~ ylag\", data=df2).fit() # Note this can deal with NaN in top row\nprint(regress_results.params)\n\nhalflife=-np.log(2)/regress_results.params['ylag']\nprint('halflife=%f days' % halflife)\nlookback=np.round(halflife).astype(int)\nnumUnits =-(yport-yport.rolling(lookback).mean())/yport.rolling(lookback).std() \n# print(numUnits)\n# 为yport增加索引列\nyport.index = data_df.index\n\nyport.plot()\n# print(yport.iloc[0])\n# data_df = data_df.reset_index()\ndata_EWA_new = data_df[['EWA']].rename(columns={'EWA':'close'})\ndata_EWC_new = data_df[['EWC']].rename(columns={'EWC':'close'})\ndata_IGE_new = data_df[['IGE']].rename(columns={'IGE':'close'})\n\n# print('data_EWA_new:',data_EWA_new)\n# 创建PandasData对象\ndata_EWA = bt.feeds.PandasData(dataname=data_EWA_new)\ndata_EWC = bt.feeds.PandasData(dataname=data_EWC_new)\ndata_IGE = bt.feeds.PandasData(dataname=data_IGE_new)\n\n\nclass TestStrategy(bt.Strategy):\n    def __init__(self, lookback, proportion ):\n        self.lookback = lookback\n        self.proportion  = proportion \n        self.ctg_price = self.datas[0].close\n        self.EWA = self.datas[1].close\n        self.EWC = self.datas[2].close\n        self.IGE = self.datas[3].close\n        print('__init__', lookback, proportion)\n        print('__init__',self.datas[0].buflen())\n        print('__init__',self.datas[1].buflen())\n        print('__init__',self.datas[2].buflen())\n        print('__init__',self.datas[3].buflen())\n        # print('__init__',self.datas[0].close[0])\n        print(type(self.proportion))\n        # 创建一个滚动窗口均值指标  Add a MovingAverageSimple indicator\n        self.mean = bt.indicators.SimpleMovingAverage(self.datas[0], period=self.lookback)\n        # 创建一个滚动窗口标准差指标  \n        self.std = bt.indicators.StandardDeviation(self.datas[0], period=self.lookback)\n        # 使用上面的指标计算标准化差异\n        # self.zscore = -(self.datas[0] - self.mean) / self.std\n        # self.zscore = bt.indicators.ZScore(self.datas[0].close, period=self.params.lookback)\n        print(\"Mean:\", self.mean.buflen())  # 注意：索引 0 可能在第一个数据点上不可用  此时均值并没有计算，只有在next中才会实时计算\n        print(\"Std Dev:\", self.std.buflen())  \n        # print(\"Z-Score:\", self.zscore.buflen())  \n\n    def next(self):\n        # 在这里编写你的策略逻辑\n        # 你可以使用 self.param1 和 self.param2 访问传递的参数\n        # print(self.data.datetime.date(0))\n        # print(self.datas[0].close[0])\n        # print('price:',self.data.close[0])\n        # print('price:',self.ctg_price[0])\n        # print('EWA price:',self.EWA[0])\n        # print('EWC price:',self.EWC[0])\n        # print('IGE price:',self.IGE[0])\n        # print(self.ctg_price[0])\n         # 检查是否有足够的数据点来计算均值和标准差  \n        if len(self.mean) &gt; 0 and len(self.std) &gt; 0:  \n            # 计算当前的 Z-score  \n            # 注意：我们需要确保标准差不为0（虽然在实际金融数据中这很少见）  \n            if self.std[0] != 0:  \n                z = -(self.ctg_price[0] - self.mean[0]) / self.std[0]  # 这个买入还是卖出问题还没闹清楚\n                self.zscore = z  # 存储 Z-score 值以便后续使用（如果需要的话）\n                # print('zscore:',self.zscore)\n                positions = self.zscore*self.proportion\n                # print('positions:',positions)\n                # 这里计算权重的方法并不确定\n                w1 = (positions[0]/np.abs(positions).sum()).round(2)\n                w2 = (positions[1]/np.abs(positions).sum()).round(2)\n                w3 = (positions[2]/np.abs(positions).sum()).round(2)\n                # print('w1:',w1)\n                # print('w2:',w2)\n                # print('w3:',w3)\n                if self.zscore &gt; 0:  # 买入\n                    # print(\"卖出！ Current Z-Score:\", z)\n                    # print('比例：',self.proportion)\n                    # self.sell(data=self.datas[0],size=1000)\n                    if w1 &gt; 0:\n                        self.buy(data=self.datas[1],size=1000*w1)\n                    else:\n                        self.sell(data=self.datas[1],size=1000*np.abs(w1))\n                    if w2 &gt; 0:\n                        self.buy(data=self.datas[2],size=1000*w2)\n                    else:\n                        self.sell(data=self.datas[2],size=1000*np.abs(w2))\n                    if w3 &gt; 0:\n                        self.buy(data=self.datas[3],size=1000*w3)\n                    else:\n                        self.sell(data=self.datas[3],size=1000*np.abs(w3))\n                    # print(self.sell())\n                elif self.zscore &lt; 0: # 卖出\n                    # print(\"买入！ Current Z-Score:\", z)\n                    # print('比例：',self.proportion)\n                    # self.buy(data=self.datas[0],size=1000)\n                    # print(self.buy())\n                    if w1 &gt; 0:\n                        self.sell(data=self.datas[1],size=1000*w1)\n                    else:\n                        self.buy(data=self.datas[1],size=1000*np.abs(w1))\n                    if w2 &gt; 0:\n                        self.sell(data=self.datas[2],size=1000*w2)\n                    else:\n                        self.buy(data=self.datas[2],size=1000*np.abs(w2))\n                    if w3 &gt; 0:\n                        self.sell(data=self.datas[3],size=1000*w3)\n                    else:\n                        self.buy(data=self.datas[3],size=1000*np.abs(w3))\n            else:  \n                print(\"Standard deviation is zero, cannot calculate Z-Score.\")\n\n# 创建 cerebro 实例\ncerebro = bt.Cerebro()\n# 初始资金 100,000,000\ncerebro.broker.setcash(100000000.0)\n# 佣金，双边各 0.0003\ncerebro.broker.setcommission(commission=0.0003)\n# 滑点：双边各 0.0001\ncerebro.broker.set_slippage_perc(perc=0.0001)\n# 这里有点问题，并没有得出分析结果\ncerebro.addanalyzer(bt.analyzers.TimeReturn, _name='pnl') # 返回收益率时序数据\ncerebro.addanalyzer(bt.analyzers.AnnualReturn, _name='_AnnualReturn') # 年化收益率\ncerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='_SharpeRatio') # 夏普比率\ncerebro.addanalyzer(bt.analyzers.DrawDown, _name='_DrawDown') # 回撤\n\ncerebro.addstrategy(TestStrategy, lookback=lookback, proportion=result.evec[:, 0])\n# 保存6位小数，解决策略中小数点位数边长的问题\nyport['close'] = yport['close'].round(6)\n# data_price =  bt.feeds.PandasData(dataname=yport, fromdate=datetime.datetime(2006, 4, 26), todate=datetime.datetime(2006, 4, 30))\ndata_price =  bt.feeds.PandasData(dataname=yport)\ncerebro.adddata(data_price)\ncerebro.adddata(data_EWA)\ncerebro.adddata(data_EWC)\ncerebro.adddata(data_IGE)\nprint(\"组合初始值:\",cerebro.broker.getvalue())\nresult = cerebro.run()\nprint(\"组合终结值:\",cerebro.broker.getvalue())\nprint('result:',result[0])\nstrat = result[0]\n# 返回日度收益率序列\ndaily_return = pd.Series(strat.analyzers.pnl.get_analysis())\n# 打印评价指标\nprint(\"--------------- AnnualReturn -----------------\")\nprint(strat.analyzers._AnnualReturn.get_analysis())\nprint(\"--------------- SharpeRatio -----------------\")\nprint(strat.analyzers._SharpeRatio.get_analysis())\nprint(\"--------------- DrawDown -----------------\")\nprint(strat.analyzers._DrawDown.get_analysis())\n\n特征值： [0.01121626 0.00868086 0.00298021]\n特征向量： [[ 0.7599635  -0.11204898  0.0789828 ]\n [-1.04602749 -0.5796762   0.26467204]\n [ 0.22330592  0.53159644 -0.09515547]]\n迹统计量： [34.42862022 17.53171895  4.47102054]\n迹统计量的临界值： [[27.0669 29.7961 35.4628]\n [13.4294 15.4943 19.9349]\n [ 2.7055  3.8415  6.6349]]\n最大特征值统计量： [16.89690127 13.06069841  4.47102054]\n最大特征值统计量的临界值： [[18.8928 21.1314 25.865 ]\n [12.2971 14.2639 18.52  ]\n [ 2.7055  3.8415  6.6349]]\n[27.0669 13.4294  2.7055]\nresult.lr1 &gt; result.cvt[:, 0]: [ True  True  True]\nnp.any(result.lr1 &gt; result.cvt[:, 0]): True\nresult.evec[:, 0]: [ 0.7599635  -1.04602749  0.22330592]\nIntercept   -0.115768\nylag        -0.030586\ndtype: float64\nhalflife=22.662578 days\n组合初始值: 100000000.0\n__init__ 23 [ 0.7599635  -1.04602749  0.22330592]\n__init__ 1500\n__init__ 1500\n__init__ 1500\n__init__ 1500\n&lt;class 'numpy.ndarray'&gt;\nMean: 0\nStd Dev: 0\n组合终结值: nan\nresult: &lt;__main__.TestStrategy object at 0x000001E40E83B5B0&gt;\n--------------- AnnualReturn -----------------\nOrderedDict([(2006, nan), (2007, nan), (2008, nan), (2009, nan), (2010, nan), (2011, nan), (2012, nan)])\n--------------- SharpeRatio -----------------\nOrderedDict([('sharperatio', nan)])\n--------------- DrawDown -----------------\nAutoOrderedDict([('len', 1477), ('drawdown', nan), ('moneydown', nan), ('max', AutoOrderedDict([('len', 1477), ('drawdown', 0.0), ('moneydown', 0.0)]))])"
  },
  {
    "objectID": "posts/2024-08-29-Algorithmic-Trading-3.html",
    "href": "posts/2024-08-29-Algorithmic-Trading-3.html",
    "title": "算法交易：制胜策略与原理(欧内斯特·陈)之三",
    "section": "",
    "text": "应用价差、价差的对数或相应比率所进行的配对交易\n我们用来计算对冲比率的方法属于线性回归的模式——从jplv7软件包中选取ols函数（ols指的是“正常最小二乘法”），当然，你也可以使用约翰森检验中的第一特征向量来计算相应的对冲比率。\n\n# Trading Price Spread\n\nimport numpy as np\nimport pandas as pd\n#import matplotlib.pyplot as plt\nimport statsmodels.formula.api as sm\nimport statsmodels.tsa.stattools as ts\n#import statsmodels.tsa.vector_ar.vecm as vm\n\ndf=pd.read_csv('datas/inputData_GLD_USO.csv')\ndf['Date']=pd.to_datetime(df['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ndf.set_index('Date', inplace=True)\n\nlookback=20\nhedgeRatio=np.full(df.shape[0], np.nan)\n# 计算对冲比率\nfor t in np.arange(lookback, len(hedgeRatio)):\n    regress_results=sm.ols(formula=\"USO ~ GLD\", data=df[(t-lookback):t]).fit() # Note this can deal with NaN in top row\n    hedgeRatio[t-1]=regress_results.params.iloc[1]\nprint('hedgeRatio:',hedgeRatio)\nprint('ts.add_constant(-hedgeRatio):',ts.add_constant(-hedgeRatio))\n# 计算投资组合净值\nyport=np.sum(ts.add_constant(-hedgeRatio)[:, [1,0]]*df, axis=1)\nyport.plot()\n\n# Apply a simple linear mean reversion strategy to GLD-USO\n# 实现简单的线性均值回归策略\n\n# 计算投资单位数。这个公式实际上是计算yport与其移动平均值的偏差，然后除以其标准差。\n# 这个值表示相对于其历史平均表现，今天的投资组合表现有多好（或有多差）。正值表示今天的表现优于平均水平，负值表示今天的表现低于平均水平。\n# 乘以-1是为了反转符号，使得正值表示买入，负值表示卖出。\nnumUnits =-(yport-yport.rolling(lookback).mean())/yport.rolling(lookback).std() # capital invested in portfolio in dollars.  movingAvg and movingStd are functions from epchan.com/book2\nprint('numUnits:',numUnits)\n# np.tile(numUnits.values, [2, 1]):\n# np.tile函数用于重复数组的元素以创建新的数组。\n# numUnits.values是一个一维数组（假设numUnits只有一列），[2, 1]表示在行方向上重复两次，在列方向上重复一次。\n# 结果是一个二维数组，其中第一行是numUnits.values的复制，第二行也是numUnit.values的复制。\npositions=pd.DataFrame(np.tile(numUnits.values, [2, 1]).T * ts.add_constant(-hedgeRatio)[:, [1,0]] *df.values) # results.evec(:, 1)' can be viewed as the capital allocation, while positions is the dollar capital in each ETF.\npnl=np.sum((positions.shift().values)*(df.pct_change().values), axis=1) # daily P&L of the strategy\nret=pnl/np.sum(np.abs(positions.shift()), axis=1)\n# np.cumprod 是 NumPy 库中的一个函数，用于计算数组的累积积（cumulative product）。\n# 累积积是指从数组的第一个元素开始，逐个元素相乘，直到当前元素为止的所有元素的乘积\npd.DataFrame((np.cumprod(1+ret)-1)).plot()\nprint('APR=%f Sharpe=%f' % (np.prod(1+ret)**(252/len(ret))-1, np.sqrt(252)*np.mean(ret)/np.std(ret)))\n\nhedgeRatio: [       nan        nan        nan ... 0.15994561 0.18147442        nan]\nts.add_constant(-hedgeRatio): [[ 1.                 nan]\n [ 1.                 nan]\n [ 1.                 nan]\n ...\n [ 1.         -0.15994561]\n [ 1.         -0.18147442]\n [ 1.                 nan]]\nnumUnits: Date\n2006-04-26         NaN\n2006-04-27         NaN\n2006-04-28         NaN\n2006-05-01         NaN\n2006-05-02         NaN\n                ...   \n2012-04-02    0.885846\n2012-04-03    1.690156\n2012-04-04    2.805475\n2012-04-05    2.581271\n2012-04-09   -1.475398\nLength: 1500, dtype: float64\nAPR=0.109551 Sharpe=0.598899\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Trading Log Price Spread\n# 使用价格对数值的差分模式所产生的效应\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as sm\nimport statsmodels.tsa.stattools as ts\n\ndf=pd.read_csv('datas/inputData_GLD_USO.csv')\ndf['Date']=pd.to_datetime(df['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ndf.set_index('Date', inplace=True)\n\nlookback=20\nhedgeRatio=np.full(df.shape[0], np.nan)\nfor t in np.arange(lookback, len(hedgeRatio)):\n    # regress_results=sm.ols(formula=\"USO ~ GLD\", data=df[(t-lookback):t]).fit()\n    regress_results=sm.ols(formula=\"USO ~ GLD\", data=np.log(df[(t-lookback):t])).fit() # Note this can deal with NaN in top row\n    hedgeRatio[t-1]=regress_results.params.iloc[1]\n# yport=np.sum(ts.add_constant(-hedgeRatio)[:, [1,0]]*df, axis=1)\nyport=np.sum(ts.add_constant(-hedgeRatio)[:, [1,0]]*np.log(df), axis=1)\nyport.plot()\n\n# Apply a simple linear mean reversion strategy to GLD-USO\nnumUnits =-(yport-yport.rolling(lookback).mean())/yport.rolling(lookback).std() # capital invested in portfolio in dollars.  movingAvg and movingStd are functions from epchan.com/book2\npositions=pd.DataFrame(np.tile(numUnits.values, [2, 1]).T * ts.add_constant(-hedgeRatio)[:, [1,0]] ) #  positions is the dollar capital in each ETF.\npnl=np.sum((positions.shift().values)*(df.pct_change().values), axis=1) # daily P&L of the strategy\nret=pnl/np.sum(np.abs(positions.shift()), axis=1)\npd.DataFrame(np.cumprod(1+ret)-1).plot()\nprint('APR=%f Sharpe=%f' % (np.prod(1+ret)**(252/len(ret))-1, np.sqrt(252)*np.mean(ret)/np.std(ret)))\n\nAPR=0.099520 Sharpe=0.553442\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Trading Ratio\n# 相应的价格比率模式所生成的效应与价差模式和自适应对冲比率模式相比较而言，其看起来根本就不是不平稳的。所以，如果我们发现相关的均值回归模式的表现不佳，且生成负的年化收益率时，我们都不应该感到惊讶，\n\nimport numpy as np\nimport pandas as pd\n\ndf=pd.read_csv('datas/inputData_GLD_USO.csv')\ndf['Date']=pd.to_datetime(df['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ndf.set_index('Date', inplace=True)\n\nlookback=20\nratio=df['USO']/df['GLD']\nratio.plot()\n\n#yport=np.sum(ts.add_constant(-hedgeRatio)[:, [1,0]]*np.log(df), axis=1)\n#yport.plot()\n\n# Apply a simple linear mean reversion strategy to GLD-USO\nnumUnits =-(ratio-ratio.rolling(lookback).mean())/ratio.rolling(lookback).std() # capital invested in portfolio in dollars.  movingAvg and movingStd are functions from epchan.com/book2\npositions=pd.DataFrame(np.tile(numUnits.values, [2, 1]).T * np.ones((numUnits.shape[0], 2)) * np.array([-1, 1]) ) # positions in dollar invested\npnl=np.sum((positions.shift().values)*(df.pct_change().values), axis=1) # daily P&L of the strategy\nret=pnl/np.sum(np.abs(positions.shift()), axis=1)\n# (np.cumprod(1+ret)-1).plot()\npd.DataFrame((np.cumprod(1+ret)-1)).plot()\nprint('APR=%f Sharpe=%f' % (np.prod(1+ret)**(252/len(ret))-1, np.sqrt(252)*np.mean(ret)/np.std(ret)))\n\nAPR=-0.140674 Sharpe=-0.749583\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Bollinger Band Mean Reversion Strategy\n# 布林线带均值回归策略\nimport numpy as np\nimport pandas as pd\n#import matplotlib.pyplot as plt\nimport statsmodels.formula.api as sm\nimport statsmodels.tsa.stattools as ts\n#import statsmodels.tsa.vector_ar.vecm as vm\n\ndf=pd.read_csv('datas/inputData_GLD_USO.csv')\ndf['Date']=pd.to_datetime(df['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ndf.set_index('Date', inplace=True)\n\nlookback=20\nhedgeRatio=np.full(df.shape[0], np.nan)\nfor t in np.arange(lookback, len(hedgeRatio)):\n    regress_results=sm.ols(formula=\"USO ~ GLD\", data=df[(t-lookback):t]).fit() # Note this can deal with NaN in top row\n    hedgeRatio[t-1]=regress_results.params.iloc[1]\n\nyport=np.sum(ts.add_constant(-hedgeRatio)[:, [1,0]]*df, axis=1)\nyport.plot()\n# 这里我们需要注意的是：多单入场信号longsEntry与空单入场信号shortsEntry是T×1型的逻辑数组，多单离场信号longsExit和空单离场信号shortsExit也是如此。\n# 我们以numUnitsLong函数将与多单相关的单位投资组合的数量单位进行初始化的处理，这是一个T×1型的数组，然后，在出现多单入场信号之时，设置其值为1，而在多单离场信号出现之时，则设置其值为0；\n# 而对于空单的数量单位，我们则进行反向设置即可；在没有入场、离场信号的时间序列之内，我们使用失踪数据填充函数fillMissingData来提取前一天的数量单位（fillMissingData函数从数组的第二行开始，\n# 并以前一行各单元格的价值覆盖本期单元格的非数变量（NaN值），相应程序可以从我的网站下载）。一旦我们计算出代表多单数量单位的numUnitsLong函数和代表空单单位数量的numUnitsShort函数的数值，\n# 我们就可以计算相关数量单位的净值。\n# Bollinger band strategy\nentryZscore=1\nexitZscore=0\n\nMA=yport.rolling(lookback).mean()\nMSTD=yport.rolling(lookback).std()\nzScore=(yport-MA)/MSTD\n\nlongsEntry=zScore &lt; -entryZscore\nlongsExit =zScore &gt; -entryZscore\n\nshortsEntry=zScore &gt; entryZscore\nshortsExit =zScore &lt; exitZscore\n\nnumUnitsLong=np.zeros(longsEntry.shape)\nnumUnitsLong[:]=np.nan\n\nnumUnitsShort=np.zeros(shortsEntry.shape)\nnumUnitsShort[:]=np.nan\n\nnumUnitsLong[0]=0\nnumUnitsLong[longsEntry]=1\nnumUnitsLong[longsExit]=0\nnumUnitsLong=pd.DataFrame(numUnitsLong)\n# numUnitsLong.fillna(method='ffill', inplace=True)\nnumUnitsLong.ffill(inplace=True)\n\nnumUnitsShort[0]=0\nnumUnitsShort[shortsEntry]=-1\nnumUnitsShort[shortsExit]=0\nnumUnitsShort=pd.DataFrame(numUnitsShort)\n# numUnitsShort.fillna(method='ffill', inplace=True)\nnumUnitsShort.ffill(inplace=True)\n\nnumUnits=numUnitsLong+numUnitsShort\npositions=pd.DataFrame(np.tile(numUnits.values, [1, 2]) * ts.add_constant(-hedgeRatio)[:, [1,0]] *df.values) #  [hedgeRatio -ones(size(hedgeRatio))] is the shares allocation, [hedgeRatio -ones(size(hedgeRatio))].*y2 is the dollar capital allocation, while positions is the dollar capital in each ETF.\npnl=np.sum((positions.shift().values)*(df.pct_change().values), axis=1) # daily P&L of the strategy\nret=pnl/np.sum(np.abs(positions.shift()), axis=1)\npd.DataFrame((np.cumprod(1+ret)-1)).plot()\n\nprint('APR=%f Sharpe=%f' % (np.prod(1+ret)**(252/len(ret))-1, np.sqrt(252)*np.mean(ret)/np.std(ret)))\n\nAPR=0.149725 Sharpe=1.110355\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n对于一对真正的、具有协整性质的价格系列来说，确定对冲比率的方法非常简单：只要我们能够找到尽可能多的历史数据，然后，使用回归拟合相关的普通最小二乘法（OLS），或使用约翰森检验法则找到特征向量就可以了。\n对于到目前为止我们所讨论过的所有均值回归之交易策略而言，我们只是提取一个移动的回溯期，然后在此期限内计算回归系数，或特定的约翰森检验相关的特征向量，不过，此种方法有一个缺陷，即如果回溯周期过短，那么，随着时间的推进，我们需要删除初始的价格以及最新的价格，如此则会对相关比率产生一个突然的、人为的冲击；如果我们使用移动平均线，或者移动标准差来计算当前价格系列的平均值和标准差，我们也会面临相同的问题。 其实，指数加权移动平均线（EMA）就相当于前述的这种加权方式，但是，目前关于“为什么指数权重下降就是一个最优化的指标”这个问题，尚无比较清晰的答案。这里，我们将描述一个使用卡尔曼过滤法则更新对冲比率的模式，此模式可以避免因随意挑选一个加权方案而产生误差的问题（Montana、Triantafyllopoulos和Tsagaris，2009）。\n卡尔曼过滤法是最优化的线性算法，它能够基于一个可观测变量的最新值更新一个隐变量的期望值（关于这个话题，我们可以参看“卡尔曼，2007”）。此法则是线性的，因为它假定可观测变量是与噪声相关之隐变量的函数，同时，此法则还假设t时刻所对应的隐变量是其本身于t-1时刻（与噪声变量相关）的线性函数，而出现在这些函数中的噪声变量具有高斯分布的性质（因此，我们可以指定一个渐变式的协方差矩阵，这里我们假设相应的均值都是零）。因为所有的这些线性关系，t时刻相对应的隐变量的期望值也是其自身于观测期t时刻之前的期望值的一个线性函数，同时，它也是于t时刻所能观测到的变量的数值的线性函数。如果我们把噪声的分布假定为高斯分布，则卡尔曼过滤法就是一种最优化的方法，并且可以将相应的预期变量的均方误差最小化。\n对于卡尔曼过滤法则的每一次应用而言，我们需要先弄清楚相关的变量和矩阵： * （1）可观测变量（向量） * （2）隐变量（向量） * （3）状态转换模型（矩阵） * （4）观测模型（矩阵）\n上述模式是相关应用程序当中唯一具有创造性的部分，因为一旦相关数值被指定之后，那剩下的就是一个机械的应用算法了。作为交易者，我们不需要知道如何推导这些物理变量之间的关系，我们只需要知道在哪里能够找到好的软件包，进而为我们提供正确的答案。\n\n# Kalman Filter Mean Reversion Strategy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n#import statsmodels.formula.api as sm\nimport statsmodels.tsa.stattools as ts\n#import statsmodels.tsa.vector_ar.vecm as vm\n\ndf=pd.read_csv('datas/inputData_EWA_EWC_IGE.csv')\n# 删除列'IGE'\ndf = df.drop('IGE', axis=1)\n\ndf['Date']=pd.to_datetime(df['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ndf.set_index('Date', inplace=True)\n\nx=df['EWA']\ny=df['EWC']\n# 对x进行预处理，添加一列常数项以适应可能的截距偏移\nx=np.array(ts.add_constant(x))[:, [1,0]] # Augment x with ones to  accomodate possible offset in the regression between y vs x.\n\n# 初始化一些变量，如卡尔曼滤波器的状态协方差矩阵R、预测误差协方差矩阵P、回归系数beta、过程噪声协方差矩阵Vw和观测噪声协方差矩阵Ve。\ndelta=0.0001 # delta=1 gives fastest change in beta, delta=0.000....1 allows no change (like traditional linear regression).\n\nyhat=np.full(y.shape[0], np.nan) # measurement prediction\ne=yhat.copy()\nQ=yhat.copy()\n\n# For clarity, we denote R(t|t) by P(t). Initialize R, P and beta.\nR=np.zeros((2,2))\nP=R.copy()\nbeta=np.full((2, x.shape[0]), np.nan)\nVw=delta/(1-delta)*np.eye(2)\nVe=0.001\n\n# Initialize beta(:, 1) to zero\nbeta[:, 0]=0\n# 使用卡尔曼滤波器迭代更新回归系数beta。在每个时间步长t，首先根据上一时刻的beta值更新R，然后计算预测值yhat、预测误差e和观测噪声协方差Q。接着计算卡尔曼增益K，并使用它来更新beta和P\n# Given initial beta and R (and P)\nfor t in range(len(y)):\n    if t &gt; 0:\n        beta[:, t]=beta[:, t-1]\n        R=P+Vw\n            \n    yhat[t]=np.dot(x[t, :], beta[:, t])\n#    print('FIRST: yhat[t]=', yhat[t])\n    \n    Q[t]=np.dot(np.dot(x[t, :], R), x[t, :].T)+Ve\n#    print('Q[t]=', Q[t])\n\n    # Observe y(t)\n    e[t]=y.iloc[t]-yhat[t] # measurement prediction error\n#    print('e[t]=', e[t])\n#    print('SECOND: yhat[t]=', yhat[t])\n\n    \n    K=np.dot(R, x[t, :].T)/Q[t] #  Kalman gain\n#    print(K)\n    \n    beta[:, t]=beta[:, t]+np.dot(K, e[t]) #  State update. Equation 3.11\n#    print(beta[:, t])\n    \n    # P=R-np.dot(np.dot(K, x[t, :]), R) # State covariance update. Euqation 3.12\n    P=R-np.dot(np.outer(K, x[t, :]), R) # Thanks to Matthias for chaning np.dot -&gt; np.outer!\n\n#    print(R)\n\nplt.plot(beta[0, :])\nplt.plot(beta[1, :])\nplt.plot(e[2:])\nplt.plot(np.sqrt(Q[2:]))\n\n# 根据预测误差e和观测噪声协方差Q来确定买入和卖出的信号（longsEntry、longsExit、shortsEntry和shortsExit）。\nlongsEntry=e &lt; -np.sqrt(Q)\nlongsExit =e &gt; 0\n\nshortsEntry=e &gt; np.sqrt(Q)\nshortsExit =e &lt; 0\n\n# 计算每个时间步长的持仓数量（numUnitsLong和numUnitsShort），并将它们合并为一个总持仓数量numUnits。\nnumUnitsLong=np.zeros(longsEntry.shape)\nnumUnitsLong[:]=np.nan\n\nnumUnitsShort=np.zeros(shortsEntry.shape)\nnumUnitsShort[:]=np.nan\n\nnumUnitsLong[0]=0\nnumUnitsLong[longsEntry]=1\nnumUnitsLong[longsExit]=0\nnumUnitsLong=pd.DataFrame(numUnitsLong)\n# numUnitsLong.fillna(method='ffill', inplace=True)\nnumUnitsLong.ffill(inplace=True)\n\nnumUnitsShort[0]=0\nnumUnitsShort[shortsEntry]=-1\nnumUnitsShort[shortsExit]=0\nnumUnitsShort=pd.DataFrame(numUnitsShort)\n# numUnitsShort.fillna(method='ffill', inplace=True)\nnumUnitsShort.ffill(inplace=True)\n\nnumUnits=numUnitsLong+numUnitsShort\n# 根据持仓数量和回归系数beta构建投资组合，并计算每日盈亏pnl和投资组合的累计收益率。\npositions=pd.DataFrame(np.tile(numUnits.values, [1, 2]) * ts.add_constant(-beta[0,:].T)[:, [1,0]] *df.values) #  [hedgeRatio -ones(size(hedgeRatio))] is the shares allocation, [hedgeRatio -ones(size(hedgeRatio))].*y2 is the dollar capital allocation, while positions is the dollar capital in each ETF.\npnl=np.sum((positions.shift().values)*(df.pct_change().values), axis=1) # daily P&L of the strategy\nret=pnl/np.sum(np.abs(positions.shift()), axis=1)\n(np.cumprod(1+ret)-1).plot()\nprint('APR=%f Sharpe=%f' % (np.prod(1+ret)**(252/len(ret))-1, np.sqrt(252)*np.mean(ret)/np.std(ret)))\n#APR=0.313225 Sharpe=3.464060\n\nAPR=0.305703 Sharpe=3.314831"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html",
    "title": "算法交易中的一些概念",
    "section": "",
    "text": "高斯分布，也称为正态分布，是一种在自然界和社会科学中广泛存在的概率分布。它以数学家卡尔·弗里德里希·高斯的名字命名，因其图形呈钟形曲线而得名“钟形曲线”。以下是详细介绍：\n高斯分布的基本原理\n定义：高斯分布是一种连续概率分布，其图形显示为对称于平均值的钟形曲线。\n数学表达：若随机变量X服从数学期望为μ、方差为σ²的正态分布，记为N(μ，σ²)。\n应用领域：高斯分布在自然科学、工程学和社会科学等领域中广泛应用，用于描述连续型的随机变量。\n高斯分布的特性\n对称性：分布曲线关于平均值μ对称。\n集中性：大部分数据集中在平均值附近，离平均值越远，数据出现的概率越低。\n数学特性：高斯分布的曲线由均值μ和标准差σ决定，其中标准差σ越小，分布越集中；σ越大，分布越分散。\n高斯分布的应用\n统计学：在统计学中，高斯分布是描述连续型随机变量的重要工具，如测量误差分析。\n机器学习：作为许多机器学习算法（如线性回归、聚类分析）的基础假设。\n金融领域：用于风险评估和资产定价。\n图像处理：在图像处理中，高斯分布用于噪声模型和图像平滑。\n自然和社会现象：高斯分布在描述人口智力、身高、体重等自然和社会现象中无处不在。"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#高斯分布",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#高斯分布",
    "title": "算法交易中的一些概念",
    "section": "",
    "text": "高斯分布，也称为正态分布，是一种在自然界和社会科学中广泛存在的概率分布。它以数学家卡尔·弗里德里希·高斯的名字命名，因其图形呈钟形曲线而得名“钟形曲线”。以下是详细介绍：\n高斯分布的基本原理\n定义：高斯分布是一种连续概率分布，其图形显示为对称于平均值的钟形曲线。\n数学表达：若随机变量X服从数学期望为μ、方差为σ²的正态分布，记为N(μ，σ²)。\n应用领域：高斯分布在自然科学、工程学和社会科学等领域中广泛应用，用于描述连续型的随机变量。\n高斯分布的特性\n对称性：分布曲线关于平均值μ对称。\n集中性：大部分数据集中在平均值附近，离平均值越远，数据出现的概率越低。\n数学特性：高斯分布的曲线由均值μ和标准差σ决定，其中标准差σ越小，分布越集中；σ越大，分布越分散。\n高斯分布的应用\n统计学：在统计学中，高斯分布是描述连续型随机变量的重要工具，如测量误差分析。\n机器学习：作为许多机器学习算法（如线性回归、聚类分析）的基础假设。\n金融领域：用于风险评估和资产定价。\n图像处理：在图像处理中，高斯分布用于噪声模型和图像平滑。\n自然和社会现象：高斯分布在描述人口智力、身高、体重等自然和社会现象中无处不在。"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#学生t分布",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#学生t分布",
    "title": "算法交易中的一些概念",
    "section": "学生t分布",
    "text": "学生t分布\n学生t分布，也称为Student’s t-distribution，是一种连续概率分布，它在统计学中特别重要，尤其是在小样本情况下估计呈正态分布且标准差未知的总体均值时。以下是关于学生t分布理论的详细介绍：\n\n学生t分布的由来\n学生t分布最早由英国统计学家威廉·塞弗顿(William Sealy Gosset)在1908年提出，当时他使用笔名“学生”发表了自己的研究成果。Gosset的工作是在酿酒厂进行的，他需要分析小样本数据，因此开发了t分布来解决小样本量下的统计问题。\n\n\n学生t分布的定义\nt分布的概率密度函数(PDF)定义为：\n\\[f(t) = \\frac{\\Gamma\\left(\\frac{v+1}{2}\\right)}{\\sqrt{v\\pi}}\\left(1+\\frac{t^2}{v}\\right)^{-\\frac{v+1}{2}}\\]\n其中，\\(v\\) 是自由度，是一个正整数。\n\n\n学生t分布的性质\n\n对称性：t分布以0为中心，左右对称。\n自由度的影响：自由度越大，t分布的形状越接近于标准正态分布，尾部越窄；自由度越小，尾部越厚，形状更加扁平。\n应用：t分布广泛应用于统计学中的假设检验和置信区间估计，特别是在小样本情况下。\n\n\n\n学生t分布与正态分布的关系\n当样本量足够大时，样本平均数的分布可以转化为标准正态分布。但是当样本量较小时，这个比值的分布不再是标准正态分布，而是t分布。\n\n\n学生t分布的自由度\n自由度(degrees of freedom, df)是一个参数，用来确定t分布的形状。它通常与样本量有关，但并不等于样本量。在不同的统计问题中，自由度的计算方式略有不同，但它们都与样本量和统计模型的复杂度有关。\n通过了解学生t分布的基本原理、定义、性质及其与正态分布的关系，可以更好地应用这一理论于实际的统计分析和研究中。"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#帕累托分布",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#帕累托分布",
    "title": "算法交易中的一些概念",
    "section": "帕累托分布",
    "text": "帕累托分布\n帕累托分布理论，也称为帕累托法则或80/20法则，是由意大利经济学家维弗雷多·帕雷托提出的。这一理论指出，在许多情况下，大约80%的结果来自于20%的原因。这一原则不仅在经济学中有广泛应用，也被应用于社会学、管理学等多个领域。以下是关于帕累托分布理论的详细介绍：\n\n帕累托分布的定义和原理\n帕累托分布是一种幂次定律分布，描述了在许多情况下，一小部分原因会导致大部分结果的现象。例如，20%的人口可能拥有80%的财富，或者20%的客户可能贡献80%的销售额。\n\n\n帕累托分布的应用实例\n\n销售领域：识别并专注于最重要的20%的客户，以增加销售额。\n时间管理：优先处理能带来最大效益的20%的关键任务。\n商品库存管理：对贡献80%销售额的20%的商品给予更多关注。\n生产质量控制：集中解决导致80%客户投诉的20%的缺陷问题。\n健身锻炼：专注于对身体产生80%锻炼效果的20%的关键动作。\n软件开发：优先修复导致80%错误的20%的关键代码。\n\n\n\n帕累托分布的数学表达\n帕累托分布的概率密度函数(PDF)通常表示为：\n\\[f(x) = \\frac{k \\cdot x^{-\\alpha}}{1 - x^{-\\alpha}}\\]\n其中，\\(x\\) 是大于某个最小值 \\(x_{min}\\) 的正数，\\(k\\) 是分布的尺度参数，而 \\(\\alpha\\) 是形状参数，决定了分布的形状。当 \\(\\alpha &gt; 1\\) 时，分布是长尾的，这在描述财富分布等自然和社会现象时非常有用。\n\n\n帕累托分布与正态分布的区别\n\n形状：帕累托分布是长尾的，而正态分布是钟形的。\n应用领域：帕累托分布常用于描述极端值分布，如财富分布，而正态分布则适用于描述大多数自然和社会现象中的连续变量。\n\n通过了解帕累托分布的基本原理、定义、性质及其与正态分布的区别，可以更好地应用这一理论于实际的统计分析和研究中。帕累托分布不仅是一个数学工具，更是一种理解和分析复杂系统的思维方式。"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#乌伦贝克随机微分方程",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#乌伦贝克随机微分方程",
    "title": "算法交易中的一些概念",
    "section": "乌伦贝克随机微分方程",
    "text": "乌伦贝克随机微分方程\n乌伦贝克（Uhlenbeck）随机微分方程是一类重要的随机微分方程，通常用于描述物理、金融等领域中的随机现象。这类方程通常具有以下形式：\n\\[\ndX_t = b(X_t)dt + \\sigma(X_t)dW_t\n\\]\n其中，\\(X_t\\) 是一个随机过程，\\(b(X_t)\\) 是漂移项，\\(\\sigma(X_t)\\) 是扩散项，\\(W_t\\) 是一个标准布朗运动。\n乌伦贝克随机微分方程的一个特殊情况是奥恩斯坦-乌伦贝克（Ornstein-Uhlenbeck）过程，其形式如下：\n\\[\ndX_t = -\\theta X_t dt + \\sigma dW_t\n\\]\n其中，\\(\\theta\\) 和 \\(\\sigma\\) 是常数。这个过程描述了一个随机变量在受到线性恢复力和随机扰动的影响下的演化。\n解乌伦贝克随机微分方程通常需要使用随机微积分的理论和技术。对于一般的乌伦贝克随机微分方程，可以使用伊藤公式（Ito’s lemma）来求解。对于奥恩斯坦-乌伦贝克过程，可以直接求解得到其解析解：\n\\[\nX_t = X_0 e^{-\\theta t} + \\sigma \\int_0^t e^{-\\theta (t-s)} dW_s\n\\]\n其中，\\(X_0\\) 是初始条件。\n在实际应用中，乌伦贝克随机微分方程被广泛用于模拟和分析各种随机现象，如金融市场的波动、物理系统的布朗运动等。"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#均值标准差偏度和峰度",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#均值标准差偏度和峰度",
    "title": "算法交易中的一些概念",
    "section": "均值、标准差、偏度和峰度",
    "text": "均值、标准差、偏度和峰度\n均值、标准差、偏度和峰度是统计学中用于描述数据分布特征的四个重要指标。\n\n均值（Mean）：\n\n定义：所有数据的总和除以数据的个数。\n计算公式：μ = (Σx_i) / n，其中 x_i 是每个数据点，n 是数据点的数量。\n意义：均值反映了数据的集中趋势，即数据的一般水平。\n\n标准差（Standard Deviation）：\n\n定义：衡量数据点相对于均值的离散程度。\n计算公式：σ = sqrt(Σ(x_i - μ)^2 / n)，其中 x_i 是每个数据点，μ 是均值，n 是数据点的数量。\n意义：标准差越大，数据越分散；标准差越小，数据越集中。\n\n偏度（Skewness）：\n\n定义：衡量数据分布的对称性。\n计算公式：Sk = (Σ(x_i - μ)^3 / n) / σ^3，其中 x_i 是每个数据点，μ 是均值，σ 是标准差，n 是数据点的数量。\n意义：偏度为正表示数据右偏（尾部向右延伸），偏度为负表示数据左偏（尾部向左延伸），偏度为0表示数据对称。\n\n峰度（Kurtosis）：\n\n定义：衡量数据分布的尖峭程度。\n计算公式：K = (Σ(x_i - μ)^4 / n) / σ^4 - 3，其中 x_i 是每个数据点，μ 是均值，σ 是标准差，n 是数据点的数量。\n意义：峰度大于3表示数据分布比正态分布更尖峭，峰度小于3表示数据分布比正态分布更平坦。\n\n\n这些指标可以帮助我们更好地理解数据的分布特征，从而做出更合理的分析和决策。"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#皮尔逊分布",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#皮尔逊分布",
    "title": "算法交易中的一些概念",
    "section": "皮尔逊分布",
    "text": "皮尔逊分布\n皮尔逊分布，也称为皮尔逊III型分布，是一种连续概率分布，常用于统计学中描述偏态分布的数据。它是由Karl Pearson在19世纪提出的，作为一种更一般化的分布，用于描述那些不符合正态分布假设的数据。以下是关于皮尔逊分布的相关信息：\n\n定义\n皮尔逊分布的概率密度函数为：\n$ f(x, ) = ((x - ))^{- 1} (-(x - )) $\n其中：\n\n\\(\\beta = \\frac{2}{\\kappa}\\)\n\\(\\alpha = \\beta^2 = \\frac{4}{\\kappa^2}\\)\n\\(\\zeta = -\\frac{\\alpha}{\\beta} = -\\beta\\)\n\n这个概率密度函数在“标准化”形式下定义，通过loc和scale参数可以移动和/或缩放分布。\n\n\nScipy中的实现\n在Python的SciPy库中，可以通过scipy.stats.pearson3对象来生成Pearson III分布的随机变量、计算概率密度函数、累积分布函数(CDF)、逆累积分布函数(PPF)等。例如，生成随机数或显示概率密度函数的代码示例如下：\nimport numpy as np\nfrom scipy.stats import pearson3\n\n# 生成1000个随机数\nr = pearson3.rvs(skew=-2, size=1000)\n\n# 显示概率密度函数\nx = np.linspace(pearson3.ppf(0.01, skew=-2), pearson3.ppf(0.99, skew=-2), 100)\nplt.plot(x, pearson3.pdf(x, skew=-2), label='pearson3 pdf')\n通过这些工具，研究者可以更好地理解和分析偏态分布的数据，以及进行相关的统计推断和预测。\n\n\n应用场景\n皮尔逊III型分布适用于偏态分布数据的描述，特别是在统计学、金融、经济学等领域中，当数据分布明显偏离正态分布时，Pearson III分布提供了一个有效的模型来分析和预测数据。\n通过上述信息，可以看出皮尔逊分布在统计学和相关领域中具有重要的应用价值。"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#零假设",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#零假设",
    "title": "算法交易中的一些概念",
    "section": "零假设",
    "text": "零假设\n零假设（null hypothesis）是统计学中一个核心概念，它代表了研究者试图通过数据来反驳或拒绝的假设。以下是关于零假设的相关信息：\n\n零假设的定义\n零假设，也称为原假设，是在进行统计检验时预先建立的假设。它通常表达为总体参数等于某个固定值，例如“两组之间没有差异”或“两个事件之间没有关联”。\n\n\n零假设的理念\n零假设的理念在于，它作为一个起点，研究者通过收集和分析数据来试图证明这个假设是错误的，从而接受备择假设。这一过程体现了科学方法中的怀疑和验证精神。\n\n\n零假设与备择假设的关系\n\n零假设：研究者希望收集证据予以反对的假设。\n备择假设：与零假设相对，是研究者希望证明为真的假设。\n\n\n\n零假设在统计学中的重要性\n零假设是假设检验的基础，它帮助研究者系统地收集和评估证据，以决定是接受还是拒绝某个假设。通过假设检验，研究者可以基于数据做出更加科学和理性的决策。\n\n\n零假设的应用示例\n例如，在药物疗效研究中，零假设可能是“新药物对治疗疾病没有效果”，而备择假设则是“新药物对治疗疾病有显著效果”。通过统计分析，研究者会尝试收集证据来推翻零假设，从而支持备择假设。\n通过理解零假设的理念、定义及其在统计学中的应用，研究者可以更加有效地进行假设检验，从而得出更加科学和可靠的结论。"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#自协方差最小二乘法",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#自协方差最小二乘法",
    "title": "算法交易中的一些概念",
    "section": "自协方差最小二乘法",
    "text": "自协方差最小二乘法\n自协方差最小二乘法（Autocovariance Least Squares, ALS）是一种用于时间序列分析的方法，主要用于估计自回归移动平均模型（ARMA）的参数。自协方差最小二乘法的核心思想是通过最小化预测误差的自协方差来估计模型参数。\n以下是关于自协方差最小二乘法的详细介绍：\n\n自协方差最小二乘法的原理\n自协方差最小二乘法的基本原理是：对于给定的时间序列数据，首先构建一个自回归移动平均模型（ARMA），然后通过最小化预测误差的自协方差来估计模型的参数。这种方法可以有效地处理时间序列数据中的自相关性和异方差性。\n\n\n自协方差最小二乘法的步骤\n\n数据预处理：对时间序列数据进行预处理，包括去趋势、去季节性和缺失值处理等。\n模型选择：根据数据的特点和先验知识，选择合适的自回归移动平均模型（ARMA）。\n参数估计：通过最小化预测误差的自协方差来估计模型的参数。这通常涉及到求解一个优化问题，可以使用梯度下降法、牛顿法等优化算法来求解。\n模型检验：对估计得到的模型进行检验，包括残差分析、模型拟合优度检验等。\n模型应用：将估计得到的模型应用于预测和分析时间序列数据。\n\n\n\n自协方差最小二乘法的优点和局限性\n\n优点：自协方差最小二乘法能够有效地处理时间序列数据中的自相关性和异方差性，估计得到的模型参数较为准确。\n局限性：自协方差最小二乘法对初始参数的选择较为敏感，不同的初始参数可能导致不同的估计结果；此外，该方法在处理非线性时间序列数据时可能存在一定的局限性。\n\n总之，自协方差最小二乘法是一种有效的时间序列分析方法，适用于估计自回归移动平均模型的参数。在实际应用中，需要根据数据的特点和先验知识选择合适的模型和方法。"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#蒙大拿假设",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#蒙大拿假设",
    "title": "算法交易中的一些概念",
    "section": "蒙大拿假设",
    "text": "蒙大拿假设\n“蒙大拿假设”（Montana Hypothesis）是一个关于金融市场泡沫形成机制的理论，由美国经济学家约翰·肯尼斯·加尔布雷思（John Kenneth Galbraith）提出。这个假设以美国蒙大拿州的一个著名金矿小镇——赫勒拿（Helena）为名，因为该镇在19世纪末的金矿热潮中经历了价格的剧烈波动。\n蒙大拿假设的核心观点是，金融市场泡沫的形成往往源于投资者对某种资产价值的过度乐观预期，以及这种预期引发的投机行为。具体来说，这个假设包括以下几个要点：\n\n过度乐观预期：投资者对某种资产（如股票、房地产等）的未来价值持过度乐观的态度，认为其价格将持续上涨。\n投机行为：由于过度乐观预期，投资者纷纷涌入市场购买该资产，推高其价格。随着价格上涨，更多的投资者加入投机行列，形成正反馈循环。\n价格脱离基本面：在投机行为的推动下，资产价格逐渐脱离其内在价值，形成泡沫。此时，市场价格不再反映资产的真实价值，而是取决于投资者的心理预期和投机行为。\n泡沫破裂：当市场参与者意识到资产价格过高并开始纠正时，泡沫破裂，资产价格迅速下跌，导致市场恐慌和经济衰退。\n\n蒙大拿假设强调了投资者心理预期和投机行为在金融市场泡沫形成中的重要作用。然而，这个假设并非绝对正确，因为金融市场的泡沫形成机制可能因市场环境、政策因素等多种原因而有所不同。因此，在实际应用中，投资者应综合考虑多种因素，谨慎判断市场走势。"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#卡尔曼滤波法则",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#卡尔曼滤波法则",
    "title": "算法交易中的一些概念",
    "section": "卡尔曼滤波法则",
    "text": "卡尔曼滤波法则\n卡尔曼滤波法则是一种数学算法，用于估计和预测未知变量的值，特别是那些受到随机过程影响的系统。它是由匈牙利出生的美国工程师拉尔夫·伊万·卡尔曼在1960年发明的。卡尔曼滤波器的发展源于20世纪50年代末到60年代初的航天任务需求。当时需要一种能够实时处理复杂动态系统的测量数据的方法，以便进行精确导航和控制。卡尔曼滤波器正是在这种背景下应运而生，成为解决这些问题的有效工具。它的成功应用很快得到了广泛认可，并在之后的几十年里，在多个科学和工程领域中都得到了深入的研究和广泛应用。以下是关于卡尔曼滤波法则的相关信息：\n\n卡尔曼滤波的基本原理\n卡尔曼滤波的基本思想是通过观测数据对系统状态进行递归估计，不断更新状态的估计值。它假设系统状态是一个高斯分布，而观测值是由真实值和高斯噪声组成的。在每个时间步，卡尔曼滤波通过当前状态的预测和观测值的比较，得到一个新的状态估计值，并且利用上一个时间步的估计误差来调整预测误差的协方差矩阵，从而使得估计误差最小化。\n\n\n卡尔曼滤波的主要步骤\n\n预测步骤：根据上一时刻的状态估计和状态转移方程预测当前时刻的状态估计和协方差矩阵。\n更新步骤：根据当前时刻的观测值和观测方程，计算出当前时刻的状态估计和协方差矩阵。\n\n\n\n卡尔曼滤波的应用领域\n\n航天领域：飞船自主导航。\n自动驾驶车辆：高级驾驶辅助系统(ADAS)和自动驾驶车辆。\n机器人导航和运动控制：融合来自多种传感器的信息，包括激光雷达、视觉传感器和惯性测量单元(IMU)。\n医学影像处理：图像去噪和增强。\n金融领域：股票价格预测、风险管理以及利率模型的构建。\n\n通过上述分析，我们可以看到卡尔曼滤波法则不仅在理论上有着严谨的数学基础，而且在实际应用中也有着广泛的应用和重要的地位。"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#布林带指标",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#布林带指标",
    "title": "算法交易中的一些概念",
    "section": "布林带指标",
    "text": "布林带指标\n布林带指标（Bollinger Bands）是一种基于移动平均线和标准差计算的技术分析工具，用于判断价格的波动趋势和判断市场支撑和阻力位。它由三条线组成：中轨线（通常是20日移动平均线）、上轨线（中轨线加上两倍标准差）和下轨线（中轨线减去两倍标准差）。布林带指标通过这三条线描绘出价格波动的范围，帮助投资者判断市场是处于正常波动状态还是极端波动状态。以下是关于布林带指标的相关信息：\n\n布林带指标的原理\n布林带指标的原理基于统计学中的标准差原理，通过计算股票或其他资产价格的移动平均线（通常是20日移动平均线）和标准差来描绘价格波动的范围。上轨线和下轨线分别表示价格的标准差倍数，通常为2倍和-2倍，从而展示出价格在过去的20个交易日中，相对于其移动平均线的波动范围。\n\n\n布林带指标的使用方法\n\n基本用法：股价通常在布林通道区间内运行，股价运行到上轨附近时强压力位，一般可以作为卖点；股价运行到下轨附近是强支撑位，一般可以作为买点。\n与其他指标的配合使用：布林带指标可以与其他技术指标如成交量、KDJ指标等配合使用，以获得更准确的买卖信号。\n\n\n\n注意事项\n\n参数设定：布林线参数的设定不得小于6，静态钱龙设定值通常是10；动态钱龙设定时通常为20。\n市场环境：布林带指标适用于相对平稳的市场，当市场波动较大的时候，布林带指标提供的参考价值就不高了。\n确认信号：最好使用其他指标（如MACD、RSI）来确认布林带提供的信号，以避免误判。\n\n通过以上信息，投资者可以更好地理解和使用布林带指标，以提高股票交易的准确性和效率。"
  },
  {
    "objectID": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#帕累托-莱维分布",
    "href": "posts/2024-08-21-conceptions-in-algorithmic-trading.html#帕累托-莱维分布",
    "title": "算法交易中的一些概念",
    "section": "帕累托-莱维分布",
    "text": "帕累托-莱维分布\n帕累托-莱维分布（Pareto-Levy Distribution）并非一个标准的统计学术语，可能是指帕累托分布（Pareto Distribution）与莱维分布（Levy Distribution）的结合或误称。以下分别介绍帕累托分布和莱维分布，以及它们的特点和应用领域。\n\n帕累托分布\n帕累托分布，也称为Pareto分布，是以意大利经济学家维弗雷多·帕雷托命名的。它是从大量真实世界的现象中发现的幂次定律分布，这个分布在经济学以外，也被称为布拉德福分布。帕累托因对意大利20%的人口拥有80%的财产的观察而著名，后来被约瑟夫·朱兰和其他人概括为帕累托法则(80/20法则)，后来进一步概括为帕累托分布的概念。\n\n\n莱维分布\n莱维分布(Levy Distribution)是一种连续概率分布，常用于描述金融市场的收益率、自然科学中的某些现象以及其他需要描述重尾特性的场景。莱维分布的特点是它的概率密度函数在定义域内可能没有有限的最大值，这意味着极端值的概率不为零，与正态分布等分布形成对比。\n\n\n帕累托分布与莱维分布的关系\n\n定义与性质：帕累托分布关注的是财富或资源的分配，强调少数人掌握多数资源的现象。莱维分布则是一种更广泛的分布，用于描述具有重尾特性的数据，不局限于财富分配。\n应用领域：帕累托分布在经济学、社会学等领域有广泛应用，特别是在描述财富分配不均时。莱维分布则在金融、自然科学等多个领域都有应用，特别是在分析极端事件时。\n\n\n\n帕累托-莱维分布的特点\n由于帕累托-莱维分布并非一个标准的分布，因此没有特定的定义和特点。然而，我们可以从帕累托分布和莱维分布各自的特点中推测，帕累托-莱维分布可能结合了帕累托分布的重尾性和莱维分布的重尾特性，这意味着它可能用于描述既具有幂律分布特征又具有重尾特性的数据。\n\n\n帕累托-莱维分布的应用领域\n由于帕累托-莱维分布并非一个标准的分布，因此没有特定的应用领域。然而，我们可以推测它可能在描述具有幂律分布特征和重尾特性的复合数据集时有用，例如在金融市场的极端事件分析或自然界中的灾害风险分析中。\n帕累托-莱维分布并非一个标准的统计学术语，可能是指帕累托分布与莱维分布的结合或误称。帕累托分布关注财富或资源的分配，强调少数人掌握多数资源的现象，而莱维分布则用于描述具有重尾特性的数据。"
  },
  {
    "objectID": "posts/2024-09-05-Algorithmic-Trading-6.html",
    "href": "posts/2024-09-05-Algorithmic-Trading-6.html",
    "title": "算法交易：制胜策略与原理(欧内斯特·陈)之六",
    "section": "",
    "text": "日间动量型交易策略之所以存在的主要原因有四个：\n研究人员有时将资产价格所显现的动量模式分为两种类型，即时间序列动量和横向动量.\n时间序列型动量模式非常简单和直观：过往价格系列的收益率与未来的收益呈正相关性；横向动量是指某种金融工具的价格系列相对于其他工具的价格系列的相对性——当某种价格系列的收益胜过其他系列的收益，那么，此种势头将来可能会被延续，反之亦然。"
  },
  {
    "objectID": "posts/2024-09-05-Algorithmic-Trading-6.html#盘中动量型交易策略",
    "href": "posts/2024-09-05-Algorithmic-Trading-6.html#盘中动量型交易策略",
    "title": "算法交易：制胜策略与原理(欧内斯特·陈)之六",
    "section": "盘中动量型交易策略",
    "text": "盘中动量型交易策略\n我们看到大多数的金融工具，无论是股票还是期货，大都表现出横向型的动量模式，同时也经常呈现时间序列的性质。但不幸的是，这种动量行为模式的时间周期往往是一个月或更长的时间。而较长的持有期存在两个问题，即从统计学意义的回测程序上看，其所导致的夏普比率比较低，这主要是因为其间的独立交易信号比较罕见，进而在财政危机发生之后，相应的交易模式就会表现不佳。\n盘中动量交易策略可以触发的特定事件不仅仅是价格的运行模式，相关事件还包括诸如盈利公告之类的企业信息，或者分析师的更改建议，以及宏观经济新闻等，这些事件所生成的时间序列型动量交易策略早已为人所知，但是现在，我要对每个特定类别事件的影响做一些新的研究。\n\n“敞口”交易策略\n在第4章中，我们讨论了一个缺口买入型的、股票相关的均值回归策略，而于同等情境之下，动量型策略在操作期货和货币交易之时，其运行模式正好相反，即当相关金融工具的价格向上跳开时做多，而在其向下跳开时做空。\n\n# Example 7.1: Opening Gap Strategy for FSTX\n\nimport numpy as np\nimport pandas as pd\n#import matplotlib.pyplot as plt\n\nentryZscore=0.1\n\n\ndf=pd.read_csv('datas/inputDataDaily_FSTX_20120517.csv')\ndf['Date']=pd.to_datetime(df['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ndf.set_index('Date', inplace=True)\n\nstdretC2C90d=df['Close'].pct_change().rolling(90).std().shift()\n\nlongs= df['Open'] &gt;= df['High'].shift()*(1+entryZscore*stdretC2C90d)\nshorts=df['Open'] &gt;= df['Low'].shift()*(1-entryZscore*stdretC2C90d)\n\npositions=np.zeros(longs.shape)\n\npositions[longs]=1\npositions[shorts]=-1\n\nret=positions*(df['Close']-df['Open'])/df['Open']\n\ncumret=(np.cumprod(1+ret)-1)\ncumret.plot()\n\nprint('APR=%f Sharpe=%f' % (np.prod(1+ret)**(252/len(ret))-1, np.sqrt(252)*np.mean(ret)/np.std(ret)))\n# from calculateMaxDD import calculateMaxDD\nmaxDD, maxDDD, i=calculateMaxDD(cumret.fillna(0))\nprint('Max DD=%f Max DDD in days=%i' % (maxDD, maxDDD))\n#APR=0.074864 Sharpe=0.494857\n#Max DD=-0.233629 Max DDD in days=789\n\nAPR=0.074864 Sharpe=0.494857\nMax DD=-0.233629 Max DDD in days=789\n\n\nC:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_4936\\1180818726.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  highwatermark[t]=np.maximum(highwatermark[t-1], cumret[t])\nC:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_4936\\1180818726.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  drawdown[t]=(1+cumret[t])/(1+highwatermark[t])-1\n\n\n\n\n\n\n\n\n\n\n\n信息驱动的动量交易策略\n正如许多人认为的那样：价格的动量运行模式是由缓慢扩散的新闻事件所导致的。当然，我们可以在一个有新闻价值的事件之后的前几天、几小时甚至几秒之后有所受益，这就是传统的财报收益报告公布之后的行情漂移的基本原理模型（post-earnings announcement drift models，PEAD模型）；同时，还有一些其他的模型，它们大都基于不同企业或宏观经济新闻等因素来生成相应的动量运行模式。\n如果在前一个交易日收盘之后，股票相关企业公布其收益报告，我们于第二个交易日的开盘时刻入场——如果相关收益率为正，我们就做多该股票；如果相应收益率为负，我们就做空该股票，同时，在当天清算所有头寸，那么，这个交易策略仍然是有利可图的。这里需要注意的是：这种策略不需要交易员解释收益报告是“好”还是“坏”，甚至不要求交易员知道相应收益是否高于或低于分析师的预期，我们可以让市场行情告诉自己相应的收益是好还是坏。\n\n# Example 4.1: Buy-on-Gap Model on SPX Stocks\n\nimport numpy as np\nimport pandas as pd\n#import matplotlib.pyplot as plt\n#import statsmodels.formula.api as sm\n#import statsmodels.tsa.stattools as ts\n#import statsmodels.tsa.vector_ar.vecm as vm\n\nop=pd.read_csv('datas/inputDataOHLCDaily_20120424_op.csv')\ncl=pd.read_csv('datas/inputDataOHLCDaily_20120424_cl.csv')\n\nstocks=pd.read_csv('datas/inputDataOHLCDaily_20120424_stocks.csv')\n\nop['Var1']=pd.to_datetime(op['Var1'],  format='%Y%m%d').dt.date # remove HH:MM:SS\nop.columns=np.insert(stocks.values, 0, 'Date')\nop.set_index('Date', inplace=True)\n\ncl['Var1']=pd.to_datetime(cl['Var1'],  format='%Y%m%d').dt.date # remove HH:MM:SS\ncl.columns=np.insert(stocks.values, 0, 'Date')\ncl.set_index('Date', inplace=True)\n\nearnann=pd.read_csv('datas/earnannFile.csv')\nearnann['Date']=pd.to_datetime(earnann['Date'],  format='%Y%m%d').dt.date # remove HH:MM:SS\nearnann.set_index('Date', inplace=True)\n\nnp.testing.assert_array_equal(stocks.iloc[0,:], earnann.columns)\n\ndf=pd.merge(op, cl, how='inner', left_index=True, right_index=True, suffixes=('_op', '_cl'))\ndf=pd.merge(earnann, df, how='inner', left_index=True, right_index=True)\n\nearnann=df.iloc[:, 0:(earnann.shape[1])].astype(bool)\nop=df.iloc[:, (earnann.shape[1]):((earnann.shape[1])+op.shape[1])]\ncl=df.iloc[:, ((earnann.shape[1])+op.shape[1]):]\n\nop.columns=stocks.iloc[0,:]\ncl.columns=stocks.iloc[0,:]\n\nlookback=90\n\nretC2O=(op-cl.shift())/cl.shift()\nstdC2O=retC2O.rolling(lookback).std()\n\npositions=np.zeros(cl.shape) \n\nlongs=  (retC2O &gt;=  0.5*stdC2O) & earnann\nshorts= (retC2O &lt;= -0.5*stdC2O) & earnann\n\npositions[longs]=1\npositions[shorts]=-1\n\nret=np.sum(positions*(cl-op)/op, axis=1)/30\n\ncumret=(np.cumprod(1+ret)-1)\ncumret.plot()\n\nprint('APR=%f Sharpe=%f' % (np.prod(1+ret)**(252/len(ret))-1, np.sqrt(252)*np.mean(ret)/np.std(ret)))\n# from calculateMaxDD import calculateMaxDD\nmaxDD, maxDDD, i=calculateMaxDD(cumret.fillna(0))\nprint('Max DD=%f Max DDD in days=%i' % (maxDD, maxDDD))\n#APR=0.068126 Sharpe=1.494743\n#Max DD=-0.026052 Max DDD in days=109\n\nAPR=0.068126 Sharpe=1.494743\nMax DD=-0.026052 Max DDD in days=109\n\n\nC:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_4936\\1180818726.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  highwatermark[t]=np.maximum(highwatermark[t-1], cumret[t])\nC:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_4936\\1180818726.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  drawdown[t]=(1+cumret[t])/(1+highwatermark[t])-1\n\n\n\n\n\n\n\n\n\n我们在均值回归策略所相关的例5-1中，对蒙特卡罗技术进行了详细的解析。但首先，我们依据测试集中的天收益率就会很容易地计算出相应的凯利杠杆比例是18.4；我们应该记住这个数字，且与蒙特卡罗模拟的测试结果进行比较；接下来，我们基于相应日收益率所相关的初始四个阶矩来构造一个皮尔逊系统，并在系统中生成100000个随机收益率的数值\n\n# Box 8.1\nimport numpy as np\nimport pandas as pd\n#from scipy.stats import describe \nfrom scipy.stats import pearson3\n#import matplotlib.pyplot as plt\n#import statsmodels.formula.api as sm\n#import statsmodels.tsa.stattools as ts\n#import statsmodels.tsa.vector_ar.vecm as vm\nfrom scipy.optimize import minimize\n \ndf=pd.read_csv('datas/AUDCAD_unequal_ret.csv')\n# 使用pearson3函数拟合数据，并计算了数据的偏度、位置和尺度参数\nskew_, loc_, scale_=pearson3.fit(df) \nprint('skew=%f loc=%f scale=%f' % (skew_, loc_, scale_))\nmean,var,skew,kurt=pearson3.stats(skew_, loc_, scale_, moments='mvks')\nprint('mean=%f var=%f skew=%f kurt=%f' % (mean, var, skew, kurt))\n# 使用拟合得到的参数生成了100,000个模拟收益率\nret_sim=pearson3.rvs(skew_, loc_, scale_, size=100000, random_state=0)\n# 定义了一个函数g，用于计算给定收益率和杠杆率的增长率\ndef g(f, R):\n    return np.sum(np.log(1+f*R), axis=0)/R.shape[0]\n# 对于不同的杠杆率（从1到23），计算了模拟收益率的增长率，并绘制了图表\nmyf=range(1, 24)\nmyg=np.full(24, np.nan)\n\nfor f in myf:\n    myg[f]=g(f, ret_sim)\n\nmyg=myg[1:]\nmyg=pd.DataFrame(myg)\nmyg.plot()\n# 定义了一个最小化函数minusG和minusGsim，用于找到最优的杠杆率\nminusG = lambda f : -g(f, df)\nminusGsim = lambda f : -g(f, ret_sim)\n# 使用minimize函数找到了最优的杠杆率和对应的最大增长率\n#optimal leverage based on simulated returns\nres = minimize(minusGsim, 0, method='Nelder-Mead')\noptimalF=res.x\nprint('Optimal leverage=%f optimal growth rate=%f' % (optimalF[0], -res.fun))\n#Optimal leverage=25.512625 optimal growth rate=0.005767\n\nminR=np.min(ret_sim)\nprint('minR=%f' % (minR))\n#minR=-0.018201\n\n# max drawdown with optimal leverage\n# from calculateMaxDD import calculateMaxDD\nmaxDD, maxDDD, i=calculateMaxDD((np.cumprod(1+optimalF*ret_sim)-1))\nprint('Max DD=%f with optimal leverage=%f' % (maxDD, optimalF[0]))\n#Max DD=-0.996312 with optimal leverage=25.512625\n\n#max drawdown with half of optimal leverage\nmaxDD, maxDDD, i=calculateMaxDD((np.cumprod(1+optimalF/2*ret_sim)-1))\nprint('Max DD=%f with half optimal leverage=%f' % (maxDD, optimalF[0]/2))\n#Max DD=-0.900276 with half optimal leverage=12.756313\n\n# max drawdown with 1/7 of optimal leverage\nmaxDD, maxDDD, i=calculateMaxDD((np.cumprod(1+optimalF/7*ret_sim)-1))\nprint('Max DD=%f with half optimal leverage=%f' % (maxDD, optimalF[0]/7))\n#Max DD=-0.429629 with half optimal leverage=3.644661\n\n#max drawdown with 1/1.4 of optimal leverage for historical returns\nmaxDD, maxDDD, i=calculateMaxDD((np.cumprod(1+optimalF/1.4*df.values)-1))\nprint('Max DD=%f with historical returns=%f' % (maxDD, optimalF[0]/1.4))\n#Max DD=-0.625894 with historical returns=18.223304\n\nD=0.5\nprint('Growth rate on simulated returns using D=%3.1f of optimal leverage on full account=%f' % (D, -minusGsim(optimalF*D)))\n#Growth rate on simulated returns using D=0.5 of optimal leverage on full account=0.004317\nmaxDD, maxDDD, i=calculateMaxDD((np.cumprod(1+optimalF*D*ret_sim)-1))\nprint('MaxDD on simulated returns using D of optimal leverage on full account=%f' % (maxDD))\n#MaxDD on simulated returns using D of optimal leverage on full account=-0.900276\n# 使用CPPI策略计算了模拟收益率和历史收益率的增长率和最大回撤\n# CPPI\ng_cppi=0\ndrawdown=0\nfor t in range(ret_sim.shape[0]):\n    g_cppi+=np.log(1+ret_sim[t]*D*optimalF*(1+drawdown))\n    drawdown=min([0, (1+drawdown)*(1+ret_sim[t])-1])\n    \ng_cppi=g_cppi/len(ret_sim)\nprint('Growth rate on simulated returns using CPPI=%f' % g_cppi[0])\n#Growth rate on simulated returns using CPPI=0.004264\nprint('Growth rate on historical returns using D of optimal leverage on full account=%f' % (-minusG(optimalF*D)))\n#Growth rate on historical returns using D of optimal leverage on full account=0.004053\nmaxDD, maxDDD, i=calculateMaxDD((np.cumprod(1+optimalF*D*df.values)-1))\nprint('MaxDD on historical returns using D of optimal leverage on full account=%f' % (maxDD))\n#MaxDD on historical returns using D of optimal leverage on full account=-0.303448\n\n# CPPI\ng_cppi=0\ndrawdown=0\nfor t in range(df.shape[0]):\n    g_cppi+=np.log(1+df.iloc[t,]*D*optimalF*(1+drawdown))\n    # print(\"=========\")\n    # print(type((1+drawdown)*(1+df.iloc[t,])-1))\n    # print((1+drawdown)*(1+df.iloc[t,])-1)\n    # print(((1+drawdown)*(1+df.iloc[t,])-1)[0])\n    # print(\"=========\")\n    drawdown=np.min([0, ((1+drawdown)*(1+df.iloc[t,])-1)[0]])\n    \ng_cppi=g_cppi/len(df)\nprint('Growth rate on historical returns CPPI=%f' % (g_cppi))\n\nskew=0.122820 loc=0.000432 scale=0.004231\nmean=0.000432 var=0.000018 skew=0.122820 kurt=0.022627\nOptimal leverage=25.512625 optimal growth rate=0.005767\nminR=-0.018201\nMax DD=-0.996312 with optimal leverage=25.512625\nMax DD=-0.900276 with half optimal leverage=12.756313\nMax DD=-0.429629 with half optimal leverage=3.644661\nMax DD=-0.625894 with historical returns=18.223304\nGrowth rate on simulated returns using D=0.5 of optimal leverage on full account=0.004317\nMaxDD on simulated returns using D of optimal leverage on full account=-0.900276\nGrowth rate on simulated returns using CPPI=0.004264\nGrowth rate on historical returns using D of optimal leverage on full account=0.004053\nMaxDD on historical returns using D of optimal leverage on full account=-0.482626\n\n\nC:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_4936\\158564989.py:85: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n  print('Growth rate on historical returns using D of optimal leverage on full account=%f' % (-minusG(optimalF*D)))\nC:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_4936\\158564989.py:101: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  drawdown=np.min([0, ((1+drawdown)*(1+df.iloc[t,])-1)[0]])\n\n\nGrowth rate on historical returns CPPI=0.003991\n\n\nC:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_4936\\158564989.py:104: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n  print('Growth rate on historical returns CPPI=%f' % (g_cppi))\n\n\n\n\n\n\n\n\n\nCPPI（Constant Proportion Portfolio Insurance）策略，即恒定比例投资组合保险策略，是一种投资组合管理技术，主要用于风险管理和资产配置。CPPI策略的核心思想是通过动态调整风险资产（如股票、期货等）和安全资产（如债券、现金等）的比例，以保证投资组合在特定时间段内的最低价值。\nCPPI策略的基本原理如下：\n\n确定保本额：投资者首先确定一个保本额，即在投资期末希望至少能够收回的金额。\n计算安全垫：安全垫是指投资组合当前价值与保本额之间的差额。安全垫的大小决定了可以投资于风险资产的最大金额。\n确定风险乘数：风险乘数是一个系数，用于确定投资于风险资产的金额。风险乘数的大小取决于投资者的风险承受能力和投资目标。\n动态调整资产配置：根据市场情况和投资组合的表现，定期调整风险资产和安全资产的比例。当投资组合价值上升时，可以增加对风险资产的投资；当投资组合价值下降时，减少对风险资产的投资，以保证投资组合的价值不低于保本额。\n\nCPPI策略的优点：\n\n保本性：通过设定保本额和安全垫，CPPI策略能够在投资期末保证投资者至少能够收回保本额。\n灵活性：CPPI策略可以根据市场情况和投资者的风险承受能力动态调整资产配置，以实现最佳的风险收益平衡。\n简单易行：CPPI策略的计算方法相对简单，易于理解和实施。\n\n然而，CPPI策略也存在一定的缺点：\n\n市场风险：尽管CPPI策略能够在一定程度上降低市场风险，但在极端市场情况下，如市场大幅下跌，投资组合的价值仍可能低于保本额。\n机会成本：为了保证投资组合的保本性，投资者可能需要放弃部分潜在的高收益机会。\n\n总之，CPPI策略是一种适用于风险厌恶型投资者的投资组合管理技术，通过动态调整资产配置，可以在保证投资组合保本性的同时，追求一定的投资收益。"
  }
]